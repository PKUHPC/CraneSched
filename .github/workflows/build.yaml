name: Build, Upload and Test

on:
  schedule:
    - cron: "0 2 * * *"
  push:
    branches:
      - master
  pull_request:
    types:
      - opened
      - synchronize
      - reopened
      - ready_for_review
    branches:
      - master
      - release/*
  workflow_dispatch:
    inputs:
      frontend_branch:
        description: 'Branch of FrontEnd to use'
        required: true
        default: 'master'
      cache_buster:
        description: 'Optional cache buster to force rebuild (any non-empty string)'
        required: false
        default: ''

concurrency:
  group: >-
    ${{ github.event_name == 'pull_request'
        && format('pr-{0}', github.event.pull_request.number)
        || format('ref-{0}', github.ref) }}
  cancel-in-progress: true

env:
  # Self-hosted runner paths
  SCRIPT_PATH: /opt/actions-runner-external/script
  CACHE_PATH: /opt/actions-runner-external/cache

  # Unified frontend ref across all triggers
  FRONTEND_REF: ${{ inputs.frontend_branch || 'master' }}

  # Cache buster for forcing rebuilds on workflow_dispatch
  CACHE_BUSTER: ${{ inputs.cache_buster || '' }}

  # CI image (tag used only for lookup; fingerprint uses resolved digest)
  CI_IMAGE_TAG: ghcr.io/nativu5/cranedev:ci

jobs:

  build:
    # Skip if this is a draft PR
    if: github.event_name != 'pull_request' || !github.event.pull_request.draft
    runs-on: ["self-hosted", "CraneSched"]
    defaults:
      run:
        shell: bash -leo pipefail {0}

    outputs:
      # Expose the name of the bundle artifact to the test job
      output_bundle_artifact: build-output.tgz

    steps:
      # Checkout backend repository
      - name: Checkout backend
        uses: actions/checkout@v4
        with:
          path: CraneSched

      # Checkout frontend repository at the unified ref
      - name: Checkout frontend
        uses: actions/checkout@v4
        with:
          repository: PKUHPC/CraneSched-FrontEnd
          path: CraneSched-FrontEnd
          ref: ${{ env.FRONTEND_REF }}

      # Resolve CI image digest without pulling layers if possible.
      # Strategy (first match wins):
      # 1) skopeo inspect docker://$CI_IMAGE_TAG -> .Digest
      # 2) podman manifest inspect $CI_IMAGE_TAG -> manifests[0].digest or config.digest
      # 3) Fallback: podman pull, then podman image inspect -> .Digest
      - name: Resolve CI image digest
        id: imagedigest
        run: |
          set -euo pipefail
          IMG="${CI_IMAGE_TAG}"

          resolve_with_skopeo() {
            command -v skopeo >/dev/null 2>&1 || return 1
            DIGEST=$(skopeo inspect "docker://${IMG}" | jq -r '.Digest' 2>/dev/null) || return 1
            [ -n "${DIGEST}" ] || return 1
            echo "${DIGEST}"
          }

          resolve_with_manifest() {
            command -v podman >/dev/null 2>&1 || return 1
            RAW=$(podman manifest inspect "${IMG}" 2>/dev/null) || return 1
            # Try manifest list first
            if echo "${RAW}" | jq -e '.manifests' >/dev/null 2>&1; then
              # Optional: filter by current platform to improve cache hit rate
              ARCH=$(uname -m)
              OS=$(uname -s | awk '{print tolower($0)}')
              # Normalize common arch values (e.g., x86_64 -> amd64)
              if [ "$ARCH" = "x86_64" ]; then ARCH="amd64"; fi
              DIGEST=$(echo "${RAW}" | jq -r --arg arch "$ARCH" --arg os "$OS" \
                '.manifests[] | select(.platform.architecture==$arch and .platform.os==$os) | .digest' | head -n1)
              # Fallback to the first manifest if platform-filter failed
              if [ -z "${DIGEST}" ] || [ "${DIGEST}" = "null" ]; then
                DIGEST=$(echo "${RAW}" | jq -r '.manifests[0].digest')
              fi
            fi
            # Fallback to single manifest config digest
            if [ -z "${DIGEST:-}" ] || [ "${DIGEST}" = "null" ]; then
              DIGEST=$(echo "${RAW}" | jq -r '.config.digest' 2>/dev/null) || true
            fi
            [ -n "${DIGEST:-}" ] && [ "${DIGEST}" != "null" ] || return 1
            echo "${DIGEST}"
          }

          resolve_with_pull() {
            command -v podman >/dev/null 2>&1 || return 1
            podman pull "${IMG}" >/dev/null
            podman image inspect "${IMG}" --format '{{.Digest}}'
          }

          DIGEST=""
          if [ -z "${DIGEST}" ]; then DIGEST="$(resolve_with_skopeo || true)"; fi
          if [ -z "${DIGEST}" ]; then DIGEST="$(resolve_with_manifest || true)"; fi
          if [ -z "${DIGEST}" ]; then DIGEST="$(resolve_with_pull || true)"; fi

          if [ -z "${DIGEST}" ]; then
            echo "Failed to resolve image digest for ${IMG}" >&2
            exit 1
          fi

          echo "CI_IMAGE_DIGEST=${DIGEST}" | tee -a "$GITHUB_ENV"
          echo "digest=${DIGEST}" >> "$GITHUB_OUTPUT"

      # Compute build fingerprint from both reposâ€™ HEAD SHAs, frontend ref, CI image digest, and optional CACHE_BUSTER
      - name: Compute build fingerprint
        id: fp
        run: |
          CRANE_SHA=$(git -C CraneSched rev-parse HEAD)
          FE_SHA=$(git -C CraneSched-FrontEnd rev-parse HEAD)
          FE_REF="${FRONTEND_REF}"
          CI_DIGEST="${CI_IMAGE_DIGEST}"
          FINGERPRINT="${CRANE_SHA}-${FE_REF}-${FE_SHA}-${CI_DIGEST}-${CACHE_BUSTER}"
          echo "CRANE_SHA=${CRANE_SHA}" | tee -a $GITHUB_ENV
          echo "FE_SHA=${FE_SHA}" | tee -a $GITHUB_ENV
          echo "FE_REF=${FE_REF}" | tee -a $GITHUB_ENV
          echo "FINGERPRINT=$FINGERPRINT" | tee -a $GITHUB_ENV
          echo "key=build-output-${FINGERPRINT}" >> $GITHUB_OUTPUT

      # Prepare workspace required regardless of cache hit/miss
      - name: Prepare workspace
        run: |
          echo "Linking $SCRIPT_PATH to $(pwd)/script"
          ln -sfT "$SCRIPT_PATH" "$(pwd)/script"
          echo "Linking $CACHE_PATH to $(pwd)/cache"
          ln -sfT "$CACHE_PATH" "$(pwd)/cache"
          mkdir -p output
          mkdir -p log

      # Try to restore previously built outputs using the fingerprint
      - name: Restore build output cache
        id: restore_build
        uses: actions/cache/restore@v4
        with:
          # Cache the whole output/ directory (every file is meaningful)
          path: output
          key: ${{ steps.fp.outputs.key }}

      # Pull CI image only on cache miss (build is needed)
      - name: Pull CI Image (only on miss)
        if: steps.restore_build.outputs.cache-hit != 'true'
        run: |
          podman pull "${CI_IMAGE_TAG}"

      # Build inside container only when cache miss
      - name: Build in Container (only on miss)
        if: steps.restore_build.outputs.cache-hit != 'true'
        run: |
          podman run --rm \
              -v ./CraneSched:/Workspace/CraneSched \
              -v ./CraneSched-FrontEnd:/Workspace/CraneSched-FrontEnd \
              -v ./output:/Workspace/output \
              -v ./script:/Workspace/script \
              -v ./cache/ccache:/root/.ccache \
              "${CI_IMAGE_TAG}" /bin/bash --login script/build.sh

      # Write/refresh a human-readable fingerprint file into output/ (runs for both cache-hit and cache-miss).
      # For cache-miss this happens BEFORE saving the cache, so the file is included in the cache.
      - name: Write fingerprint file
        run: |
          TS="$(date -u +"%Y-%m-%dT%H:%M:%SZ")"
          RUN_URL="https://github.com/${GITHUB_REPOSITORY}/actions/runs/${GITHUB_RUN_ID}"
          {
            echo "CraneSched Build Fingerprint"
            echo "================================"
            echo "Timestamp (UTC):     ${TS}"
            echo "GitHub Run:          ${RUN_URL}"
            echo
            echo "Backend (CraneSched):"
            echo "  HEAD SHA:          ${CRANE_SHA}"
            echo
            echo "Frontend (CraneSched-FrontEnd):"
            echo "  Ref:               ${FE_REF}"
            echo "  HEAD SHA:          ${FE_SHA}"
            echo
            echo "CI Image:"
            echo "  Tag:               ${CI_IMAGE_TAG}"
            echo "  Digest:            ${CI_IMAGE_DIGEST}"
            echo
            echo "Cache Buster:        ${CACHE_BUSTER:-<empty>}"
            echo
            echo "Composite Fingerprint Key:"
            echo "  ${FINGERPRINT}"
          } > output/fingerprint.txt

      # Validate that output/ is not empty; fail early if no files were produced/restored
      - name: Validate non-empty output
        run: |
          if ! find output -type f -print -quit | grep -q .; then
            echo "ERROR: output/ is empty; failing early." >&2
            ls -la output || true
            exit 1
          fi

      # Save outputs to cache only when newly built (after fingerprint.txt is written)
      - name: Save build output cache (only on miss)
        if: steps.restore_build.outputs.cache-hit != 'true'
        uses: actions/cache/save@v4
        with:
          path: output
          key: ${{ steps.fp.outputs.key }}

      # Create a single bundle for publishing and for the downstream test job
      - name: Create output bundle
        run: |
          tar -czf build-output.tgz -C output .

      # Upload only ONE artifact: the unified bundle for release & cross-job
      - name: Upload output bundle
        uses: actions/upload-artifact@v4
        with:
          name: build-output.tgz
          path: build-output.tgz
          retention-days: 14
          if-no-files-found: error

  test:
    needs: build
    runs-on: ["self-hosted", "CraneSched"]
    defaults:
      run:
        shell: bash -leo pipefail {0}
    env:
      TEST_FAILED: false
      SCRIPT_PATH: /opt/actions-runner-external/script
      CACHE_PATH: /opt/actions-runner-external/cache

    steps:
      # Prepare workspace for AutoTest
      - name: Prepare workspace
        run: |
          echo "Linking $SCRIPT_PATH to $(pwd)/script"
          ln -sfT "$SCRIPT_PATH" "$(pwd)/script"
          echo "Linking $CACHE_PATH to $(pwd)/cache"
          ln -sfT "$CACHE_PATH" "$(pwd)/cache"
          mkdir -p output
          mkdir -p log

      # Download and extract the unified bundle produced by the build job
      - name: Download build outputs
        uses: actions/download-artifact@v4
        with:
          name: ${{ needs.build.outputs.output_bundle_artifact }}
          path: .
      - name: Extract build outputs
        run: |
          mkdir -p output
          tar -xzf build-output.tgz -C output

      # Run AutoTest using the restored output directory
      - name: Run AutoTest
        continue-on-error: true
        run: |
          NETWORK_ID=$(podman container inspect mongodb | jq -r '.[0].NetworkSettings.Networks | keys[]' || echo "")
          if [ -z "$NETWORK_ID" ]; then
            echo "WARN: mongodb container network not found; falling back to default podman network"
          fi
          podman run -d --rm --name autotest \
            --privileged \
            --systemd true \
            -v /lib/modules:/lib/modules:ro \
            -v ./script:/CraneSched-AutoTest/script \
            -v ./output:/CraneSched-AutoTest/output \
            -v ./log:/CraneSched-AutoTest/log \
            $( [ -n "$NETWORK_ID" ] && echo "--network $NETWORK_ID" ) \
            localhost/autotest
          podman exec autotest /bin/bash --login script/run.sh || echo "TEST_FAILED=true" >> $GITHUB_ENV
          podman stop autotest || true

      # Upload AutoTest results (kept independent from build bundle)
      - name: Upload AutoTest Results
        uses: actions/upload-artifact@v4
        with:
          name: result.json
          path: output/result.json
          retention-days: 14
          if-no-files-found: error

      # Fail the job if tests failed
      - name: Alarm on test failure
        if: env.TEST_FAILED == 'true'
        run: exit 1
