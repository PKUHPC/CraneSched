{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"CraneSched Documentation A distributed intelligent scheduler for HPC and AI workloads, designed for performance, scale, and simplicity. What is CraneSched? CraneSched is an open-source, distributed scheduling system developed by the High Performance Computing Platform of Peking University. It unifies HPC and AI scheduling with efficient resource management, robust isolation, and a modern architecture. Key components: Cranectld: the central controller and scheduler (control node) Craned: the node agent/daemon (compute nodes) Frontend tools and services: CLI commands (cbatch, cqueue, crun, etc.), cfored, cplugind Try CraneSched on Web: Demo cluster Source code: Backend repository Frontend repository Highlights Unified workloads: HPC and AI job payload in a single, integrated system High throughput: 100k+ scheduling decisions per second for rapid job\u2013resource matching Massive scalability: designed to manage clusters with millions of cores User-friendly: concise, consistent user and admin commands and workflows Security-conscious: role-based access control (RBAC), mTLS-encrypted communications, and secure-by-default configurations Designed for resilience: automatic job recovery, no single point of failure, and fast state restoration Open and extensible: community-driven, pluggable architecture for customization and integration Latest updates 2025-04-08 \u2014 v1.1.2: GCC 15/Clang 20 toolchains, node drain/resume events, partition account control, Vault integration 2025-01-24 \u2014 v1.1.0: X11 forwarding, user QoS limits, multi-GID, cgroupv2 & Ascend NPU, scheduler/event optimizations 2024-10-24 \u2014 v1.0.0: Job monitoring, plugins, device support, IPv6, resource & job management improvements Quick start Choose your deployment guide: Backend (Rocky 9, recommended) Backend (CentOS 7, legacy) Frontend components eBPF for GRES on cgroup v2 Install and start services: Start cranectld on control node(s) Start craned on all compute nodes Deploy optional frontend services where needed (cfored, cplugind) Submit a job Batch jobs Interactive jobs: crun and calloc Architecture CraneSched introduces a Resources Manager abstraction to handle different workload types: HPC jobs: Cgroup Manager allocates resources and provides cgroup-based isolation AI jobs: Container Manager leverages Kubernetes for resource allocation and container lifecycle management Licenses CraneSched is dual-licensed under AGPLv3 and a commercial license. See LICENSE for details or contact mayinping@pku.edu.cn for commercial licensing.","title":"Home"},{"location":"#cranesched-documentation","text":"A distributed intelligent scheduler for HPC and AI workloads, designed for performance, scale, and simplicity.","title":"CraneSched Documentation"},{"location":"#what-is-cranesched","text":"CraneSched is an open-source, distributed scheduling system developed by the High Performance Computing Platform of Peking University. It unifies HPC and AI scheduling with efficient resource management, robust isolation, and a modern architecture. Key components: Cranectld: the central controller and scheduler (control node) Craned: the node agent/daemon (compute nodes) Frontend tools and services: CLI commands (cbatch, cqueue, crun, etc.), cfored, cplugind Try CraneSched on Web: Demo cluster Source code: Backend repository Frontend repository","title":"What is CraneSched?"},{"location":"#highlights","text":"Unified workloads: HPC and AI job payload in a single, integrated system High throughput: 100k+ scheduling decisions per second for rapid job\u2013resource matching Massive scalability: designed to manage clusters with millions of cores User-friendly: concise, consistent user and admin commands and workflows Security-conscious: role-based access control (RBAC), mTLS-encrypted communications, and secure-by-default configurations Designed for resilience: automatic job recovery, no single point of failure, and fast state restoration Open and extensible: community-driven, pluggable architecture for customization and integration","title":"Highlights"},{"location":"#latest-updates","text":"2025-04-08 \u2014 v1.1.2: GCC 15/Clang 20 toolchains, node drain/resume events, partition account control, Vault integration 2025-01-24 \u2014 v1.1.0: X11 forwarding, user QoS limits, multi-GID, cgroupv2 & Ascend NPU, scheduler/event optimizations 2024-10-24 \u2014 v1.0.0: Job monitoring, plugins, device support, IPv6, resource & job management improvements","title":"Latest updates"},{"location":"#quick-start","text":"Choose your deployment guide: Backend (Rocky 9, recommended) Backend (CentOS 7, legacy) Frontend components eBPF for GRES on cgroup v2 Install and start services: Start cranectld on control node(s) Start craned on all compute nodes Deploy optional frontend services where needed (cfored, cplugind) Submit a job Batch jobs Interactive jobs: crun and calloc","title":"Quick start"},{"location":"#architecture","text":"CraneSched introduces a Resources Manager abstraction to handle different workload types: HPC jobs: Cgroup Manager allocates resources and provides cgroup-based isolation AI jobs: Container Manager leverages Kubernetes for resource allocation and container lifecycle management","title":"Architecture"},{"location":"#licenses","text":"CraneSched is dual-licensed under AGPLv3 and a commercial license. See LICENSE for details or contact mayinping@pku.edu.cn for commercial licensing.","title":"Licenses"},{"location":"command/cacct/","text":"cacct \u67e5\u770b\u4f5c\u4e1a\u4fe1\u606f cacct\u53ef\u4ee5\u67e5\u770b\u961f\u5217\u4e2d\u7684\u4f5c\u4e1a\u4fe1\u606f\u3002 \u67e5\u770b\u96c6\u7fa4\u4e2d\u6240\u6709\u961f\u5217\u7684\u4f5c\u4e1a\u4fe1\u606f\uff08\u5305\u62ec\u6240\u6709\u72b6\u6001\uff09\uff0c\u9ed8\u8ba4\u8f93\u51fa100\u6761\u4fe1\u606f\u3002 cacct cacct\u8fd0\u884c\u7ed3\u679c\u5c55\u793a \u4e3b\u8981\u8f93\u51fa\u9879 TaskId \uff1a\u4f5c\u4e1a\u53f7 TaskName : \u4f5c\u4e1a\u540d Partition \uff1a\u4f5c\u4e1a\u6240\u5728\u5206\u533a Account \uff1a\u4f5c\u4e1a\u6240\u5c5e\u8d26\u6237 AllocCPUs \uff1a\u4f5c\u4e1a\u5206\u914d\u7684CPU\u6570\u91cf State \uff1a\u4f5c\u4e1a\u72b6\u6001 ExitCode \uff1a\u4f5c\u4e1a\u72b6\u6001\u7801 \u4e3b\u8981\u53c2\u6570 -A/--account string \uff1a\u6307\u5b9a\u67e5\u8be2\u4f5c\u4e1a\u7684\u6240\u5c5e\u8d26\u6237\uff0c\u6307\u5b9a\u591a\u4e2a\u8d26\u6237\u65f6\u7528\u9017\u53f7\u9694\u5f00 -C/--config string\uff1a \u914d\u7f6e\u6587\u4ef6\u8def\u5f84(\u9ed8\u8ba4\u4e3a \"/etc/crane/config.yaml\") -E/--end-time string \uff1a\u6307\u5b9a\u67e5\u8be2\u8be5\u65f6\u95f4\u4e4b\u524d\u7ed3\u675f\u7684\u4f5c\u4e1a\uff0c\u4f8b\uff1acacct -E=~2023-03-14T10:00:00 -o/--format string\uff1a \u6307\u5b9a\u8f93\u51fa\u683c\u5f0f\u3002\u7531\u767e\u5206\u53f7\uff08%\uff09\u540e\u63a5\u4e00\u4e2a\u5b57\u7b26\u6216\u5b57\u7b26\u4e32\u6807\u8bc6\u3002 \u5728 % \u548c\u683c\u5f0f\u5b57\u7b26/\u5b57\u7b26\u4e32\u4e4b\u95f4\u7528\u70b9\uff08.\uff09\u548c\u6570\u5b57\uff0c\u53ef\u6307\u5b9a\u5b57\u6bb5\u7684\u6700\u5c0f\u5bbd\u5ea6\u3002\u652f\u6301\u7684\u683c\u5f0f\u6807\u8bc6\u7b26\u6216\u5b57\u7b26\u4e32\uff08\u4e0d\u533a\u5206\u5927\u5c0f\u5199\uff09\uff1a %a/%Account\uff1a \u663e\u793a\u4f5c\u4e1a\u5173\u8054\u7684\u8d26\u6237 %c/%AllocCpus\uff1a \u663e\u793a\u4f5c\u4e1a\u5df2\u5206\u914d\u7684 CPU \u6570\u91cf %e/%CpuPerNode\uff1a \u663e\u793a\u4f5c\u4e1a\u6bcf\u4e2a\u8282\u70b9\u8bf7\u6c42\u7684 CPU \u6570\u91cf %h/%ElapsedTime\uff1a \u663e\u793a\u4f5c\u4e1a\u81ea\u542f\u52a8\u4ee5\u6765\u7684\u5df2\u7528\u65f6\u95f4 %j/%JobId\uff1a \u663e\u793a\u4f5c\u4e1a ID %k/%Comment\uff1a \u663e\u793a\u4f5c\u4e1a\u7684\u5907\u6ce8 %l/%NodeList\uff1a \u663e\u793a\u4f5c\u4e1a\u6b63\u5728\u8fd0\u884c\u7684\u8282\u70b9\u5217\u8868 %m/%TimeLimit\uff1a \u663e\u793a\u4f5c\u4e1a\u7684\u65f6\u95f4\u9650\u5236 %n/%MemPerNode\uff1a \u663e\u793a\u4f5c\u4e1a\u6bcf\u4e2a\u8282\u70b9\u8bf7\u6c42\u7684\u5185\u5b58\u91cf %N/%NodeNum\uff1a \u663e\u793a\u4f5c\u4e1a\u8bf7\u6c42\u7684\u8282\u70b9\u6570\u91cf %n/%Name\uff1a \u663e\u793a\u4f5c\u4e1a\u540d\u79f0 %P/%Partition\uff1a \u663e\u793a\u4f5c\u4e1a\u8fd0\u884c\u6240\u5728\u7684\u5206\u533a %p/%Priority\uff1a \u663e\u793a\u4f5c\u4e1a\u7684\u4f18\u5148\u7ea7 %Q/%QOS \uff1a\u663e\u793a\u4f5c\u4e1a\u7684\u670d\u52a1\u8d28\u91cf\uff08QoS\uff09\u7ea7\u522b %R/%Reason\uff1a \u663e\u793a\u4f5c\u4e1a\u6302\u8d77\u7684\u539f\u56e0 %r/%ReqNodes\uff1a \u663e\u793a\u4f5c\u4e1a\u8bf7\u6c42\u7684\u8282\u70b9 %S/%StartTime\uff1a \u663e\u793a\u4f5c\u4e1a\u7684\u5f00\u59cb\u65f6\u95f4 %s/%SubmitTime\uff1a \u663e\u793a\u4f5c\u4e1a\u7684\u63d0\u4ea4\u65f6\u95f4 %t/%State\uff1a \u663e\u793a\u4f5c\u4e1a\u7684\u5f53\u524d\u72b6\u6001 %T/%JobType\uff1a \u663e\u793a\u4f5c\u4e1a\u7c7b\u578b %u/%Uid \uff1a**\u663e\u793a\u4f5c\u4e1a\u7684 UID %U/%User\uff1a \u663e\u793a\u63d0\u4ea4\u4f5c\u4e1a\u7684\u7528\u6237 %x/%ExcludeNodes\uff1a \u663e\u793a\u4f5c\u4e1a\u6392\u9664\u7684\u8282\u70b9 \u6bcf\u4e2a\u683c\u5f0f\u6807\u8bc6\u7b26\u6216\u5b57\u7b26\u4e32\u53ef\u7528\u5bbd\u5ea6\u8bf4\u660e\u7b26\u4fee\u6539\uff08\u5982 \"%.5j\" \uff09\u3002 \u82e5\u6307\u5b9a\u5bbd\u5ea6\uff0c\u5219\u4f1a\u88ab\u683c\u5f0f\u5316\u4e3a\u81f3\u5c11\u8fbe\u5230\u8be5\u5bbd\u5ea6\u3002 \u82e5\u683c\u5f0f\u65e0\u6548\u6216\u65e0\u6cd5\u8bc6\u522b\uff0c\u7a0b\u5e8f\u4f1a\u62a5\u9519\u5e76\u7ec8\u6b62\u3002 \u4f8b\uff1a--format \"%.5j %.20n %t\" \u4f1a\u8f93\u51fa\u4f5c\u4e1a ID\uff08\u6700\u5c0f\u5bbd\u5ea6 5\uff09\u3001\u540d\u79f0\uff08\u6700\u5c0f\u5bbd\u5ea6 20\uff09\u548c\u72b6\u6001\u3002 -F/-full \uff1a\u663e\u793a\u5b8c\u6574\u4fe1\u606f -h/--help : \u663e\u793a\u5e2e\u52a9 -j/--job string \uff1a\u6307\u5b9a\u67e5\u8be2\u4f5c\u4e1a\u53f7\uff0c\u6307\u5b9a\u591a\u4e2a\u4f5c\u4e1a\u53f7\u65f6\u7528\u9017\u53f7\u9694\u5f00\u3002\u5982 -j=2,3,4 --json \uff1ajson\u683c\u5f0f\u8f93\u51fa\u547d\u4ee4\u6267\u884c\u7ed3\u679c -m/--max-lines uint32 \uff1a\u6307\u5b9a\u8f93\u51fa\u7ed3\u679c\u7684\u6700\u5927\u6761\u6570\u3002\u5982-m=500\u8868\u793a\u6700\u591a\u8f93\u51fa500\u884c\u67e5\u8be2\u7ed3\u679c -n/ --name string \uff1a\u6307\u5b9a\u67e5\u8be2\u4f5c\u4e1a\u540d\uff0c\u6307\u5b9a\u591a\u4e2a\u4f5c\u4e1a\u540d\u65f6\u7528\u9017\u53f7\u9694\u5f00 -N/--no header \uff1a\u8f93\u51fa\u9690\u85cf\u8868\u5934 -p/--partition string \uff1a\u6307\u5b9a\u8981\u67e5\u770b\u7684\u5206\u533a\uff0c\u591a\u4e2a\u5206\u533a\u540d\u7528\u9017\u53f7\u9694\u5f00\uff0c\u9ed8\u8ba4\u4e3a\u5168\u90e8 -q/--qos string \uff1a\u6307\u5b9a\u8981\u67e5\u770b\u7684Qos\uff0c\u591a\u4e2aQos\u7528\u9017\u53f7\u9694\u5f00\uff0c\u9ed8\u8ba4\u4e3a\u5168\u90e8 -S/--start-time string \uff1a\u7b5b\u9009\u5f00\u59cb\u65f6\u95f4\u5728\u7279\u5b9a\u65f6\u95f4\u6bb5\u5185\u7684\u4f5c\u4e1a\uff0c\u53ef\u4f7f\u7528\u95ed\u533a\u95f4\uff08\u65f6\u95f4\u683c\u5f0f\uff1a2024-01-02T15:04:05~2024-01-11T11:12:41 \uff09\u6216\u534a\u5f00\u533a\u95f4\uff08\u65f6\u95f4\u683c\u5f0f\uff1a2024-01-02T15:04:05~ \u6216 ~2024-01-11T11:12:41 \uff09 -t/--state string \uff1a\u6307\u5b9a\u8981\u67e5\u770b\u7684\u4f5c\u4e1a\u72b6\u6001\uff0c\u652f\u6301\u7684\u72b6\u6001\uff1apending(p)\uff08\u6302\u8d77 \uff09\u3001running\u00ae\uff08\u8fd0\u884c\u4e2d \uff09\u3001completed\u00a9\uff08\u5df2\u5b8c\u6210 \uff09\u3001failed(f)\uff08\u5931\u8d25 \uff09\u3001cancelled(x)\uff08\u5df2\u53d6\u6d88 \uff09\u3001time-limit-exceeded(t)\uff08\u8d85\u65f6 \uff09\u3001all\uff08\u6240\u6709 \uff09\u3002\uff08\u9ed8\u8ba4 \u201call\u201d \uff09 -s/--submit-time string \uff1a\u7b5b\u9009\u63d0\u4ea4\u65f6\u95f4\u5728\u7279\u5b9a\u65f6\u95f4\u6bb5\u5185\u7684\u4f5c\u4e1a\uff0c\u53ef\u4f7f\u7528\u95ed\u533a\u95f4\uff08\u65f6\u95f4\u683c\u5f0f\uff1a2024-01-02T15:04:05~2024-01-11T11:12:41 \uff09\u6216\u534a\u5f00\u533a\u95f4\uff08\u65f6\u95f4\u683c\u5f0f\uff1a2024-01-02T15:04:05~ \u6216 ~2024-01-11T11:12:41 \uff09 -u/--user string \uff1a\u6307\u5b9a\u67e5\u8be2\u67d0\u4e2a\u7528\u6237\u7684\u4f5c\u4e1a\uff0c\u6307\u5b9a\u591a\u4e2a\u7528\u6237\u65f6\u7528\u9017\u53f7\u9694\u5f00 -v/--version\uff1a \u67e5\u8be2\u7248\u672c\u53f7 \u4f8b\uff1a cacct cacct - h cacct - N cacct - S = 2024 - 07 - 22 T10 : 00 : 00 ~ 2024 - 07 - 24 T10 : 00 : 00 cacct - E = 2024 - 07 - 22 T10 : 00 : 00 ~ 2024 - 07 - 24 T10 : 00 : 00 cacct - j = 30618 , 30619 , 30620 cacct - u = cranetest cacct - A = CraneTest cacct - m = 10 cacct - p GPU cacct - n = Test_Job cacct - o = \"%j %.10n %P %a %t\" cacct -A ROOT -m 10 cacct -m 10 -j 783925 ,783889 -t = c -F cacct -n test cacct -q test_qos cacct -m 10 -E = 2024 -10-08T10:00:00~2024-10-10T110:00:00 -p CPU -t c","title":"cacct"},{"location":"command/cacct/#cacct","text":"cacct\u53ef\u4ee5\u67e5\u770b\u961f\u5217\u4e2d\u7684\u4f5c\u4e1a\u4fe1\u606f\u3002 \u67e5\u770b\u96c6\u7fa4\u4e2d\u6240\u6709\u961f\u5217\u7684\u4f5c\u4e1a\u4fe1\u606f\uff08\u5305\u62ec\u6240\u6709\u72b6\u6001\uff09\uff0c\u9ed8\u8ba4\u8f93\u51fa100\u6761\u4fe1\u606f\u3002 cacct cacct\u8fd0\u884c\u7ed3\u679c\u5c55\u793a \u4e3b\u8981\u8f93\u51fa\u9879 TaskId \uff1a\u4f5c\u4e1a\u53f7 TaskName : \u4f5c\u4e1a\u540d Partition \uff1a\u4f5c\u4e1a\u6240\u5728\u5206\u533a Account \uff1a\u4f5c\u4e1a\u6240\u5c5e\u8d26\u6237 AllocCPUs \uff1a\u4f5c\u4e1a\u5206\u914d\u7684CPU\u6570\u91cf State \uff1a\u4f5c\u4e1a\u72b6\u6001 ExitCode \uff1a\u4f5c\u4e1a\u72b6\u6001\u7801","title":"cacct \u67e5\u770b\u4f5c\u4e1a\u4fe1\u606f"},{"location":"command/cacct/#_1","text":"-A/--account string \uff1a\u6307\u5b9a\u67e5\u8be2\u4f5c\u4e1a\u7684\u6240\u5c5e\u8d26\u6237\uff0c\u6307\u5b9a\u591a\u4e2a\u8d26\u6237\u65f6\u7528\u9017\u53f7\u9694\u5f00 -C/--config string\uff1a \u914d\u7f6e\u6587\u4ef6\u8def\u5f84(\u9ed8\u8ba4\u4e3a \"/etc/crane/config.yaml\") -E/--end-time string \uff1a\u6307\u5b9a\u67e5\u8be2\u8be5\u65f6\u95f4\u4e4b\u524d\u7ed3\u675f\u7684\u4f5c\u4e1a\uff0c\u4f8b\uff1acacct -E=~2023-03-14T10:00:00 -o/--format string\uff1a \u6307\u5b9a\u8f93\u51fa\u683c\u5f0f\u3002\u7531\u767e\u5206\u53f7\uff08%\uff09\u540e\u63a5\u4e00\u4e2a\u5b57\u7b26\u6216\u5b57\u7b26\u4e32\u6807\u8bc6\u3002 \u5728 % \u548c\u683c\u5f0f\u5b57\u7b26/\u5b57\u7b26\u4e32\u4e4b\u95f4\u7528\u70b9\uff08.\uff09\u548c\u6570\u5b57\uff0c\u53ef\u6307\u5b9a\u5b57\u6bb5\u7684\u6700\u5c0f\u5bbd\u5ea6\u3002\u652f\u6301\u7684\u683c\u5f0f\u6807\u8bc6\u7b26\u6216\u5b57\u7b26\u4e32\uff08\u4e0d\u533a\u5206\u5927\u5c0f\u5199\uff09\uff1a %a/%Account\uff1a \u663e\u793a\u4f5c\u4e1a\u5173\u8054\u7684\u8d26\u6237 %c/%AllocCpus\uff1a \u663e\u793a\u4f5c\u4e1a\u5df2\u5206\u914d\u7684 CPU \u6570\u91cf %e/%CpuPerNode\uff1a \u663e\u793a\u4f5c\u4e1a\u6bcf\u4e2a\u8282\u70b9\u8bf7\u6c42\u7684 CPU \u6570\u91cf %h/%ElapsedTime\uff1a \u663e\u793a\u4f5c\u4e1a\u81ea\u542f\u52a8\u4ee5\u6765\u7684\u5df2\u7528\u65f6\u95f4 %j/%JobId\uff1a \u663e\u793a\u4f5c\u4e1a ID %k/%Comment\uff1a \u663e\u793a\u4f5c\u4e1a\u7684\u5907\u6ce8 %l/%NodeList\uff1a \u663e\u793a\u4f5c\u4e1a\u6b63\u5728\u8fd0\u884c\u7684\u8282\u70b9\u5217\u8868 %m/%TimeLimit\uff1a \u663e\u793a\u4f5c\u4e1a\u7684\u65f6\u95f4\u9650\u5236 %n/%MemPerNode\uff1a \u663e\u793a\u4f5c\u4e1a\u6bcf\u4e2a\u8282\u70b9\u8bf7\u6c42\u7684\u5185\u5b58\u91cf %N/%NodeNum\uff1a \u663e\u793a\u4f5c\u4e1a\u8bf7\u6c42\u7684\u8282\u70b9\u6570\u91cf %n/%Name\uff1a \u663e\u793a\u4f5c\u4e1a\u540d\u79f0 %P/%Partition\uff1a \u663e\u793a\u4f5c\u4e1a\u8fd0\u884c\u6240\u5728\u7684\u5206\u533a %p/%Priority\uff1a \u663e\u793a\u4f5c\u4e1a\u7684\u4f18\u5148\u7ea7 %Q/%QOS \uff1a\u663e\u793a\u4f5c\u4e1a\u7684\u670d\u52a1\u8d28\u91cf\uff08QoS\uff09\u7ea7\u522b %R/%Reason\uff1a \u663e\u793a\u4f5c\u4e1a\u6302\u8d77\u7684\u539f\u56e0 %r/%ReqNodes\uff1a \u663e\u793a\u4f5c\u4e1a\u8bf7\u6c42\u7684\u8282\u70b9 %S/%StartTime\uff1a \u663e\u793a\u4f5c\u4e1a\u7684\u5f00\u59cb\u65f6\u95f4 %s/%SubmitTime\uff1a \u663e\u793a\u4f5c\u4e1a\u7684\u63d0\u4ea4\u65f6\u95f4 %t/%State\uff1a \u663e\u793a\u4f5c\u4e1a\u7684\u5f53\u524d\u72b6\u6001 %T/%JobType\uff1a \u663e\u793a\u4f5c\u4e1a\u7c7b\u578b %u/%Uid \uff1a**\u663e\u793a\u4f5c\u4e1a\u7684 UID %U/%User\uff1a \u663e\u793a\u63d0\u4ea4\u4f5c\u4e1a\u7684\u7528\u6237 %x/%ExcludeNodes\uff1a \u663e\u793a\u4f5c\u4e1a\u6392\u9664\u7684\u8282\u70b9 \u6bcf\u4e2a\u683c\u5f0f\u6807\u8bc6\u7b26\u6216\u5b57\u7b26\u4e32\u53ef\u7528\u5bbd\u5ea6\u8bf4\u660e\u7b26\u4fee\u6539\uff08\u5982 \"%.5j\" \uff09\u3002 \u82e5\u6307\u5b9a\u5bbd\u5ea6\uff0c\u5219\u4f1a\u88ab\u683c\u5f0f\u5316\u4e3a\u81f3\u5c11\u8fbe\u5230\u8be5\u5bbd\u5ea6\u3002 \u82e5\u683c\u5f0f\u65e0\u6548\u6216\u65e0\u6cd5\u8bc6\u522b\uff0c\u7a0b\u5e8f\u4f1a\u62a5\u9519\u5e76\u7ec8\u6b62\u3002 \u4f8b\uff1a--format \"%.5j %.20n %t\" \u4f1a\u8f93\u51fa\u4f5c\u4e1a ID\uff08\u6700\u5c0f\u5bbd\u5ea6 5\uff09\u3001\u540d\u79f0\uff08\u6700\u5c0f\u5bbd\u5ea6 20\uff09\u548c\u72b6\u6001\u3002 -F/-full \uff1a\u663e\u793a\u5b8c\u6574\u4fe1\u606f -h/--help : \u663e\u793a\u5e2e\u52a9 -j/--job string \uff1a\u6307\u5b9a\u67e5\u8be2\u4f5c\u4e1a\u53f7\uff0c\u6307\u5b9a\u591a\u4e2a\u4f5c\u4e1a\u53f7\u65f6\u7528\u9017\u53f7\u9694\u5f00\u3002\u5982 -j=2,3,4 --json \uff1ajson\u683c\u5f0f\u8f93\u51fa\u547d\u4ee4\u6267\u884c\u7ed3\u679c -m/--max-lines uint32 \uff1a\u6307\u5b9a\u8f93\u51fa\u7ed3\u679c\u7684\u6700\u5927\u6761\u6570\u3002\u5982-m=500\u8868\u793a\u6700\u591a\u8f93\u51fa500\u884c\u67e5\u8be2\u7ed3\u679c -n/ --name string \uff1a\u6307\u5b9a\u67e5\u8be2\u4f5c\u4e1a\u540d\uff0c\u6307\u5b9a\u591a\u4e2a\u4f5c\u4e1a\u540d\u65f6\u7528\u9017\u53f7\u9694\u5f00 -N/--no header \uff1a\u8f93\u51fa\u9690\u85cf\u8868\u5934 -p/--partition string \uff1a\u6307\u5b9a\u8981\u67e5\u770b\u7684\u5206\u533a\uff0c\u591a\u4e2a\u5206\u533a\u540d\u7528\u9017\u53f7\u9694\u5f00\uff0c\u9ed8\u8ba4\u4e3a\u5168\u90e8 -q/--qos string \uff1a\u6307\u5b9a\u8981\u67e5\u770b\u7684Qos\uff0c\u591a\u4e2aQos\u7528\u9017\u53f7\u9694\u5f00\uff0c\u9ed8\u8ba4\u4e3a\u5168\u90e8 -S/--start-time string \uff1a\u7b5b\u9009\u5f00\u59cb\u65f6\u95f4\u5728\u7279\u5b9a\u65f6\u95f4\u6bb5\u5185\u7684\u4f5c\u4e1a\uff0c\u53ef\u4f7f\u7528\u95ed\u533a\u95f4\uff08\u65f6\u95f4\u683c\u5f0f\uff1a2024-01-02T15:04:05~2024-01-11T11:12:41 \uff09\u6216\u534a\u5f00\u533a\u95f4\uff08\u65f6\u95f4\u683c\u5f0f\uff1a2024-01-02T15:04:05~ \u6216 ~2024-01-11T11:12:41 \uff09 -t/--state string \uff1a\u6307\u5b9a\u8981\u67e5\u770b\u7684\u4f5c\u4e1a\u72b6\u6001\uff0c\u652f\u6301\u7684\u72b6\u6001\uff1apending(p)\uff08\u6302\u8d77 \uff09\u3001running\u00ae\uff08\u8fd0\u884c\u4e2d \uff09\u3001completed\u00a9\uff08\u5df2\u5b8c\u6210 \uff09\u3001failed(f)\uff08\u5931\u8d25 \uff09\u3001cancelled(x)\uff08\u5df2\u53d6\u6d88 \uff09\u3001time-limit-exceeded(t)\uff08\u8d85\u65f6 \uff09\u3001all\uff08\u6240\u6709 \uff09\u3002\uff08\u9ed8\u8ba4 \u201call\u201d \uff09 -s/--submit-time string \uff1a\u7b5b\u9009\u63d0\u4ea4\u65f6\u95f4\u5728\u7279\u5b9a\u65f6\u95f4\u6bb5\u5185\u7684\u4f5c\u4e1a\uff0c\u53ef\u4f7f\u7528\u95ed\u533a\u95f4\uff08\u65f6\u95f4\u683c\u5f0f\uff1a2024-01-02T15:04:05~2024-01-11T11:12:41 \uff09\u6216\u534a\u5f00\u533a\u95f4\uff08\u65f6\u95f4\u683c\u5f0f\uff1a2024-01-02T15:04:05~ \u6216 ~2024-01-11T11:12:41 \uff09 -u/--user string \uff1a\u6307\u5b9a\u67e5\u8be2\u67d0\u4e2a\u7528\u6237\u7684\u4f5c\u4e1a\uff0c\u6307\u5b9a\u591a\u4e2a\u7528\u6237\u65f6\u7528\u9017\u53f7\u9694\u5f00 -v/--version\uff1a \u67e5\u8be2\u7248\u672c\u53f7 \u4f8b\uff1a cacct cacct - h cacct - N cacct - S = 2024 - 07 - 22 T10 : 00 : 00 ~ 2024 - 07 - 24 T10 : 00 : 00 cacct - E = 2024 - 07 - 22 T10 : 00 : 00 ~ 2024 - 07 - 24 T10 : 00 : 00 cacct - j = 30618 , 30619 , 30620 cacct - u = cranetest cacct - A = CraneTest cacct - m = 10 cacct - p GPU cacct - n = Test_Job cacct - o = \"%j %.10n %P %a %t\" cacct -A ROOT -m 10 cacct -m 10 -j 783925 ,783889 -t = c -F cacct -n test cacct -q test_qos cacct -m 10 -E = 2024 -10-08T10:00:00~2024-10-10T110:00:00 -p CPU -t c","title":"\u4e3b\u8981\u53c2\u6570"},{"location":"command/cacctmgr/","text":"cacctmgr \u7ba1\u7406\u7528\u6237/\u8d26\u6237\u4fe1\u606f Warning This document is a work in progress and may contain inaccuracies or incomplete information. Please refer to the official documentation for the most accurate and up-to-date details. cacctmgr \u53ef\u4ee5\u7ba1\u7406\u8d26\u6237/\u7528\u6237\u4fe1\u606f\uff0c\u5305\u62ec\u6dfb\u52a0\u8d26\u6237/\u7528\u6237\u3001\u5220\u9664\u8d26\u6237/\u7528\u6237\u3001\u67e5\u627e\u8d26\u6237/\u7528\u6237\u3002 CraneSched\u4f5c\u4e1a\u8c03\u5ea6\u7cfb\u7edf\u4e2d\u6709\u56db\u4e2a\u7528\u6237\u89d2\u8272\uff1a \u7cfb\u7edf\u7ba1\u7406\u5458\uff08Admin\uff09 \uff1a\u4e00\u822c\u4e3aroot\u7528\u6237\uff0c\u53ef\u4ee5\u589e\u5220\u67e5\u6539\u4efb\u4f55\u8d26\u6237\u548c\u7528\u6237\u4fe1\u606f \u5e73\u53f0\u7ba1\u7406\u5458\uff08Operator\uff09 \uff1a\u5bf9\u8d26\u6237\u7cfb\u7edf\u5177\u6709\u5b8c\u5168\u6743\u9650 \u8d26\u6237\u8c03\u5ea6\u5458\uff08Coordinator\uff09 \uff1a\u5bf9\u4e0e\u81ea\u8eab\u540c\u4e00\u8d26\u6237\u4e0b\u7684\u7528\u6237\u4ee5\u53ca\u5bf9\u81ea\u8eab\u8d26\u6237\u7684\u5b50\u8d26\u6237\u5177\u6709\u64cd\u4f5c\u6743\u9650\uff0c\u5305\u62ec\u6dfb\u52a0\u7528\u6237 \u666e\u901a\u7528\u6237(None) \uff1a \u9664\u4e86\u67e5\u8be2\u529f\u80fd\u5916\u4e0d\u5177\u5907\u5176\u4ed6\u6743\u9650\uff0c\u80fd\u591f\u67e5\u8be2\u4e0e\u81ea\u8eab\u540c\u4e00\u8d26\u6237\u4e0b\u7684\u4fe1\u606f\uff0c\u4e0d\u53ef\u4ee5\u4fee\u6539\u6240\u6709\u7528\u6237\u548c\u8d26\u6237\u4fe1\u606f \u4e3b\u8981\u53c2\u6570 -h/--help : \u663e\u793a\u5e2e\u52a9 --json \u4ee5 json \u683c\u5f0f\u8f93\u51fa -C/--config string \uff1a \u914d\u7f6e\u6587\u4ef6\u8def\u5f84\uff08\u9ed8\u8ba4\u4e3a\"/etc/crane/config.yaml\"\uff09 -v/ --version cacctmgr \u547d\u4ee4\u7684\u7248\u672c \u4e3b\u8981\u547d\u4ee4 help \uff1a\u663e\u793a\u5e2e\u52a9 add : \u6dfb\u52a0\u5b9e\u4f53\uff08\u5b9e\u4f53\u5305\u62ecQoS\u3001\u8d26\u6237\u3001\u7528\u6237\uff09 block \uff1a\u7981\u7528\u8be5\u5b9e\u4f53\uff0c\u4f7f\u5176\u65e0\u6cd5\u4f7f\u7528 delete \uff1a\u5220\u9664\u5b9e\u4f53 modify \uff1a\u4fee\u6539\u5b9e\u4f53 show \uff1a\u663e\u793a\u4e00\u7c7b\u5b9e\u4f53\u7684\u6240\u6709\u8bb0\u5f55 unblock \uff1a\u89e3\u9664\u7981\u7528 completion\uff1a \u81ea\u52a8\u8865\u5168\u9002\u7528\u4e8e\u6307\u5b9a Shell \u7684\u811a\u672c 1. \u6dfb\u52a0 1.1 \u6dfb\u52a0qos \u4e3b\u8981\u53c2\u6570 -D/--description string \uff1a qos\u63cf\u8ff0\u4fe1\u606f -h/--help \uff1a \u5e2e\u52a9 -c/--max_cpus_per_user uint32 \uff1a \u9ed8\u8ba4\u4e3a10 -J/--max_jobs_per_user uint32 -T/--max_time_limit_per_task uint \uff1a \u4ee5\u79d2\u4e3a\u5355\u4f4d\u7684\u65f6\u95f4\uff08\u9ed8\u8ba43600\uff09 -N/--name string \uff1aqos\u7684\u540d\u79f0 -P/--priority uint32 \uff1a\u9ed8\u8ba4\u4e3a1000 \u4f8b\uff1a SQL cacctmgr add qos -N=test-qos -D=\"test qos\" 1. 2 \u6dfb\u52a0\u8d26\u6237 \u4e3b\u8981\u53c2\u6570 -Q/--default_qos string \uff1a \u8d26\u6237\u9ed8\u8ba4qos -D/--description string \uff1a\u8d26\u53f7\u63cf\u8ff0\u4fe1\u606f -h/--help \uff1a \u5e2e\u52a9 -N/--name string \uff1a \u8d26\u6237\u7684\u540d\u79f0 -P/--parent string\uff1a \u6b64\u8d26\u6237\u7684\u7236\u8d26\u6237 -p/--partition strings \uff1a \u8be5\u8d26\u53f7\u53ef\u4ee5\u8bbf\u95ee\u7684\u5206\u533a\u5217\u8868 -q/--qos_list strings \uff1a\u8d26\u53f7\u53ef\u4ee5\u8bbf\u95ee\u7684qos\u5217\u8868 \u4f8b\uff1a\uff08\u6dfb\u52a0\u8d26\u6237PKU\u5e76\u6dfb\u52a0PKU\u7684\u5b50\u8d26\u6237ComputingCentre\uff09 cacctmgr add account - N = PKU - D = school - p = CPU , GPU - q = test - qos cacctmgr add account - N = ComputingCentre - D = department - P = PKU 1. 3 \u6dfb\u52a0\u7528\u6237 \u7cfb\u7edf\u7ba1\u7406\u5458\u53ef\u4ee5\u6dfb\u52a0\u4efb\u610f\u8d26\u6237\u7684\u7528\u6237\uff0c \u8d26\u6237\u7ba1\u7406\u5458\u53ef\u4ee5\u6dfb\u52a0\u540c\u4e00\u8d26\u53f7\u4e0b\u7684\u65b0\u7528\u6237\u3002 \u6dfb\u52a0\u7684\u7528\u6237\u9700\u8981\u5148\u6709uid\uff08\u5148\u4f7f\u7528useradd\u5728linux\u7cfb\u7edf\u6dfb\u52a0\u8be5\u7528\u6237\uff09 \u3002 \u4e3b\u8981\u53c2\u6570 -A/--account string \uff1a \u6b64\u7528\u6237\u6240\u5c5e\u7684\u7236\u8d26\u6237 -c/--coordinate \uff1a\u8bbe\u7f6e\u7528\u6237\u662f\u5426\u4e3a\u7236\u8d26\u53f7\u7684\u8d26\u6237\u8c03\u5ea6\u5458\uff08coordinator\uff09 -h/--help \uff1a \u5e2e\u52a9 -L/--level string \uff1a\u8bbe\u7f6e\u7528\u6237\u6743\u9650(none/operator/admin) (\u9ed8\u8ba4\u4e3a \"none\") -N/--name string \uff1a \u7528\u6237\u7684\u540d\u79f0 -p/--partition strings \uff1a \u8be5\u7528\u6237\u53ef\u4ee5\u8bbf\u95ee\u7684\u5206\u533a\u5217\u8868 \u4f8b\uff1a SQL useradd CS Plaintext cacctmgr add user -N=CS -A=PKU -p=CPU,GPU -L=admin # -p\u53c2\u6570\u6307\u660e\u7528\u6237\u53ef\u7528\u5206\u533a\u4e3aCPU\u548cGPU\uff08\u5206\u533a\u5fc5\u987b\u540c\u65f6\u4e3a\u7236\u8d26\u6237PKU\u7684\u53ef\u7528\u5206\u533a\uff09\uff0c\u5206\u533a\u7684allowed_qos_list\u4e0edefault_qos\u4fe1\u606f\u4e0d\u652f\u6301\u6307\u5b9a\uff0c\u9ed8\u8ba4\u4ece\u7236\u8d26\u6237PKU\u4e2d\u7ee7\u627f Plaintext cacctmgr add user -N=lab -A=ComputingCentre # \u672a\u6307\u660e-p\u53c2\u6570\uff0cpartition\u4e0eqos\u4fe1\u606f\u90fd\u4ece\u7236\u8d26\u6237ComputingCentre\u4e2d\u7ee7\u627f 2. \u5220\u9664 2.1 \u5220\u9664\u7528\u6237 cacctmgr delete user -N lab 2.2 \u5220\u9664\u8d26\u6237 \u4ec5\u7cfb\u7edf\u7ba1\u7406\u5458\u53ef\u4ee5\u5220\u9664\u8d26\u6237\uff0c\u5220\u9664\u8d26\u6237\u65f6\u4f1a\u68c0\u67e5\u8d26\u6237\u4e0b\u662f\u5426\u8fd8\u6709\u5b50\u8d26\u6237\u6216\u8005\u7528\u6237\uff0c\u5982\u679c\u6709\u5219\u4e0d\u5141\u8bb8\u5220\u9664\uff0c\u9632\u6b62\u4ea7\u751f\u6e38\u79bb\u7684\u7528\u6237\uff0c\u9700\u8981\u5c06\u5176\u5b50\u8d26\u6237\u548c\u7528\u6237\u90fd\u8bbe\u7f6e\u5728\u65b0\u8d26\u6237\u4e0b\u3002 cacctmgr delete account -N ComputingCentre 2 .3 \u5220\u9664qos cacctmgr delete qos -N test-qos 3. \u7981\u7528 3.1 \u963b\u6b62\u7528\u6237\u6216\u8d26\u6237 \u4e3b\u8981\u547d\u4ee4 account \uff1a \u963b\u6b62\u8d26\u6237 user \uff1a\u963b\u6b62\u8d26\u6237\u4e0b\u7684\u7528\u6237 cacctmgr block user lab -A=ComputingCentre cacctmgr block account ComputingCentre 4. \u89e3\u7981 4.1 \u89e3\u9664\u963b\u6b62\u7528\u6237\u6216\u8d26\u6237 \u4e3b\u8981\u547d\u4ee4 account \uff1a \u89e3\u9664\u963b\u6b62\u8d26\u6237 user \uff1a\u89e3\u9664\u963b\u6b62\u8d26\u6237\u4e0b\u7684\u7528\u6237 cacctmgr unblock user lab -A=ComputingCentre cacctmgr unblock account ComputingCentre 5. \u67e5\u8be2 5.1 \u67e5\u627e\u7528\u6237 \u6240\u6709\u7528\u6237\u5747\u53ef\u4ee5\u4f7f\u7528\u67e5\u8be2\u529f\u80fd cacctmgr find user lab cacctmgr find user CS cacctmgr show user 5.2 \u67e5\u627e\u8d26\u6237 cacctmgr find account ComputingCentre cacctmgr find account PKU cacctmgr show account 5.3 \u67e5\u627eqos cacctmgr find qos test-qos cacctmgr show qos 6. \u4fee\u6539 6.1 \u4fee\u6539\u8d26\u6237 \u7cfb\u7edf\u7ba1\u7406\u5458\u53ef\u4ee5\u4fee\u6539\u4efb\u610f\u4fe1\u606f\uff0c \u8d26\u6237\u7ba1\u7406\u5458\u53ef\u4ee5\u4fee\u6539\u672c\u8eab\u8d26\u6237\u7684\u4fe1\u606f\uff0c\u4f46\u4e0d\u80fd\u66f4\u6539\u8d26\u6237\u7684\u7236\u8d26\u6237\u3002 \u4e3b\u8981\u53c2\u6570 --add_allowed_partition string \uff1a\u5c06\u65b0\u9879\u6dfb\u52a0\u5230\u5141\u8bb8\u7684\u5206\u533a\u5217\u8868 --add_allowed_qos_list strings \uff1a\u5c06\u65b0\u9879\u6dfb\u52a0\u5230\u5141\u8bb8\u7684qos\u5217\u8868 -Q/--default_qos string \uff1a \u4fee\u6539\u8d26\u6237\u9ed8\u8ba4qos --delete_allowed_partition string \uff1a\u4ece\u5141\u8bb8\u7684\u5206\u533a\u5217\u8868\u4e2d\u5220\u9664\u7279\u5b9a\u9879\u76ee --delete_allowed_qos_list strings \uff1a\u4ece\u5141\u8bb8\u7684qos\u5217\u8868\u4e2d\u5220\u9664\u7279\u5b9a\u9879 -D/--description string \uff1a\u4fee\u6539\u8d26\u6237\u7684\u63cf\u8ff0\u4fe1\u606f -F/--force \uff1a \u5f3a\u5236\u64cd\u4f5c -h/--help \uff1a \u5e2e\u52a9 -N/--name string \uff1a\u9700\u8981\u8fdb\u884c\u4fee\u6539\u7684\u8d26\u6237\u540d\u79f0 --set_allowed_partition strings \uff1a\u8bbe\u7f6e\u5141\u8bb8\u7684\u5206\u533a\u5217\u8868\u7684\u5185\u5bb9 --set_allowed_qos_list strings \uff1a\u8bbe\u7f6e\u5141\u8bb8\u7684qos\u5217\u8868\u7684\u5185\u5bb9 \u4f8b\uff1a Plaintext cacctmgr modify account -N=ComputingCentre -D=\"Located in PKU\" 6.2 \u4fee\u6539\u7528\u6237 \u7cfb\u7edf\u7ba1\u7406\u5458\u53ef\u4ee5\u4fee\u6539\u4efb\u610f\u4fe1\u606f\uff0c \u8d26\u6237\u7ba1\u7406\u5458\u53ef\u4ee5\u4fee\u6539\u540c\u8d26\u6237\u4e0b\u7528\u6237\u7684\u4fe1\u606f\uff0c\u4f46\u4e0d\u80fd\u66f4\u6539\u7528\u6237\u7684\u8d26\u6237\u3002 \u4e3b\u8981\u53c2\u6570 -A/--account string \uff1a\u8bbe\u7f6e\u7528\u6237\u4f7f\u7528\u7684\u5e10\u53f7 --add_allowed_partition strings \uff1a\u5c06\u65b0\u9879\u6dfb\u52a0\u5230\u5141\u8bb8\u7684\u5206\u533a\u5217\u8868 --add_allowed_qos_list string \uff1a\u5c06\u65b0\u9879\u6dfb\u52a0\u5230\u5141\u8bb8\u7684qos\u5217\u8868 -L/--admin_level string \uff1a\u8bbe\u7f6e\u7528\u6237\u7ba1\u7406\u6743\u9650\uff08none/operator/admin\uff09 -Q/--default_qos string \uff1a \u4fee\u6539\u8d26\u6237\u9ed8\u8ba4qos --delete_allowed_partition strings \uff1a\u4ece\u5141\u8bb8\u7684\u5206\u533a\u5217\u8868\u4e2d\u5220\u9664\u7279\u5b9a\u9879\u76ee --delete_allowed_qos_list string \uff1a\u4ece\u5141\u8bb8\u7684qos\u5217\u8868\u4e2d\u5220\u9664\u7279\u5b9a\u9879\u76ee -F/--force \uff1a \u5f3a\u5236\u64cd\u4f5c -h/--help \uff1a \u5e2e\u52a9 -N/--name string \uff1a\u9700\u8981\u8fdb\u884c\u4fee\u6539\u7684\u7528\u6237\u540d\u79f0 -D/--default-account string \uff1a\u4fee\u6539\u7528\u6237\u7684\u9ed8\u8ba4\u8d26\u53f7 -p/--partition string \uff1a\u88ab\u4fee\u6539\u7684\u5206\u533a\uff0c\u5982\u679c\u4e0d\u663e\u5f0f\u8bbe\u7f6e\u8be5\u53c2\u6570\uff0c\u9ed8\u8ba4\u4fee\u6539\u6240\u6709\u5206\u533a --set_allowed_partition strings \uff1a\u8bbe\u7f6e\u5141\u8bb8\u7684\u5206\u533a\u5217\u8868\u7684\u5185\u5bb9 --set_allowed_qos_list strings \uff1a\u8bbe\u7f6e\u5141\u8bb8\u7684qos\u5217\u8868\u7684\u5185\u5bb9 \u4f8b SQL cacctmgr modify user -N=lab -A=ComputingCentre -L=operator --delete-allowed-partition GPU 6.3 \u4fee\u6539qos \u4e3b\u8981\u53c2\u6570 -D/--description string \uff1a\u4fee\u6539qos\u7684\u63cf\u8ff0\u4fe1\u606f -h/--help \uff1a \u5e2e\u52a9 -c/--max_cpus_per_user uint32 \uff1a(\u9ed8\u8ba410) -J/--max_jobs_per_user uint32 -T/--max_time_limit_per_task uint \uff1a\u4ee5\u79d2\u4e3a\u5355\u4f4d\u7684\u65f6\u95f4\uff08\u9ed8\u8ba4 3600\uff09 -N/--name string \uff1a \u9700\u8981\u8fdb\u884c\u4fee\u6539\u7684qos\u540d\u79f0 -P/--priority uint32 \uff1a(\u9ed8\u8ba41000) 7 . \u663e\u793a 7.1 \u663e\u793a \u8d26\u6237\u6811 \u7cfb\u7edf\u7ba1\u7406\u5458\u4f1a\u663e\u793a\u6570\u636e\u5e93\u6240\u6709\u6839\u8d26\u6237\u7684\u8d26\u6237\u6811\uff0c \u8d26\u6237\u7ba1\u7406\u5458\u548c\u7528\u6237\u4f1a\u663e\u793a\u672c\u8eab\u8d26\u6237\u7684\u8d26\u6237\u6811\u3002 cacctmgr show accounts 7.2 \u663e\u793a\u7528\u6237 \u7cfb\u7edf\u7ba1\u7406\u5458\u4f1a\u663e\u793a\u6240\u6709\u7528\u6237\uff0c \u8d26\u6237\u7ba1\u7406\u5458\u548c\u7528\u6237\u4f1a\u663e\u793a\u540c\u4e00\u8d26\u6237\u4e0b\u7684\u6240\u6709\u7528\u6237\u3002 cacctmgr show users","title":"cacctmgr"},{"location":"command/cacctmgr/#cacctmgr","text":"Warning This document is a work in progress and may contain inaccuracies or incomplete information. Please refer to the official documentation for the most accurate and up-to-date details. cacctmgr \u53ef\u4ee5\u7ba1\u7406\u8d26\u6237/\u7528\u6237\u4fe1\u606f\uff0c\u5305\u62ec\u6dfb\u52a0\u8d26\u6237/\u7528\u6237\u3001\u5220\u9664\u8d26\u6237/\u7528\u6237\u3001\u67e5\u627e\u8d26\u6237/\u7528\u6237\u3002 CraneSched\u4f5c\u4e1a\u8c03\u5ea6\u7cfb\u7edf\u4e2d\u6709\u56db\u4e2a\u7528\u6237\u89d2\u8272\uff1a \u7cfb\u7edf\u7ba1\u7406\u5458\uff08Admin\uff09 \uff1a\u4e00\u822c\u4e3aroot\u7528\u6237\uff0c\u53ef\u4ee5\u589e\u5220\u67e5\u6539\u4efb\u4f55\u8d26\u6237\u548c\u7528\u6237\u4fe1\u606f \u5e73\u53f0\u7ba1\u7406\u5458\uff08Operator\uff09 \uff1a\u5bf9\u8d26\u6237\u7cfb\u7edf\u5177\u6709\u5b8c\u5168\u6743\u9650 \u8d26\u6237\u8c03\u5ea6\u5458\uff08Coordinator\uff09 \uff1a\u5bf9\u4e0e\u81ea\u8eab\u540c\u4e00\u8d26\u6237\u4e0b\u7684\u7528\u6237\u4ee5\u53ca\u5bf9\u81ea\u8eab\u8d26\u6237\u7684\u5b50\u8d26\u6237\u5177\u6709\u64cd\u4f5c\u6743\u9650\uff0c\u5305\u62ec\u6dfb\u52a0\u7528\u6237 \u666e\u901a\u7528\u6237(None) \uff1a \u9664\u4e86\u67e5\u8be2\u529f\u80fd\u5916\u4e0d\u5177\u5907\u5176\u4ed6\u6743\u9650\uff0c\u80fd\u591f\u67e5\u8be2\u4e0e\u81ea\u8eab\u540c\u4e00\u8d26\u6237\u4e0b\u7684\u4fe1\u606f\uff0c\u4e0d\u53ef\u4ee5\u4fee\u6539\u6240\u6709\u7528\u6237\u548c\u8d26\u6237\u4fe1\u606f","title":"cacctmgr \u7ba1\u7406\u7528\u6237/\u8d26\u6237\u4fe1\u606f"},{"location":"command/cacctmgr/#_1","text":"-h/--help : \u663e\u793a\u5e2e\u52a9 --json \u4ee5 json \u683c\u5f0f\u8f93\u51fa -C/--config string \uff1a \u914d\u7f6e\u6587\u4ef6\u8def\u5f84\uff08\u9ed8\u8ba4\u4e3a\"/etc/crane/config.yaml\"\uff09 -v/ --version cacctmgr \u547d\u4ee4\u7684\u7248\u672c","title":"\u4e3b\u8981\u53c2\u6570"},{"location":"command/cacctmgr/#_2","text":"help \uff1a\u663e\u793a\u5e2e\u52a9 add : \u6dfb\u52a0\u5b9e\u4f53\uff08\u5b9e\u4f53\u5305\u62ecQoS\u3001\u8d26\u6237\u3001\u7528\u6237\uff09 block \uff1a\u7981\u7528\u8be5\u5b9e\u4f53\uff0c\u4f7f\u5176\u65e0\u6cd5\u4f7f\u7528 delete \uff1a\u5220\u9664\u5b9e\u4f53 modify \uff1a\u4fee\u6539\u5b9e\u4f53 show \uff1a\u663e\u793a\u4e00\u7c7b\u5b9e\u4f53\u7684\u6240\u6709\u8bb0\u5f55 unblock \uff1a\u89e3\u9664\u7981\u7528 completion\uff1a \u81ea\u52a8\u8865\u5168\u9002\u7528\u4e8e\u6307\u5b9a Shell \u7684\u811a\u672c","title":"\u4e3b\u8981\u547d\u4ee4"},{"location":"command/cacctmgr/#1","text":"","title":"1. \u6dfb\u52a0"},{"location":"command/cacctmgr/#11-qos","text":"","title":"1.1 \u6dfb\u52a0qos"},{"location":"command/cacctmgr/#_3","text":"-D/--description string \uff1a qos\u63cf\u8ff0\u4fe1\u606f -h/--help \uff1a \u5e2e\u52a9 -c/--max_cpus_per_user uint32 \uff1a \u9ed8\u8ba4\u4e3a10 -J/--max_jobs_per_user uint32 -T/--max_time_limit_per_task uint \uff1a \u4ee5\u79d2\u4e3a\u5355\u4f4d\u7684\u65f6\u95f4\uff08\u9ed8\u8ba43600\uff09 -N/--name string \uff1aqos\u7684\u540d\u79f0 -P/--priority uint32 \uff1a\u9ed8\u8ba4\u4e3a1000 \u4f8b\uff1a SQL cacctmgr add qos -N=test-qos -D=\"test qos\"","title":"\u4e3b\u8981\u53c2\u6570"},{"location":"command/cacctmgr/#12","text":"","title":"1.2 \u6dfb\u52a0\u8d26\u6237"},{"location":"command/cacctmgr/#_4","text":"-Q/--default_qos string \uff1a \u8d26\u6237\u9ed8\u8ba4qos -D/--description string \uff1a\u8d26\u53f7\u63cf\u8ff0\u4fe1\u606f -h/--help \uff1a \u5e2e\u52a9 -N/--name string \uff1a \u8d26\u6237\u7684\u540d\u79f0 -P/--parent string\uff1a \u6b64\u8d26\u6237\u7684\u7236\u8d26\u6237 -p/--partition strings \uff1a \u8be5\u8d26\u53f7\u53ef\u4ee5\u8bbf\u95ee\u7684\u5206\u533a\u5217\u8868 -q/--qos_list strings \uff1a\u8d26\u53f7\u53ef\u4ee5\u8bbf\u95ee\u7684qos\u5217\u8868 \u4f8b\uff1a\uff08\u6dfb\u52a0\u8d26\u6237PKU\u5e76\u6dfb\u52a0PKU\u7684\u5b50\u8d26\u6237ComputingCentre\uff09 cacctmgr add account - N = PKU - D = school - p = CPU , GPU - q = test - qos cacctmgr add account - N = ComputingCentre - D = department - P = PKU","title":"\u4e3b\u8981\u53c2\u6570"},{"location":"command/cacctmgr/#13","text":"\u7cfb\u7edf\u7ba1\u7406\u5458\u53ef\u4ee5\u6dfb\u52a0\u4efb\u610f\u8d26\u6237\u7684\u7528\u6237\uff0c \u8d26\u6237\u7ba1\u7406\u5458\u53ef\u4ee5\u6dfb\u52a0\u540c\u4e00\u8d26\u53f7\u4e0b\u7684\u65b0\u7528\u6237\u3002 \u6dfb\u52a0\u7684\u7528\u6237\u9700\u8981\u5148\u6709uid\uff08\u5148\u4f7f\u7528useradd\u5728linux\u7cfb\u7edf\u6dfb\u52a0\u8be5\u7528\u6237\uff09 \u3002","title":"1.3 \u6dfb\u52a0\u7528\u6237"},{"location":"command/cacctmgr/#_5","text":"-A/--account string \uff1a \u6b64\u7528\u6237\u6240\u5c5e\u7684\u7236\u8d26\u6237 -c/--coordinate \uff1a\u8bbe\u7f6e\u7528\u6237\u662f\u5426\u4e3a\u7236\u8d26\u53f7\u7684\u8d26\u6237\u8c03\u5ea6\u5458\uff08coordinator\uff09 -h/--help \uff1a \u5e2e\u52a9 -L/--level string \uff1a\u8bbe\u7f6e\u7528\u6237\u6743\u9650(none/operator/admin) (\u9ed8\u8ba4\u4e3a \"none\") -N/--name string \uff1a \u7528\u6237\u7684\u540d\u79f0 -p/--partition strings \uff1a \u8be5\u7528\u6237\u53ef\u4ee5\u8bbf\u95ee\u7684\u5206\u533a\u5217\u8868 \u4f8b\uff1a SQL useradd CS Plaintext cacctmgr add user -N=CS -A=PKU -p=CPU,GPU -L=admin # -p\u53c2\u6570\u6307\u660e\u7528\u6237\u53ef\u7528\u5206\u533a\u4e3aCPU\u548cGPU\uff08\u5206\u533a\u5fc5\u987b\u540c\u65f6\u4e3a\u7236\u8d26\u6237PKU\u7684\u53ef\u7528\u5206\u533a\uff09\uff0c\u5206\u533a\u7684allowed_qos_list\u4e0edefault_qos\u4fe1\u606f\u4e0d\u652f\u6301\u6307\u5b9a\uff0c\u9ed8\u8ba4\u4ece\u7236\u8d26\u6237PKU\u4e2d\u7ee7\u627f Plaintext cacctmgr add user -N=lab -A=ComputingCentre # \u672a\u6307\u660e-p\u53c2\u6570\uff0cpartition\u4e0eqos\u4fe1\u606f\u90fd\u4ece\u7236\u8d26\u6237ComputingCentre\u4e2d\u7ee7\u627f","title":"\u4e3b\u8981\u53c2\u6570"},{"location":"command/cacctmgr/#2","text":"","title":"2. \u5220\u9664"},{"location":"command/cacctmgr/#21","text":"cacctmgr delete user -N lab","title":"2.1 \u5220\u9664\u7528\u6237"},{"location":"command/cacctmgr/#22","text":"\u4ec5\u7cfb\u7edf\u7ba1\u7406\u5458\u53ef\u4ee5\u5220\u9664\u8d26\u6237\uff0c\u5220\u9664\u8d26\u6237\u65f6\u4f1a\u68c0\u67e5\u8d26\u6237\u4e0b\u662f\u5426\u8fd8\u6709\u5b50\u8d26\u6237\u6216\u8005\u7528\u6237\uff0c\u5982\u679c\u6709\u5219\u4e0d\u5141\u8bb8\u5220\u9664\uff0c\u9632\u6b62\u4ea7\u751f\u6e38\u79bb\u7684\u7528\u6237\uff0c\u9700\u8981\u5c06\u5176\u5b50\u8d26\u6237\u548c\u7528\u6237\u90fd\u8bbe\u7f6e\u5728\u65b0\u8d26\u6237\u4e0b\u3002 cacctmgr delete account -N ComputingCentre","title":"2.2 \u5220\u9664\u8d26\u6237"},{"location":"command/cacctmgr/#23-qos","text":"cacctmgr delete qos -N test-qos","title":"2.3 \u5220\u9664qos"},{"location":"command/cacctmgr/#3","text":"","title":"3. \u7981\u7528"},{"location":"command/cacctmgr/#31","text":"","title":"3.1 \u963b\u6b62\u7528\u6237\u6216\u8d26\u6237"},{"location":"command/cacctmgr/#_6","text":"account \uff1a \u963b\u6b62\u8d26\u6237 user \uff1a\u963b\u6b62\u8d26\u6237\u4e0b\u7684\u7528\u6237 cacctmgr block user lab -A=ComputingCentre cacctmgr block account ComputingCentre","title":"\u4e3b\u8981\u547d\u4ee4"},{"location":"command/cacctmgr/#4","text":"","title":"4. \u89e3\u7981"},{"location":"command/cacctmgr/#41","text":"","title":"4.1 \u89e3\u9664\u963b\u6b62\u7528\u6237\u6216\u8d26\u6237"},{"location":"command/cacctmgr/#_7","text":"account \uff1a \u89e3\u9664\u963b\u6b62\u8d26\u6237 user \uff1a\u89e3\u9664\u963b\u6b62\u8d26\u6237\u4e0b\u7684\u7528\u6237 cacctmgr unblock user lab -A=ComputingCentre cacctmgr unblock account ComputingCentre","title":"\u4e3b\u8981\u547d\u4ee4"},{"location":"command/cacctmgr/#5","text":"","title":"5. \u67e5\u8be2"},{"location":"command/cacctmgr/#51","text":"\u6240\u6709\u7528\u6237\u5747\u53ef\u4ee5\u4f7f\u7528\u67e5\u8be2\u529f\u80fd cacctmgr find user lab cacctmgr find user CS cacctmgr show user","title":"5.1 \u67e5\u627e\u7528\u6237"},{"location":"command/cacctmgr/#52","text":"cacctmgr find account ComputingCentre cacctmgr find account PKU cacctmgr show account","title":"5.2 \u67e5\u627e\u8d26\u6237"},{"location":"command/cacctmgr/#53-qos","text":"cacctmgr find qos test-qos cacctmgr show qos","title":"5.3 \u67e5\u627eqos"},{"location":"command/cacctmgr/#6","text":"","title":"6. \u4fee\u6539"},{"location":"command/cacctmgr/#61","text":"\u7cfb\u7edf\u7ba1\u7406\u5458\u53ef\u4ee5\u4fee\u6539\u4efb\u610f\u4fe1\u606f\uff0c \u8d26\u6237\u7ba1\u7406\u5458\u53ef\u4ee5\u4fee\u6539\u672c\u8eab\u8d26\u6237\u7684\u4fe1\u606f\uff0c\u4f46\u4e0d\u80fd\u66f4\u6539\u8d26\u6237\u7684\u7236\u8d26\u6237\u3002","title":"6.1 \u4fee\u6539\u8d26\u6237"},{"location":"command/cacctmgr/#_8","text":"--add_allowed_partition string \uff1a\u5c06\u65b0\u9879\u6dfb\u52a0\u5230\u5141\u8bb8\u7684\u5206\u533a\u5217\u8868 --add_allowed_qos_list strings \uff1a\u5c06\u65b0\u9879\u6dfb\u52a0\u5230\u5141\u8bb8\u7684qos\u5217\u8868 -Q/--default_qos string \uff1a \u4fee\u6539\u8d26\u6237\u9ed8\u8ba4qos --delete_allowed_partition string \uff1a\u4ece\u5141\u8bb8\u7684\u5206\u533a\u5217\u8868\u4e2d\u5220\u9664\u7279\u5b9a\u9879\u76ee --delete_allowed_qos_list strings \uff1a\u4ece\u5141\u8bb8\u7684qos\u5217\u8868\u4e2d\u5220\u9664\u7279\u5b9a\u9879 -D/--description string \uff1a\u4fee\u6539\u8d26\u6237\u7684\u63cf\u8ff0\u4fe1\u606f -F/--force \uff1a \u5f3a\u5236\u64cd\u4f5c -h/--help \uff1a \u5e2e\u52a9 -N/--name string \uff1a\u9700\u8981\u8fdb\u884c\u4fee\u6539\u7684\u8d26\u6237\u540d\u79f0 --set_allowed_partition strings \uff1a\u8bbe\u7f6e\u5141\u8bb8\u7684\u5206\u533a\u5217\u8868\u7684\u5185\u5bb9 --set_allowed_qos_list strings \uff1a\u8bbe\u7f6e\u5141\u8bb8\u7684qos\u5217\u8868\u7684\u5185\u5bb9 \u4f8b\uff1a Plaintext cacctmgr modify account -N=ComputingCentre -D=\"Located in PKU\"","title":"\u4e3b\u8981\u53c2\u6570"},{"location":"command/cacctmgr/#62","text":"\u7cfb\u7edf\u7ba1\u7406\u5458\u53ef\u4ee5\u4fee\u6539\u4efb\u610f\u4fe1\u606f\uff0c \u8d26\u6237\u7ba1\u7406\u5458\u53ef\u4ee5\u4fee\u6539\u540c\u8d26\u6237\u4e0b\u7528\u6237\u7684\u4fe1\u606f\uff0c\u4f46\u4e0d\u80fd\u66f4\u6539\u7528\u6237\u7684\u8d26\u6237\u3002","title":"6.2 \u4fee\u6539\u7528\u6237"},{"location":"command/cacctmgr/#_9","text":"-A/--account string \uff1a\u8bbe\u7f6e\u7528\u6237\u4f7f\u7528\u7684\u5e10\u53f7 --add_allowed_partition strings \uff1a\u5c06\u65b0\u9879\u6dfb\u52a0\u5230\u5141\u8bb8\u7684\u5206\u533a\u5217\u8868 --add_allowed_qos_list string \uff1a\u5c06\u65b0\u9879\u6dfb\u52a0\u5230\u5141\u8bb8\u7684qos\u5217\u8868 -L/--admin_level string \uff1a\u8bbe\u7f6e\u7528\u6237\u7ba1\u7406\u6743\u9650\uff08none/operator/admin\uff09 -Q/--default_qos string \uff1a \u4fee\u6539\u8d26\u6237\u9ed8\u8ba4qos --delete_allowed_partition strings \uff1a\u4ece\u5141\u8bb8\u7684\u5206\u533a\u5217\u8868\u4e2d\u5220\u9664\u7279\u5b9a\u9879\u76ee --delete_allowed_qos_list string \uff1a\u4ece\u5141\u8bb8\u7684qos\u5217\u8868\u4e2d\u5220\u9664\u7279\u5b9a\u9879\u76ee -F/--force \uff1a \u5f3a\u5236\u64cd\u4f5c -h/--help \uff1a \u5e2e\u52a9 -N/--name string \uff1a\u9700\u8981\u8fdb\u884c\u4fee\u6539\u7684\u7528\u6237\u540d\u79f0 -D/--default-account string \uff1a\u4fee\u6539\u7528\u6237\u7684\u9ed8\u8ba4\u8d26\u53f7 -p/--partition string \uff1a\u88ab\u4fee\u6539\u7684\u5206\u533a\uff0c\u5982\u679c\u4e0d\u663e\u5f0f\u8bbe\u7f6e\u8be5\u53c2\u6570\uff0c\u9ed8\u8ba4\u4fee\u6539\u6240\u6709\u5206\u533a --set_allowed_partition strings \uff1a\u8bbe\u7f6e\u5141\u8bb8\u7684\u5206\u533a\u5217\u8868\u7684\u5185\u5bb9 --set_allowed_qos_list strings \uff1a\u8bbe\u7f6e\u5141\u8bb8\u7684qos\u5217\u8868\u7684\u5185\u5bb9 \u4f8b SQL cacctmgr modify user -N=lab -A=ComputingCentre -L=operator --delete-allowed-partition GPU","title":"\u4e3b\u8981\u53c2\u6570"},{"location":"command/cacctmgr/#63-qos","text":"","title":"6.3 \u4fee\u6539qos"},{"location":"command/cacctmgr/#_10","text":"-D/--description string \uff1a\u4fee\u6539qos\u7684\u63cf\u8ff0\u4fe1\u606f -h/--help \uff1a \u5e2e\u52a9 -c/--max_cpus_per_user uint32 \uff1a(\u9ed8\u8ba410) -J/--max_jobs_per_user uint32 -T/--max_time_limit_per_task uint \uff1a\u4ee5\u79d2\u4e3a\u5355\u4f4d\u7684\u65f6\u95f4\uff08\u9ed8\u8ba4 3600\uff09 -N/--name string \uff1a \u9700\u8981\u8fdb\u884c\u4fee\u6539\u7684qos\u540d\u79f0 -P/--priority uint32 \uff1a(\u9ed8\u8ba41000)","title":"\u4e3b\u8981\u53c2\u6570"},{"location":"command/cacctmgr/#7","text":"","title":"7. \u663e\u793a"},{"location":"command/cacctmgr/#71","text":"\u7cfb\u7edf\u7ba1\u7406\u5458\u4f1a\u663e\u793a\u6570\u636e\u5e93\u6240\u6709\u6839\u8d26\u6237\u7684\u8d26\u6237\u6811\uff0c \u8d26\u6237\u7ba1\u7406\u5458\u548c\u7528\u6237\u4f1a\u663e\u793a\u672c\u8eab\u8d26\u6237\u7684\u8d26\u6237\u6811\u3002 cacctmgr show accounts","title":"7.1 \u663e\u793a\u8d26\u6237\u6811"},{"location":"command/cacctmgr/#72","text":"\u7cfb\u7edf\u7ba1\u7406\u5458\u4f1a\u663e\u793a\u6240\u6709\u7528\u6237\uff0c \u8d26\u6237\u7ba1\u7406\u5458\u548c\u7528\u6237\u4f1a\u663e\u793a\u540c\u4e00\u8d26\u6237\u4e0b\u7684\u6240\u6709\u7528\u6237\u3002 cacctmgr show users","title":"7.2 \u663e\u793a\u7528\u6237"},{"location":"command/calloc/","text":"calloc \u63d0\u4ea4\u4ea4\u4e92\u5f0f\u4efb\u52a1 calloc \u4f7f\u7528\u547d\u4ee4\u884c\u6307\u5b9a\u7684\u53c2\u6570\u7533\u8bf7\u8d44\u6e90\uff0c\u4efb\u52a1\u542f\u52a8\u65f6\uff0c\u4f1a\u8fdb\u5165\u65b0\u7684\u7528\u6237\u7ec8\u7aef\uff0c\u7528\u6237\u9700\u8981\u81ea\u884c\u767b\u9646\u5230\u8ba1\u7b97\u8282\u70b9\u5e76\u542f\u52a8\u4efb\u52a1\u3002calloc\u9700\u8981\u5728\u6709cfored\u8fd0\u884c\u7684\u8282\u70b9\u4e0a\u542f\u52a8\u3002 calloc \u53ea\u652f\u6301\u901a\u8fc7\u547d\u4ee4\u884c\u6307\u5b9a\u8bf7\u6c42\u53c2\u6570\uff0c\u652f\u6301\u7684\u547d\u4ee4\u884c\u9009\u9879\uff1a -h/--help : \u663e\u793a\u5e2e\u52a9 -A/--account string \uff1a\u63d0\u4ea4\u4f5c\u4e1a\u7684\u8d26\u6237 -D/--chdir string \uff1a\u4efb\u52a1\u5de5\u4f5c\u8def\u5f84 -C/--config string \uff1a\u914d\u7f6e\u6587\u4ef6\u8def\u5f84(\u9ed8\u8ba4 \"/etc/crane/config.yaml\") -c/--cpus-per-task float : \u6bcf\u4e2a\u8282\u70b9\u7533\u8bf7\u7684CPU\u6838\u5fc3\u6570 --debug-level string\uff1a \u53ef\u7528\u7684\u8c03\u8bd5\u7ea7\u522b\uff1atrace\uff08\u8ddf\u8e2a\uff09\u3001debug\uff08\u8c03\u8bd5\uff09\u3001info\uff08\u4fe1\u606f\uff0c\u9ed8\u8ba4\u503c\u4e3a \u201cinfo\u201d \uff09 -x/--exclude string\uff1a \u4ece\u5206\u914d\u4e2d\u6392\u9664\u7279\u5b9a\u8282\u70b9\uff08\u4ee5\u9017\u53f7\u5206\u9694\u7684\u5217\u8868 \uff09 --export string\uff1a \u4f20\u64ad\u73af\u5883\u53d8\u91cf --extra-attr string\uff1a \u4f5c\u4e1a\u7684\u989d\u5916\u5c5e\u6027\uff08JSON \u683c\u5f0f \uff09 --get-user-env\uff1a \u52a0\u8f7d\u7528\u6237\u7684\u767b\u5f55\u73af\u5883\u53d8\u91cf --gres string\uff1a \u6bcf\u4e2a\u4efb\u52a1\u6240\u9700\u7684\u901a\u7528\u8d44\u6e90\uff08Gres\uff09\uff0c\u683c\u5f0f\uff1a\u201cgpu:a100:1\u201d \u6216 \u201cgpu:1\u201d -J/--job-name string\uff1a \u4f5c\u4e1a\u540d\u79f0 --mail-type string\uff1a \u5f53\u7279\u5b9a\u4e8b\u4ef6\u53d1\u751f\u65f6\u901a\u8fc7\u90ae\u4ef6\u901a\u77e5\u7528\u6237\uff0c\u652f\u6301\u7684\u503c\uff1aNONE\uff08\u65e0\uff09\u3001BEGIN\uff08\u5f00\u59cb\uff09\u3001END\uff08\u7ed3\u675f\uff09\u3001FAIL\uff08\u5931\u8d25\uff09\u3001TIMELIMIT\uff08\u8fbe\u5230\u65f6\u95f4\u9650\u5236\uff09\u3001ALL\uff08\u6240\u6709\uff0c\u9ed8\u8ba4\u503c\u4e3a NONE \uff09 --mail-user string\uff1a \u901a\u77e5\u63a5\u6536\u8005\u7684\u90ae\u4ef6\u5730\u5740 --mem string\uff1a \u6700\u5927\u5b9e\u9645\u5185\u5b58\u91cf\uff0c\u652f\u6301 GB\uff08G\uff0cg\uff09\u3001MB\uff08M\uff0cm\uff09\u3001KB\uff08K\uff0ck\uff09\u548c Bytes\uff08B\uff09\u4e3a\u5355\u4f4d\uff0c\u9ed8\u8ba4\u5355\u4f4d\u662f MB -w/--nodelist string\uff1a \u8981\u5206\u914d\u7ed9\u4f5c\u4e1a\u7684\u8282\u70b9\uff08\u4ee5\u9017\u53f7\u5206\u9694\u7684\u5217\u8868 \uff09 -N/--nodes uint32\uff1a \u8981\u5728\u5176\u4e0a\u8fd0\u884c\u4f5c\u4e1a\u7684\u8282\u70b9\u6570\u91cf\uff08\u9ed8\u8ba4\u503c\u4e3a 1 \uff09 --ntasks-per-node uint32\uff1a \u5728\u6bcf\u4e2a\u8282\u70b9\u4e0a\u8981\u8c03\u7528\u7684\u4efb\u52a1\u6570\u91cf\uff08\u9ed8\u8ba4\u503c\u4e3a 1 \uff09 - p/--partition string \uff1a\u8bf7\u6c42\u7684\u5206\u533a -q/ --qos string\uff1a \u4f5c\u4e1a\u4f7f\u7528\u7684\u670d\u52a1\u8d28\u91cf\uff08QoS \uff09 -r/ --reservation string\uff1a \u4f7f\u7528\u9884\u7559\u8d44\u6e90 -t/ --time string\uff1a \u65f6\u95f4\u9650\u5236\uff0c\u683c\u5f0f\uff1a\u201cday - hours:minutes:seconds\u201d\uff08\u5982 5 - 0:0:1 \u8868\u793a 5 \u5929 1 \u79d2 \uff09\u6216 \u201chours:minutes:seconds\u201d\uff08\u5982 10:1:2 \u8868\u793a 10 \u5c0f\u65f6 1 \u5206\u949f 2 \u79d2 \uff09 -v/ --version \uff1acalloc \u547d\u4ee4\u7684\u7248\u672c \u4f8b\uff1a calloc -h \u9000\u51facalloc\u65b0\u542f\u52a8\u7684\u7ec8\u7aef\u5c06\u7ed3\u675f\u4efb\u52a1\u3002 \u4f8b\uff1a\u5728CPU\u5206\u533a\uff0c\u7533\u8bf7\u4e24\u4e2a\u8282\u70b9\uff0c\u4e00\u4e2aCPU\u6838\u5fc3\uff0c200M\u5185\u5b58 calloc - c 1 -- mem 200 M - p CPU - N 2 \u8fd0\u884c\u7ed3\u679c\uff1a - \u4f8b\uff1a\u5728GPU\u5206\u533a\u4e0b\uff0c\u7533\u8bf7\u4e00\u4e2a\u8282\u70b9\uff0c\u6bcf\u4e2a\u8282\u70b9\u4e0a\u8fd0\u884c\u4e24\u4e2a\u4efb\u52a1\uff0c\u7533\u8bf7\u8282\u70b9\u7684\u5019\u9009\u5217\u8868\u4e3acrane02,crane03\uff0c\u4e14\u4efb\u52a1\u63d0\u4ea4\u5728acct-yan\u8d26\u6237\u4e0b calloc - A acct - test -- ntasks - per - node 2 - w crane02 , crane03 - p GPU - N 1 \u8fd0\u884c\u7ed3\u679c\uff1a \u4f8b\uff1a\u5728CPU\u5206\u533a\u4e0b\uff0c\u7533\u8bf7200M\u5185\u5b58\uff0c\u4efb\u52a1\u8fd0\u884c\u6700\u957f\u65f6\u95f4\u4e3a25\u5206\u949f25\u79d2\uff0c\u4e14\u4efb\u52a1\u8fd0\u884c\u5728test-qos\u4e0b calloc -- mem 200 M - p CPU - q test - qos - t 00 : 25 : 25 calloc -D /path calloc --debug-level trace calloc -x cranetest02 calloc --get-user-env calloc -J job_name","title":"calloc"},{"location":"command/calloc/#calloc","text":"calloc \u4f7f\u7528\u547d\u4ee4\u884c\u6307\u5b9a\u7684\u53c2\u6570\u7533\u8bf7\u8d44\u6e90\uff0c\u4efb\u52a1\u542f\u52a8\u65f6\uff0c\u4f1a\u8fdb\u5165\u65b0\u7684\u7528\u6237\u7ec8\u7aef\uff0c\u7528\u6237\u9700\u8981\u81ea\u884c\u767b\u9646\u5230\u8ba1\u7b97\u8282\u70b9\u5e76\u542f\u52a8\u4efb\u52a1\u3002calloc\u9700\u8981\u5728\u6709cfored\u8fd0\u884c\u7684\u8282\u70b9\u4e0a\u542f\u52a8\u3002 calloc \u53ea\u652f\u6301\u901a\u8fc7\u547d\u4ee4\u884c\u6307\u5b9a\u8bf7\u6c42\u53c2\u6570\uff0c\u652f\u6301\u7684\u547d\u4ee4\u884c\u9009\u9879\uff1a -h/--help : \u663e\u793a\u5e2e\u52a9 -A/--account string \uff1a\u63d0\u4ea4\u4f5c\u4e1a\u7684\u8d26\u6237 -D/--chdir string \uff1a\u4efb\u52a1\u5de5\u4f5c\u8def\u5f84 -C/--config string \uff1a\u914d\u7f6e\u6587\u4ef6\u8def\u5f84(\u9ed8\u8ba4 \"/etc/crane/config.yaml\") -c/--cpus-per-task float : \u6bcf\u4e2a\u8282\u70b9\u7533\u8bf7\u7684CPU\u6838\u5fc3\u6570 --debug-level string\uff1a \u53ef\u7528\u7684\u8c03\u8bd5\u7ea7\u522b\uff1atrace\uff08\u8ddf\u8e2a\uff09\u3001debug\uff08\u8c03\u8bd5\uff09\u3001info\uff08\u4fe1\u606f\uff0c\u9ed8\u8ba4\u503c\u4e3a \u201cinfo\u201d \uff09 -x/--exclude string\uff1a \u4ece\u5206\u914d\u4e2d\u6392\u9664\u7279\u5b9a\u8282\u70b9\uff08\u4ee5\u9017\u53f7\u5206\u9694\u7684\u5217\u8868 \uff09 --export string\uff1a \u4f20\u64ad\u73af\u5883\u53d8\u91cf --extra-attr string\uff1a \u4f5c\u4e1a\u7684\u989d\u5916\u5c5e\u6027\uff08JSON \u683c\u5f0f \uff09 --get-user-env\uff1a \u52a0\u8f7d\u7528\u6237\u7684\u767b\u5f55\u73af\u5883\u53d8\u91cf --gres string\uff1a \u6bcf\u4e2a\u4efb\u52a1\u6240\u9700\u7684\u901a\u7528\u8d44\u6e90\uff08Gres\uff09\uff0c\u683c\u5f0f\uff1a\u201cgpu:a100:1\u201d \u6216 \u201cgpu:1\u201d -J/--job-name string\uff1a \u4f5c\u4e1a\u540d\u79f0 --mail-type string\uff1a \u5f53\u7279\u5b9a\u4e8b\u4ef6\u53d1\u751f\u65f6\u901a\u8fc7\u90ae\u4ef6\u901a\u77e5\u7528\u6237\uff0c\u652f\u6301\u7684\u503c\uff1aNONE\uff08\u65e0\uff09\u3001BEGIN\uff08\u5f00\u59cb\uff09\u3001END\uff08\u7ed3\u675f\uff09\u3001FAIL\uff08\u5931\u8d25\uff09\u3001TIMELIMIT\uff08\u8fbe\u5230\u65f6\u95f4\u9650\u5236\uff09\u3001ALL\uff08\u6240\u6709\uff0c\u9ed8\u8ba4\u503c\u4e3a NONE \uff09 --mail-user string\uff1a \u901a\u77e5\u63a5\u6536\u8005\u7684\u90ae\u4ef6\u5730\u5740 --mem string\uff1a \u6700\u5927\u5b9e\u9645\u5185\u5b58\u91cf\uff0c\u652f\u6301 GB\uff08G\uff0cg\uff09\u3001MB\uff08M\uff0cm\uff09\u3001KB\uff08K\uff0ck\uff09\u548c Bytes\uff08B\uff09\u4e3a\u5355\u4f4d\uff0c\u9ed8\u8ba4\u5355\u4f4d\u662f MB -w/--nodelist string\uff1a \u8981\u5206\u914d\u7ed9\u4f5c\u4e1a\u7684\u8282\u70b9\uff08\u4ee5\u9017\u53f7\u5206\u9694\u7684\u5217\u8868 \uff09 -N/--nodes uint32\uff1a \u8981\u5728\u5176\u4e0a\u8fd0\u884c\u4f5c\u4e1a\u7684\u8282\u70b9\u6570\u91cf\uff08\u9ed8\u8ba4\u503c\u4e3a 1 \uff09 --ntasks-per-node uint32\uff1a \u5728\u6bcf\u4e2a\u8282\u70b9\u4e0a\u8981\u8c03\u7528\u7684\u4efb\u52a1\u6570\u91cf\uff08\u9ed8\u8ba4\u503c\u4e3a 1 \uff09 - p/--partition string \uff1a\u8bf7\u6c42\u7684\u5206\u533a -q/ --qos string\uff1a \u4f5c\u4e1a\u4f7f\u7528\u7684\u670d\u52a1\u8d28\u91cf\uff08QoS \uff09 -r/ --reservation string\uff1a \u4f7f\u7528\u9884\u7559\u8d44\u6e90 -t/ --time string\uff1a \u65f6\u95f4\u9650\u5236\uff0c\u683c\u5f0f\uff1a\u201cday - hours:minutes:seconds\u201d\uff08\u5982 5 - 0:0:1 \u8868\u793a 5 \u5929 1 \u79d2 \uff09\u6216 \u201chours:minutes:seconds\u201d\uff08\u5982 10:1:2 \u8868\u793a 10 \u5c0f\u65f6 1 \u5206\u949f 2 \u79d2 \uff09 -v/ --version \uff1acalloc \u547d\u4ee4\u7684\u7248\u672c \u4f8b\uff1a calloc -h \u9000\u51facalloc\u65b0\u542f\u52a8\u7684\u7ec8\u7aef\u5c06\u7ed3\u675f\u4efb\u52a1\u3002 \u4f8b\uff1a\u5728CPU\u5206\u533a\uff0c\u7533\u8bf7\u4e24\u4e2a\u8282\u70b9\uff0c\u4e00\u4e2aCPU\u6838\u5fc3\uff0c200M\u5185\u5b58 calloc - c 1 -- mem 200 M - p CPU - N 2 \u8fd0\u884c\u7ed3\u679c\uff1a - \u4f8b\uff1a\u5728GPU\u5206\u533a\u4e0b\uff0c\u7533\u8bf7\u4e00\u4e2a\u8282\u70b9\uff0c\u6bcf\u4e2a\u8282\u70b9\u4e0a\u8fd0\u884c\u4e24\u4e2a\u4efb\u52a1\uff0c\u7533\u8bf7\u8282\u70b9\u7684\u5019\u9009\u5217\u8868\u4e3acrane02,crane03\uff0c\u4e14\u4efb\u52a1\u63d0\u4ea4\u5728acct-yan\u8d26\u6237\u4e0b calloc - A acct - test -- ntasks - per - node 2 - w crane02 , crane03 - p GPU - N 1 \u8fd0\u884c\u7ed3\u679c\uff1a \u4f8b\uff1a\u5728CPU\u5206\u533a\u4e0b\uff0c\u7533\u8bf7200M\u5185\u5b58\uff0c\u4efb\u52a1\u8fd0\u884c\u6700\u957f\u65f6\u95f4\u4e3a25\u5206\u949f25\u79d2\uff0c\u4e14\u4efb\u52a1\u8fd0\u884c\u5728test-qos\u4e0b calloc -- mem 200 M - p CPU - q test - qos - t 00 : 25 : 25 calloc -D /path calloc --debug-level trace calloc -x cranetest02 calloc --get-user-env calloc -J job_name","title":"calloc \u63d0\u4ea4\u4ea4\u4e92\u5f0f\u4efb\u52a1"},{"location":"command/cbatch/","text":"cbatch \u63d0\u4ea4\u6279\u5904\u7406\u4f5c\u4e1a cbatch\u4e3b\u8981\u662f\u5c06\u7528\u6237\u63cf\u8ff0\u6574\u4e2a\u8ba1\u7b97\u8fc7\u7a0b\u7684\u811a\u672c\u4f20\u9012\u7ed9\u4f5c\u4e1a\u8c03\u5ea6\u7cfb\u7edf\uff0c\u5e76\u4e3a\u4f5c\u4e1a\u5206\u914d\u4f5c\u4e1a\u53f7\uff0c\u7b49\u5f85\u4f5c\u4e1a\u8c03\u5ea6\u7cfb\u7edf\u4e3a\u5176\u5206\u914d\u8d44\u6e90\u5e76\u6267\u884c\u3002 CraneSched\u7cfb\u7edf\u4e2d\u5fc5\u987b\u6709\u7528\u6237\u548c\u8d26\u53f7\u624d\u80fd\u63d0\u4ea4\u4f5c\u4e1a\uff0c\u6dfb\u52a0\u7528\u6237\u548c\u8d26\u6237\u8bf7\u53c2\u8003 cacctmgr\u6559\u7a0b \u3002 \u9996\u5148\u4ecb\u7ecd\u4e00\u4e2a\u7b80\u5355\u7684\u5355\u8282\u70b9\u4f5c\u4e1a\u7684\u4f8b\u5b50: \u4e0b\u5217\u4f5c\u4e1a\u5c06\u7533\u8bf7\u4e00\u4e2a\u8282\u70b9\uff0c\u4e00\u4e2aCPU\u6838\u5fc3\uff0c\u5e76\u5728\u8ba1\u7b97\u8282\u70b9\u4e0a\u8fd0\u884chostname\u5e76\u9000\u51fa #!/bin/bash #CBATCH --ntasks-per-node 1 #CBATCH --nodes 1 #CBATCH -c 1 #CBATCH --mem 20M #CBATCH --time 0:3:1 #CBATCH -o job.out #CBATCH -p CPU #CBATCH -J Test_Job hostname \u5047\u8bbe\u4e0a\u9762\u4f5c\u4e1a\u811a\u672c\u7684\u6587\u4ef6\u540d\u4e3acbatch_test.sh\uff0c\u901a\u8fc7cbatch\u547d\u4ee4\u63d0\u4ea4\uff1a cbatch cbatch_test.sh cbatch\u8fd0\u884c\u7ed3\u679c\u5c55\u793a \u4e3b\u8981\u53c2\u6570\uff1a -h/--help : \u663e\u793a\u5e2e\u52a9 -A/--account string \uff1a\u63d0\u4ea4\u4f5c\u4e1a\u7684\u8d26\u6237 -D/--chdir string \uff1a\u4efb\u52a1\u5de5\u4f5c\u8def\u5f84 -C/--config string \uff1a\u914d\u7f6e\u6587\u4ef6\u8def\u5f84(\u9ed8\u8ba4 \"/etc/crane/config.yaml\") -c/--cpus-per-task float : \u6bcf\u4e2a\u8282\u70b9\u7533\u8bf7\u7684CPU\u6838\u5fc3\u6570 -e/--error string\uff1a \u6307\u5b9a\u811a\u672c\u9519\u8bef\u65e5\u5fd7\u5b9a\u5411\u8def\u5f84 -x/--exclude string \uff1a \u529f\u80fd\u662f\u4ece\u5206\u914d\u4e2d\u6392\u9664\u7279\u5b9a\u8282\u70b9\uff08\u7528\u9017\u53f7\u5206\u9694\u7684\u5217\u8868 \uff09\uff0c\u7528\u4e8e\u8282\u70b9\u5206\u914d\u7ba1\u63a7\u573a\u666f\uff0c\u6307\u5b9a\u4e0d\u60f3\u53c2\u4e0e\u5206\u914d\u7684\u8282\u70b9\u3002 --export string \uff1a \u4f5c\u7528\u4e3a\u4f20\u64ad\u73af\u5883\u53d8\u91cf\uff0c\u5728\u6d89\u53ca\u73af\u5883\u53d8\u91cf\u4f20\u9012\uff0c\u8ba9\u53d8\u91cf\u5728\u76f8\u5173\u4efb\u52a1\u3001\u4f5c\u4e1a\u7b49\u6267\u884c\u73af\u5883\u4e2d\u751f\u6548\u65f6\u4f7f\u7528\u3002 --extra-attr string \uff1a \u53ef\u8bbe\u7f6e\u4f5c\u4e1a\u7684\u989d\u5916\u5c5e\u6027\uff08json\u683c\u5f0f \uff09\uff0c\u7528\u4e8e\u7ed9\u4f5c\u4e1a\u9644\u52a0\u81ea\u5b9a\u4e49\u7684\u5c5e\u6027\u4fe1\u606f\uff0c\u65b9\u4fbf\u8bc6\u522b\u3001\u7ba1\u7406\u7b49\u3002 --get-user-env \uff1a \u4f1a\u52a0\u8f7d\u7528\u6237\u7684\u767b\u5f55\u73af\u5883\u53d8\u91cf\uff0c\u8ba9\u4f5c\u4e1a\u7b49\u6267\u884c\u65f6\u80fd\u4f7f\u7528\u7528\u6237\u767b\u5f55\u65f6\u7684\u73af\u5883\u53d8\u91cf\u914d\u7f6e\u3002 --gres string \uff1a \u6307\u5b9a\u6bcf\u4e2a\u4efb\u52a1\u6240\u9700\u7684\u901a\u7528\u8d44\u6e90\uff0c\u683c\u5f0f\u5982 gpu:a100:1\uff08\u6307\u5b9a\u4f7f\u7528 1 \u5757 a100 \u578b\u53f7 GPU \uff09\u6216 gpu:1\uff08\u6307\u5b9a\u4f7f\u7528 1 \u5757 GPU \uff0c\u4e0d\u9650\u5177\u4f53\u578b\u53f7 \uff09\uff0c\u7528\u4e8e GPU \u7b49\u8d44\u6e90\u5206\u914d\u573a\u666f\u3002 -J/--job-name string \uff1a\u4f5c\u4e1a\u540d --json \uff1a\u4ee5 JSON \u683c\u5f0f\u8f93\u51fa --mail-type string\uff1a \u5f53\u7279\u5b9a\u4e8b\u4ef6\u53d1\u751f\u65f6\uff0c\u5411\u7528\u6237\u53d1\u9001\u90ae\u4ef6\u901a\u77e5\uff0c\u652f\u6301\u7684\u503c\u6709\uff1a\u65e0\uff08NONE\uff09\u3001\u5f00\u59cb\uff08BEGIN\uff09\u3001\u7ed3\u675f\uff08END\uff09\u3001\u5931\u8d25\uff08FAIL\uff09\u3001\u8fbe\u5230\u65f6\u95f4\u9650\u5236\uff08TIMELIMIT\uff09\u3001\u6240\u6709\u4e8b\u4ef6\uff08ALL\uff09\uff08\u9ed8\u8ba4\u662f\u65e0\uff08NONE) \uff09 --mail-user string\uff1a \u901a\u77e5\u63a5\u6536\u8005\u7684\u90ae\u4ef6\u5730\u5740 --mem string\uff1a \u6700\u5927\u5b9e\u9645\u5185\u5b58\u91cf\uff0c\u652f\u6301 GB\uff08G\uff0cg\uff09\u3001MB\uff08M\uff0cm\uff09\u3001KB\uff08K\uff0ck\uff09\u548c\u5b57\u8282\uff08B\uff09\u4e3a\u5355\u4f4d\uff0c\u9ed8\u8ba4\u5355\u4f4d\u662f MB -w/--nodelist string \uff1a \u8981\u5206\u914d\u7ed9\u4f5c\u4e1a\u7684\u8282\u70b9\uff08\u9017\u53f7\u5206\u9694\u7684\u5217\u8868 \uff09 -N/--nodes uint32 \uff1a \u4f5c\u4e1a\u8981\u8fd0\u884c\u7684\u8282\u70b9\u6570\u91cf\uff08\u683c\u5f0f N = min[-max] \uff0c\u9ed8\u8ba4 1 \uff09 --ntasks-per-node uint32 \uff1a \u6bcf\u4e2a\u8282\u70b9\u8981\u8c03\u7528\u7684\u4efb\u52a1\u6570\u91cf\uff08\u9ed8\u8ba4 1 \uff09 --open-mode string \uff1a \u8bbe\u7f6e\u6253\u5f00\u8f93\u51fa\u548c\u9519\u8bef\u6587\u4ef6\u7684\u6a21\u5f0f\uff0c\u652f\u6301\u7684\u503c\uff1aappend\uff08\u8ffd\u52a0 \uff09\u3001truncate\uff08\u622a\u65ad \uff0c\u9ed8\u8ba4 \uff09 -o/ --output string \uff1a\u811a\u672c\u6807\u51c6\u8f93\u51fa\u7684\u91cd\u5b9a\u5411\u8def\u5f84 -p/ --partition string \uff1a\u8bf7\u6c42\u7684\u5206\u533a -q/--qos string \uff1a\u4f5c\u4e1a\u4f7f\u7528\u7684\u670d\u52a1\u8d28\u91cf\uff08QoS \uff09 --repeat uint32 \uff1a\u591a\u6b21\u63d0\u4ea4\u4f5c\u4e1a\uff08\u9ed8\u8ba4 1 \uff09 -r/--reservation string \uff1a \u4f7f\u7528\u9884\u7559\u8d44\u6e90 -t /--time string \uff1a \u65f6\u95f4\u9650\u5236 \uff0c\u683c\u5f0f\uff1a\"day-hours:minutes:seconds\"\uff08\u5982 5-0:0:1 \u8868\u793a 5 \u5929 1 \u79d2 \uff09\u6216 \"hours:minutes:seconds\"\uff08\u5982 10:1:2 \u8868\u793a 10 \u5c0f\u65f6 1 \u5206\u949f 2 \u79d2 \uff09 -v /--version \uff1a cbatch \u7684\u7248\u672c --wrap string \uff1a \u5c06\u547d\u4ee4\u5b57\u7b26\u4e32\u5305\u88c5\u5230 sh \u811a\u672c\u4e2d\u5e76\u63d0\u4ea4 \u4f8b\uff1a cbatch cbatch_test.sh cbatch -h cbatch -A=acct-test cbatch_test.sh cbatch -x crane01,crane02 cbatch_test.sh cbatch -J testjob01 cbatch_test.sh cbatch -w crane01,crane03 cbatch_test.sh cbatch -p GPU cbatch_test.sh cbatch -t 00:25:25 cbatch_test.sh cbatch - c 2 cbatch_test . sh cbatch -- mem 123 M cbatch_test . sh cbatch - N 2 -- ntasks - per - node 2 cbatch_test . sh cbatch -D /path test.sh cbatch -e error.log test.sh cbatch --export ALL test.sh cbatch --get-user-env test.sh cbatch -o output.out test.sh cbatch -q qos_test test.sh cbatch --repeat 3 test.sh \u5e38\u7528\u73af\u5883\u53d8\u91cf \u53d8\u91cf\u540d \u8bf4\u660e CRANE_JOB_NODELIST \u4f5c\u4e1a\u5206\u914d\u7684\u8282\u70b9\u5217\u8868 %j \u4f5c\u4e1a\u53f7 \u4e0b\u9762\u4ecb\u7ecd\u63d0\u4ea4\u4e00\u4e2a\u8de8\u8282\u70b9\u591a\u6838\u5fc3\u7684\u4f8b\u5b50\uff1a c\u4e0b\u5217\u4f5c\u4e1a\u5c06\u5728\u4e09\u4e2a\u8282\u70b9\u4e0a\u8fd0\u884c\uff0c\u6bcf\u4e2a\u8282\u70b9\u4f7f\u75284\u4e2aCPU\u6838\u5fc3\u3002 #!/bin/bash #CBATCH -o crane_test%j.out #CBATCH -p CPU #CBATCH -J \"crane_test\" #CBATCH --nodes 3 #CBATCH --ntasks-per-node 4 #CBATCH -c 4 #CBATCH --time 50:00:00 # \u751f\u6210\u4f5c\u4e1a\u5206\u914d\u7684\u8282\u70b9\u7684machinefile echo \"$CRANE_JOB_NODELIST\" | tr \";\" \"\\n\" > crane.hosts #\u52a0\u8f7dMPI\u8fd0\u884c\u73af\u5883 module load mpich/4.0 #\u6267\u884c\u8de8\u8282\u70b9\u5e76\u884c\u4efb\u52a1 mpirun -n 13 -machinefile crane.hosts helloWorld > log","title":"cbatch"},{"location":"command/cbatch/#cbatch","text":"cbatch\u4e3b\u8981\u662f\u5c06\u7528\u6237\u63cf\u8ff0\u6574\u4e2a\u8ba1\u7b97\u8fc7\u7a0b\u7684\u811a\u672c\u4f20\u9012\u7ed9\u4f5c\u4e1a\u8c03\u5ea6\u7cfb\u7edf\uff0c\u5e76\u4e3a\u4f5c\u4e1a\u5206\u914d\u4f5c\u4e1a\u53f7\uff0c\u7b49\u5f85\u4f5c\u4e1a\u8c03\u5ea6\u7cfb\u7edf\u4e3a\u5176\u5206\u914d\u8d44\u6e90\u5e76\u6267\u884c\u3002 CraneSched\u7cfb\u7edf\u4e2d\u5fc5\u987b\u6709\u7528\u6237\u548c\u8d26\u53f7\u624d\u80fd\u63d0\u4ea4\u4f5c\u4e1a\uff0c\u6dfb\u52a0\u7528\u6237\u548c\u8d26\u6237\u8bf7\u53c2\u8003 cacctmgr\u6559\u7a0b \u3002 \u9996\u5148\u4ecb\u7ecd\u4e00\u4e2a\u7b80\u5355\u7684\u5355\u8282\u70b9\u4f5c\u4e1a\u7684\u4f8b\u5b50: \u4e0b\u5217\u4f5c\u4e1a\u5c06\u7533\u8bf7\u4e00\u4e2a\u8282\u70b9\uff0c\u4e00\u4e2aCPU\u6838\u5fc3\uff0c\u5e76\u5728\u8ba1\u7b97\u8282\u70b9\u4e0a\u8fd0\u884chostname\u5e76\u9000\u51fa #!/bin/bash #CBATCH --ntasks-per-node 1 #CBATCH --nodes 1 #CBATCH -c 1 #CBATCH --mem 20M #CBATCH --time 0:3:1 #CBATCH -o job.out #CBATCH -p CPU #CBATCH -J Test_Job hostname \u5047\u8bbe\u4e0a\u9762\u4f5c\u4e1a\u811a\u672c\u7684\u6587\u4ef6\u540d\u4e3acbatch_test.sh\uff0c\u901a\u8fc7cbatch\u547d\u4ee4\u63d0\u4ea4\uff1a cbatch cbatch_test.sh cbatch\u8fd0\u884c\u7ed3\u679c\u5c55\u793a \u4e3b\u8981\u53c2\u6570\uff1a -h/--help : \u663e\u793a\u5e2e\u52a9 -A/--account string \uff1a\u63d0\u4ea4\u4f5c\u4e1a\u7684\u8d26\u6237 -D/--chdir string \uff1a\u4efb\u52a1\u5de5\u4f5c\u8def\u5f84 -C/--config string \uff1a\u914d\u7f6e\u6587\u4ef6\u8def\u5f84(\u9ed8\u8ba4 \"/etc/crane/config.yaml\") -c/--cpus-per-task float : \u6bcf\u4e2a\u8282\u70b9\u7533\u8bf7\u7684CPU\u6838\u5fc3\u6570 -e/--error string\uff1a \u6307\u5b9a\u811a\u672c\u9519\u8bef\u65e5\u5fd7\u5b9a\u5411\u8def\u5f84 -x/--exclude string \uff1a \u529f\u80fd\u662f\u4ece\u5206\u914d\u4e2d\u6392\u9664\u7279\u5b9a\u8282\u70b9\uff08\u7528\u9017\u53f7\u5206\u9694\u7684\u5217\u8868 \uff09\uff0c\u7528\u4e8e\u8282\u70b9\u5206\u914d\u7ba1\u63a7\u573a\u666f\uff0c\u6307\u5b9a\u4e0d\u60f3\u53c2\u4e0e\u5206\u914d\u7684\u8282\u70b9\u3002 --export string \uff1a \u4f5c\u7528\u4e3a\u4f20\u64ad\u73af\u5883\u53d8\u91cf\uff0c\u5728\u6d89\u53ca\u73af\u5883\u53d8\u91cf\u4f20\u9012\uff0c\u8ba9\u53d8\u91cf\u5728\u76f8\u5173\u4efb\u52a1\u3001\u4f5c\u4e1a\u7b49\u6267\u884c\u73af\u5883\u4e2d\u751f\u6548\u65f6\u4f7f\u7528\u3002 --extra-attr string \uff1a \u53ef\u8bbe\u7f6e\u4f5c\u4e1a\u7684\u989d\u5916\u5c5e\u6027\uff08json\u683c\u5f0f \uff09\uff0c\u7528\u4e8e\u7ed9\u4f5c\u4e1a\u9644\u52a0\u81ea\u5b9a\u4e49\u7684\u5c5e\u6027\u4fe1\u606f\uff0c\u65b9\u4fbf\u8bc6\u522b\u3001\u7ba1\u7406\u7b49\u3002 --get-user-env \uff1a \u4f1a\u52a0\u8f7d\u7528\u6237\u7684\u767b\u5f55\u73af\u5883\u53d8\u91cf\uff0c\u8ba9\u4f5c\u4e1a\u7b49\u6267\u884c\u65f6\u80fd\u4f7f\u7528\u7528\u6237\u767b\u5f55\u65f6\u7684\u73af\u5883\u53d8\u91cf\u914d\u7f6e\u3002 --gres string \uff1a \u6307\u5b9a\u6bcf\u4e2a\u4efb\u52a1\u6240\u9700\u7684\u901a\u7528\u8d44\u6e90\uff0c\u683c\u5f0f\u5982 gpu:a100:1\uff08\u6307\u5b9a\u4f7f\u7528 1 \u5757 a100 \u578b\u53f7 GPU \uff09\u6216 gpu:1\uff08\u6307\u5b9a\u4f7f\u7528 1 \u5757 GPU \uff0c\u4e0d\u9650\u5177\u4f53\u578b\u53f7 \uff09\uff0c\u7528\u4e8e GPU \u7b49\u8d44\u6e90\u5206\u914d\u573a\u666f\u3002 -J/--job-name string \uff1a\u4f5c\u4e1a\u540d --json \uff1a\u4ee5 JSON \u683c\u5f0f\u8f93\u51fa --mail-type string\uff1a \u5f53\u7279\u5b9a\u4e8b\u4ef6\u53d1\u751f\u65f6\uff0c\u5411\u7528\u6237\u53d1\u9001\u90ae\u4ef6\u901a\u77e5\uff0c\u652f\u6301\u7684\u503c\u6709\uff1a\u65e0\uff08NONE\uff09\u3001\u5f00\u59cb\uff08BEGIN\uff09\u3001\u7ed3\u675f\uff08END\uff09\u3001\u5931\u8d25\uff08FAIL\uff09\u3001\u8fbe\u5230\u65f6\u95f4\u9650\u5236\uff08TIMELIMIT\uff09\u3001\u6240\u6709\u4e8b\u4ef6\uff08ALL\uff09\uff08\u9ed8\u8ba4\u662f\u65e0\uff08NONE) \uff09 --mail-user string\uff1a \u901a\u77e5\u63a5\u6536\u8005\u7684\u90ae\u4ef6\u5730\u5740 --mem string\uff1a \u6700\u5927\u5b9e\u9645\u5185\u5b58\u91cf\uff0c\u652f\u6301 GB\uff08G\uff0cg\uff09\u3001MB\uff08M\uff0cm\uff09\u3001KB\uff08K\uff0ck\uff09\u548c\u5b57\u8282\uff08B\uff09\u4e3a\u5355\u4f4d\uff0c\u9ed8\u8ba4\u5355\u4f4d\u662f MB -w/--nodelist string \uff1a \u8981\u5206\u914d\u7ed9\u4f5c\u4e1a\u7684\u8282\u70b9\uff08\u9017\u53f7\u5206\u9694\u7684\u5217\u8868 \uff09 -N/--nodes uint32 \uff1a \u4f5c\u4e1a\u8981\u8fd0\u884c\u7684\u8282\u70b9\u6570\u91cf\uff08\u683c\u5f0f N = min[-max] \uff0c\u9ed8\u8ba4 1 \uff09 --ntasks-per-node uint32 \uff1a \u6bcf\u4e2a\u8282\u70b9\u8981\u8c03\u7528\u7684\u4efb\u52a1\u6570\u91cf\uff08\u9ed8\u8ba4 1 \uff09 --open-mode string \uff1a \u8bbe\u7f6e\u6253\u5f00\u8f93\u51fa\u548c\u9519\u8bef\u6587\u4ef6\u7684\u6a21\u5f0f\uff0c\u652f\u6301\u7684\u503c\uff1aappend\uff08\u8ffd\u52a0 \uff09\u3001truncate\uff08\u622a\u65ad \uff0c\u9ed8\u8ba4 \uff09 -o/ --output string \uff1a\u811a\u672c\u6807\u51c6\u8f93\u51fa\u7684\u91cd\u5b9a\u5411\u8def\u5f84 -p/ --partition string \uff1a\u8bf7\u6c42\u7684\u5206\u533a -q/--qos string \uff1a\u4f5c\u4e1a\u4f7f\u7528\u7684\u670d\u52a1\u8d28\u91cf\uff08QoS \uff09 --repeat uint32 \uff1a\u591a\u6b21\u63d0\u4ea4\u4f5c\u4e1a\uff08\u9ed8\u8ba4 1 \uff09 -r/--reservation string \uff1a \u4f7f\u7528\u9884\u7559\u8d44\u6e90 -t /--time string \uff1a \u65f6\u95f4\u9650\u5236 \uff0c\u683c\u5f0f\uff1a\"day-hours:minutes:seconds\"\uff08\u5982 5-0:0:1 \u8868\u793a 5 \u5929 1 \u79d2 \uff09\u6216 \"hours:minutes:seconds\"\uff08\u5982 10:1:2 \u8868\u793a 10 \u5c0f\u65f6 1 \u5206\u949f 2 \u79d2 \uff09 -v /--version \uff1a cbatch \u7684\u7248\u672c --wrap string \uff1a \u5c06\u547d\u4ee4\u5b57\u7b26\u4e32\u5305\u88c5\u5230 sh \u811a\u672c\u4e2d\u5e76\u63d0\u4ea4 \u4f8b\uff1a cbatch cbatch_test.sh cbatch -h cbatch -A=acct-test cbatch_test.sh cbatch -x crane01,crane02 cbatch_test.sh cbatch -J testjob01 cbatch_test.sh cbatch -w crane01,crane03 cbatch_test.sh cbatch -p GPU cbatch_test.sh cbatch -t 00:25:25 cbatch_test.sh cbatch - c 2 cbatch_test . sh cbatch -- mem 123 M cbatch_test . sh cbatch - N 2 -- ntasks - per - node 2 cbatch_test . sh cbatch -D /path test.sh cbatch -e error.log test.sh cbatch --export ALL test.sh cbatch --get-user-env test.sh cbatch -o output.out test.sh cbatch -q qos_test test.sh cbatch --repeat 3 test.sh","title":"cbatch \u63d0\u4ea4\u6279\u5904\u7406\u4f5c\u4e1a"},{"location":"command/cbatch/#_1","text":"\u53d8\u91cf\u540d \u8bf4\u660e CRANE_JOB_NODELIST \u4f5c\u4e1a\u5206\u914d\u7684\u8282\u70b9\u5217\u8868 %j \u4f5c\u4e1a\u53f7 \u4e0b\u9762\u4ecb\u7ecd\u63d0\u4ea4\u4e00\u4e2a\u8de8\u8282\u70b9\u591a\u6838\u5fc3\u7684\u4f8b\u5b50\uff1a c\u4e0b\u5217\u4f5c\u4e1a\u5c06\u5728\u4e09\u4e2a\u8282\u70b9\u4e0a\u8fd0\u884c\uff0c\u6bcf\u4e2a\u8282\u70b9\u4f7f\u75284\u4e2aCPU\u6838\u5fc3\u3002 #!/bin/bash #CBATCH -o crane_test%j.out #CBATCH -p CPU #CBATCH -J \"crane_test\" #CBATCH --nodes 3 #CBATCH --ntasks-per-node 4 #CBATCH -c 4 #CBATCH --time 50:00:00 # \u751f\u6210\u4f5c\u4e1a\u5206\u914d\u7684\u8282\u70b9\u7684machinefile echo \"$CRANE_JOB_NODELIST\" | tr \";\" \"\\n\" > crane.hosts #\u52a0\u8f7dMPI\u8fd0\u884c\u73af\u5883 module load mpich/4.0 #\u6267\u884c\u8de8\u8282\u70b9\u5e76\u884c\u4efb\u52a1 mpirun -n 13 -machinefile crane.hosts helloWorld > log","title":"\u5e38\u7528\u73af\u5883\u53d8\u91cf"},{"location":"command/ccancel/","text":"ccancel \u53d6\u6d88\u4f5c\u4e1a ccancel\u53ef\u4ee5\u7ec8\u6b62\u6b63\u5728\u8fd0\u884c\u6216\u8005\u5728\u6392\u961f\u4e2d\u7684\u4f5c\u4e1a\u3002 \u4e3b\u8981\u53c2\u6570 -h/--help : \u663e\u793a\u5e2e\u52a9 -A/--account string \uff1a\u53d6\u6d88\u8d26\u6237\u4e0b\u7684\u4efb\u52a1 -C/--config string \uff1a\u914d\u7f6e\u6587\u4ef6\u8def\u5f84(\u9ed8\u8ba4 \"/etc/crane/config.yaml\") -n/--name string \uff1a\u53d6\u6d88\u6307\u5b9a\u4efb\u52a1\u540d\u7684\u4efb\u52a1 -w/--nodes strings \uff1a\u53d6\u6d88\u6307\u5b9a\u8282\u70b9\u4e0a\u8fd0\u884c\u7684\u4efb\u52a1 -p/--partition string \uff1a\u53d6\u6d88\u6307\u5b9a\u5206\u533a\u4e0a\u8fd0\u884c\u7684\u4efb\u52a1 -t/--state string \uff1a\u53d6\u6d88\u67d0\u72b6\u6001\u7684\u4efb\u52a1\u3002\u6709\u6548\u7684\u4efb\u52a1\u72b6\u6001\u662f PENDING(PD)\u3001RUNNING(R)\u3002\u4efb\u52a1\u72b6\u6001\u4e0d\u533a\u5206\u5927\u5c0f\u5199 -u/--user string \uff1a\u53d6\u6d88\u7279\u5b9a\u7528\u6237\u63d0\u4ea4\u7684\u4efb\u52a1 -v/--version\uff1a \u67e5\u8be2\u7248\u672c\u53f7 \u4f8b\uff1a ccancel - w crane02 ccancel - t Pending \u53d6\u6d88\u4f5c\u4e1a\u53f7\u4e3a30686\u7684\u4f5c\u4e1a\uff1a ccancel 30686 ccancel\u8fd0\u884c\u7ed3\u679c\u5c55\u793a \u53d6\u6d88\u4f5c\u4e1a\u540d\u4e3atest1\u7684\u4f5c\u4e1a\uff1a ccancel - n test1 ccancel\u8fd0\u884c\u7ed3\u679c\u5c55\u793a \u53d6\u6d88CPU\u5206\u533a\u4e0a\u7684\u4f5c\u4e1a ccancel - p GPU ccancel -A PKU ccancel -u ROOT \u53d6\u6d88\u4f5c\u4e1a\u4e4b\u540e\uff0c\u5982\u679c\u88ab\u5206\u914d\u8282\u70b9\u4e0a\u6ca1\u6709\u7528\u6237\u7684\u5176\u4ed6\u4f5c\u4e1a\uff0c\u4f5c\u4e1a\u8c03\u5ea6\u7cfb\u7edf\u4f1a\u7ec8\u6b62\u7528\u6237\u5728\u6240\u5206\u914d\u8282\u70b9\u7684\u6240\u6709\u8fdb\u7a0b\uff0c\u5e76\u53d6\u6d88\u7528\u6237\u5728\u6240\u5206\u914d\u8282\u70b9\u4e0a\u7684ssh\u6743\u9650\u3002","title":"ccancel"},{"location":"command/ccancel/#ccancel","text":"ccancel\u53ef\u4ee5\u7ec8\u6b62\u6b63\u5728\u8fd0\u884c\u6216\u8005\u5728\u6392\u961f\u4e2d\u7684\u4f5c\u4e1a\u3002","title":"ccancel \u53d6\u6d88\u4f5c\u4e1a"},{"location":"command/ccancel/#_1","text":"-h/--help : \u663e\u793a\u5e2e\u52a9 -A/--account string \uff1a\u53d6\u6d88\u8d26\u6237\u4e0b\u7684\u4efb\u52a1 -C/--config string \uff1a\u914d\u7f6e\u6587\u4ef6\u8def\u5f84(\u9ed8\u8ba4 \"/etc/crane/config.yaml\") -n/--name string \uff1a\u53d6\u6d88\u6307\u5b9a\u4efb\u52a1\u540d\u7684\u4efb\u52a1 -w/--nodes strings \uff1a\u53d6\u6d88\u6307\u5b9a\u8282\u70b9\u4e0a\u8fd0\u884c\u7684\u4efb\u52a1 -p/--partition string \uff1a\u53d6\u6d88\u6307\u5b9a\u5206\u533a\u4e0a\u8fd0\u884c\u7684\u4efb\u52a1 -t/--state string \uff1a\u53d6\u6d88\u67d0\u72b6\u6001\u7684\u4efb\u52a1\u3002\u6709\u6548\u7684\u4efb\u52a1\u72b6\u6001\u662f PENDING(PD)\u3001RUNNING(R)\u3002\u4efb\u52a1\u72b6\u6001\u4e0d\u533a\u5206\u5927\u5c0f\u5199 -u/--user string \uff1a\u53d6\u6d88\u7279\u5b9a\u7528\u6237\u63d0\u4ea4\u7684\u4efb\u52a1 -v/--version\uff1a \u67e5\u8be2\u7248\u672c\u53f7 \u4f8b\uff1a ccancel - w crane02 ccancel - t Pending \u53d6\u6d88\u4f5c\u4e1a\u53f7\u4e3a30686\u7684\u4f5c\u4e1a\uff1a ccancel 30686 ccancel\u8fd0\u884c\u7ed3\u679c\u5c55\u793a \u53d6\u6d88\u4f5c\u4e1a\u540d\u4e3atest1\u7684\u4f5c\u4e1a\uff1a ccancel - n test1 ccancel\u8fd0\u884c\u7ed3\u679c\u5c55\u793a \u53d6\u6d88CPU\u5206\u533a\u4e0a\u7684\u4f5c\u4e1a ccancel - p GPU ccancel -A PKU ccancel -u ROOT \u53d6\u6d88\u4f5c\u4e1a\u4e4b\u540e\uff0c\u5982\u679c\u88ab\u5206\u914d\u8282\u70b9\u4e0a\u6ca1\u6709\u7528\u6237\u7684\u5176\u4ed6\u4f5c\u4e1a\uff0c\u4f5c\u4e1a\u8c03\u5ea6\u7cfb\u7edf\u4f1a\u7ec8\u6b62\u7528\u6237\u5728\u6240\u5206\u914d\u8282\u70b9\u7684\u6240\u6709\u8fdb\u7a0b\uff0c\u5e76\u53d6\u6d88\u7528\u6237\u5728\u6240\u5206\u914d\u8282\u70b9\u4e0a\u7684ssh\u6743\u9650\u3002","title":"\u4e3b\u8981\u53c2\u6570"},{"location":"command/ccontrol/","text":"ccontrol\u8bed\u6cd5 \u547d\u4ee4\u4e3b\u8981\u7ed3\u6784 ccontrol\u53ef\u4ee5\u67e5\u770b\u5206\u533a\u548c\u8282\u70b9\u7684\u72b6\u6001\u3002 \u4e3b\u8981\u547d\u4ee4 help \uff1a\u663e\u793a\u5e2e\u52a9 show \uff1a\u663e\u793a\u5b9e\u4f53\u7684\u72b6\u6001\uff0c\u9ed8\u8ba4\u4e3a\u6240\u6709\u8bb0\u5f55 update \uff1a\u4fee\u6539\u4f5c\u4e1a/\u5206\u533a/\u8282\u70b9\u4fe1\u606f hold \uff1a\u6682\u505c\u4f5c\u4e1a\u8c03\u5ea6 release \uff1a\u7ee7\u7eed\u4f5c\u4e1a\u8c03\u5ea6 create\uff1a \u521b\u5efa\u4e00\u4e2a\u65b0\u5b9e\u4f53 delete\uff1a \u5220\u9664\u6307\u5b9a\u7684\u5b9e\u4f53 \u652f\u6301\u7684\u547d\u4ee4\u884c\u9009\u9879\uff1a -h/--help : \u663e\u793a\u5e2e\u52a9 --json \uff1ajson\u683c\u5f0f\u8f93\u51fa\u547d\u4ee4\u6267\u884c\u7ed3\u679c -v/--version\uff1a \u67e5\u8be2\u7248\u672c\u53f7 -C/--config string \uff1a\u914d\u7f6e\u6587\u4ef6\u8def\u5f84(\u9ed8\u8ba4 \"/etc/crane/config.yaml\") \u5404\u547d\u4ee4\u4f7f\u7528\u65b9\u5f0f \u67e5\u770b \u67e5\u770b\u5206\u533a\u72b6\u6001 \u65e0\u53ef\u9009\u53c2\u6570 eg: ccontrol show partition \u8fd0\u884c\u622a\u56fe \uff1a \u67e5\u770b\u8282\u70b9\u72b6\u6001 \u65e0\u53ef\u9009\u53c2\u6570 eg: ccontrol show node \u8fd0\u884c\u622a\u56fe \uff1a \u67e5\u770b\u4f5c\u4e1a\u72b6\u6001 \u65e0\u53ef\u9009\u53c2\u6570 eg: ccontrol show job \u8fd0\u884c\u622a\u56fe \uff1a \u67e5\u770b\u9884\u5b9a\u72b6\u6001 \u65e0\u53ef\u9009\u53c2\u6570 eg\uff1a ccontrol show reservation \u8fd0\u884c\u622a\u56fe \uff1a \u4fee\u6539 \u4fee\u6539\u4f5c\u4e1a\u4fe1\u606f \u53ef\u9009\u53c2\u6570\uff1a priority\uff1a\u4f18\u5148\u7ea7 timelimit\uff1a\u65f6\u95f4\u9650\u5236(\u91c7\u7528\u65f6\u95f4\u683c\u5f0f) eg\uff1a ccontrol update jobid=11 priority=1 timelimit=01:00:00 \u8fd0\u884c\u622a\u56fe \uff1a \u4fee\u6539\u8282\u70b9\u4fe1\u606f \u5fc5\u586b\u53c2\u6570\uff1a state\uff1a\u72b6\u6001 reason\uff1a\u539f\u56e0 eg: ccontrol update nodename=test_node state=drain reason=\"test\" \u8fd0\u884c\u622a\u56fe \uff1a \u4fee\u6539\u5206\u533a\u4fe1\u606f \u53ef\u9009\u53c2\u6570\uff1a accounts\uff1a\u5141\u8bb8\u7528\u6237 deniedaccounts\uff1a\u62d2\u7edd\u7684\u7528\u6237 eg\uff1a ccontrol update partition=test accounts=test_user \u8fd0\u884c\u622a\u56fe \uff1a \u6682\u505c/\u6062\u590d \u6682\u505c\u4f5c\u4e1a \u53ef\u9009\u53c2\u6570\uff1a timelimit:\u65f6\u95f4\u9650\u5236(\u91c7\u7528\u65f6\u95f4\u683c\u5f0f) eg: ccontrol hold 1 timelimit=01:00:00 \u8fd0\u884c\u622a\u56fe \uff1a \u6062\u590d\u4f5c\u4e1a \u65e0\u53ef\u9009\u53c2\u6570 eg: ccontrol release 1 \u8fd0\u884c\u622a\u56fe \uff1a \u521b\u5efa/\u5220\u9664 \u521b\u5efa\u9884\u8ba2 \u5fc5\u9009\u53c2\u6570\uff1a starttime\uff1a\u5f00\u59cb\u65f6\u95f4\uff08\u91c7\u7528\u65e5\u671f|\u65f6\u95f4\u683c\u5f0f\uff09 duration\uff1a\u957f\u5ea6(\u91c7\u7528\u65f6\u95f4\u683c\u5f0f) account\uff1a\u4f7f\u7528\u8d26\u6237 \u53ef\u9009\u53c2\u6570\uff1a partition\uff1a\u4f7f\u7528\u5206\u533a nodes\uff1a\u4f7f\u7528\u8282\u70b9 user\uff1a\u4f7f\u7528\u7528\u6237 eg\uff1a ccontrol create reservation test_reservation duration=01:00:00 partition=test_partition nodes=test_node account=test_account \u6ce8\u610f\uff1a \u8fd9\u91ccuser\u652f\u6301=+\u548c=-\u7684\u7528\u6cd5\uff0c\u5f53=+\u8868\u793a\u5141\u8bb8\u7528\u6237\uff0c\u5f53=-\u7684\u65f6\u5019\u8868\u793a\u5220\u9664\u67d0\u7528\u6237 \u8fd0\u884c\u622a\u56fe \uff1a \u5220\u9664\u9884\u8ba2 \u65e0\u53ef\u9009\u53c2\u6570 eg: ccontrol delete reservation test_reservation \u8fd0\u884c\u622a\u56fe \uff1a \u5220\u9664\u7684\u547d\u4ee4 \u7531\u4e8e\u5220\u9664\u4e86cobra\u6846\u67b6\uff0c\u73b0\u5728completion\u81ea\u52a8\u8865\u5168\u4ee3\u7801\u547d\u4ee4\u5df2\u7ecf\u65e0\u6cd5\u4f7f\u7528 \u6587\u4ef6\u7ed3\u6784 help.go\uff1a\u7531\u4e8e\u5220\u9664\u4e86cobra\u6846\u67b6\uff0c\u73b0\u5728help\u547d\u4ee4\u7684\u8f93\u51fa\u7f16\u5199\u5728help.go\u4e2d ccontrol.go\uff1a\u5411\u540e\u7aef\u901a\u4fe1\u7684\u51fd\u6570\u5b9e\u73b0 CmdArgParser.go\uff1a\u547d\u4ee4\u89e3\u6790\u6587\u4ef6\uff0c\u5c06\u4f7f\u7528\u89e3\u6790\u5230\u7684\u547d\u4ee4\u884c\u6570\u636e\u6765\u8c03\u7528ccontrol.go\u7684\u51fd\u6570 parser.go\uff1a\u89e3\u6790\u5668\u51fd\u6570\uff0c\u5b9a\u4e49\u6570\u636e\u7ed3\u6784\u4e0e\u89e3\u6790\u89c4\u5219","title":"ccontrol"},{"location":"command/ccontrol/#ccontrol","text":"","title":"ccontrol\u8bed\u6cd5"},{"location":"command/ccontrol/#_1","text":"ccontrol\u53ef\u4ee5\u67e5\u770b\u5206\u533a\u548c\u8282\u70b9\u7684\u72b6\u6001\u3002 \u4e3b\u8981\u547d\u4ee4 help \uff1a\u663e\u793a\u5e2e\u52a9 show \uff1a\u663e\u793a\u5b9e\u4f53\u7684\u72b6\u6001\uff0c\u9ed8\u8ba4\u4e3a\u6240\u6709\u8bb0\u5f55 update \uff1a\u4fee\u6539\u4f5c\u4e1a/\u5206\u533a/\u8282\u70b9\u4fe1\u606f hold \uff1a\u6682\u505c\u4f5c\u4e1a\u8c03\u5ea6 release \uff1a\u7ee7\u7eed\u4f5c\u4e1a\u8c03\u5ea6 create\uff1a \u521b\u5efa\u4e00\u4e2a\u65b0\u5b9e\u4f53 delete\uff1a \u5220\u9664\u6307\u5b9a\u7684\u5b9e\u4f53 \u652f\u6301\u7684\u547d\u4ee4\u884c\u9009\u9879\uff1a -h/--help : \u663e\u793a\u5e2e\u52a9 --json \uff1ajson\u683c\u5f0f\u8f93\u51fa\u547d\u4ee4\u6267\u884c\u7ed3\u679c -v/--version\uff1a \u67e5\u8be2\u7248\u672c\u53f7 -C/--config string \uff1a\u914d\u7f6e\u6587\u4ef6\u8def\u5f84(\u9ed8\u8ba4 \"/etc/crane/config.yaml\")","title":"\u547d\u4ee4\u4e3b\u8981\u7ed3\u6784"},{"location":"command/ccontrol/#_2","text":"","title":"\u5404\u547d\u4ee4\u4f7f\u7528\u65b9\u5f0f"},{"location":"command/ccontrol/#_3","text":"\u67e5\u770b\u5206\u533a\u72b6\u6001 \u65e0\u53ef\u9009\u53c2\u6570 eg: ccontrol show partition \u8fd0\u884c\u622a\u56fe \uff1a \u67e5\u770b\u8282\u70b9\u72b6\u6001 \u65e0\u53ef\u9009\u53c2\u6570 eg: ccontrol show node \u8fd0\u884c\u622a\u56fe \uff1a \u67e5\u770b\u4f5c\u4e1a\u72b6\u6001 \u65e0\u53ef\u9009\u53c2\u6570 eg: ccontrol show job \u8fd0\u884c\u622a\u56fe \uff1a \u67e5\u770b\u9884\u5b9a\u72b6\u6001 \u65e0\u53ef\u9009\u53c2\u6570 eg\uff1a ccontrol show reservation \u8fd0\u884c\u622a\u56fe \uff1a","title":"\u67e5\u770b"},{"location":"command/ccontrol/#_4","text":"\u4fee\u6539\u4f5c\u4e1a\u4fe1\u606f \u53ef\u9009\u53c2\u6570\uff1a priority\uff1a\u4f18\u5148\u7ea7 timelimit\uff1a\u65f6\u95f4\u9650\u5236(\u91c7\u7528\u65f6\u95f4\u683c\u5f0f) eg\uff1a ccontrol update jobid=11 priority=1 timelimit=01:00:00 \u8fd0\u884c\u622a\u56fe \uff1a \u4fee\u6539\u8282\u70b9\u4fe1\u606f \u5fc5\u586b\u53c2\u6570\uff1a state\uff1a\u72b6\u6001 reason\uff1a\u539f\u56e0 eg: ccontrol update nodename=test_node state=drain reason=\"test\" \u8fd0\u884c\u622a\u56fe \uff1a \u4fee\u6539\u5206\u533a\u4fe1\u606f \u53ef\u9009\u53c2\u6570\uff1a accounts\uff1a\u5141\u8bb8\u7528\u6237 deniedaccounts\uff1a\u62d2\u7edd\u7684\u7528\u6237 eg\uff1a ccontrol update partition=test accounts=test_user \u8fd0\u884c\u622a\u56fe \uff1a","title":"\u4fee\u6539"},{"location":"command/ccontrol/#_5","text":"\u6682\u505c\u4f5c\u4e1a \u53ef\u9009\u53c2\u6570\uff1a timelimit:\u65f6\u95f4\u9650\u5236(\u91c7\u7528\u65f6\u95f4\u683c\u5f0f) eg: ccontrol hold 1 timelimit=01:00:00 \u8fd0\u884c\u622a\u56fe \uff1a \u6062\u590d\u4f5c\u4e1a \u65e0\u53ef\u9009\u53c2\u6570 eg: ccontrol release 1 \u8fd0\u884c\u622a\u56fe \uff1a","title":"\u6682\u505c/\u6062\u590d"},{"location":"command/ccontrol/#_6","text":"\u521b\u5efa\u9884\u8ba2 \u5fc5\u9009\u53c2\u6570\uff1a starttime\uff1a\u5f00\u59cb\u65f6\u95f4\uff08\u91c7\u7528\u65e5\u671f|\u65f6\u95f4\u683c\u5f0f\uff09 duration\uff1a\u957f\u5ea6(\u91c7\u7528\u65f6\u95f4\u683c\u5f0f) account\uff1a\u4f7f\u7528\u8d26\u6237 \u53ef\u9009\u53c2\u6570\uff1a partition\uff1a\u4f7f\u7528\u5206\u533a nodes\uff1a\u4f7f\u7528\u8282\u70b9 user\uff1a\u4f7f\u7528\u7528\u6237 eg\uff1a ccontrol create reservation test_reservation duration=01:00:00 partition=test_partition nodes=test_node account=test_account \u6ce8\u610f\uff1a \u8fd9\u91ccuser\u652f\u6301=+\u548c=-\u7684\u7528\u6cd5\uff0c\u5f53=+\u8868\u793a\u5141\u8bb8\u7528\u6237\uff0c\u5f53=-\u7684\u65f6\u5019\u8868\u793a\u5220\u9664\u67d0\u7528\u6237 \u8fd0\u884c\u622a\u56fe \uff1a \u5220\u9664\u9884\u8ba2 \u65e0\u53ef\u9009\u53c2\u6570 eg: ccontrol delete reservation test_reservation \u8fd0\u884c\u622a\u56fe \uff1a","title":"\u521b\u5efa/\u5220\u9664"},{"location":"command/ccontrol/#_7","text":"\u7531\u4e8e\u5220\u9664\u4e86cobra\u6846\u67b6\uff0c\u73b0\u5728completion\u81ea\u52a8\u8865\u5168\u4ee3\u7801\u547d\u4ee4\u5df2\u7ecf\u65e0\u6cd5\u4f7f\u7528","title":"\u5220\u9664\u7684\u547d\u4ee4"},{"location":"command/ccontrol/#_8","text":"help.go\uff1a\u7531\u4e8e\u5220\u9664\u4e86cobra\u6846\u67b6\uff0c\u73b0\u5728help\u547d\u4ee4\u7684\u8f93\u51fa\u7f16\u5199\u5728help.go\u4e2d ccontrol.go\uff1a\u5411\u540e\u7aef\u901a\u4fe1\u7684\u51fd\u6570\u5b9e\u73b0 CmdArgParser.go\uff1a\u547d\u4ee4\u89e3\u6790\u6587\u4ef6\uff0c\u5c06\u4f7f\u7528\u89e3\u6790\u5230\u7684\u547d\u4ee4\u884c\u6570\u636e\u6765\u8c03\u7528ccontrol.go\u7684\u51fd\u6570 parser.go\uff1a\u89e3\u6790\u5668\u51fd\u6570\uff0c\u5b9a\u4e49\u6570\u636e\u7ed3\u6784\u4e0e\u89e3\u6790\u89c4\u5219","title":"\u6587\u4ef6\u7ed3\u6784"},{"location":"command/ceff/","text":"ceff \u67e5\u770b\u4f5c\u4e1a\u8fd0\u884c\u5b9e\u51b5 ceff \u7528\u6765\u663e\u793a\u4f5c\u4e1a\u8fd0\u884c\u8fc7\u7a0b\u4e2d\u7684\u5b9e\u65f6\u60c5\u51b5\u3002 \u67e5\u770b\u4efb\u52a1\u5b9e\u65f6\u60c5\u51b5\uff1a ceff \u4f5c\u4e1a\u540d,\u4f5c\u4e1a\u540d ceff\u8fd0\u884c\u7ed3\u679c\u5c55\u793a \u4e3b\u8981\u8f93\u51fa\u9879 JobId : \u4f5c\u4e1a\u7684\u552f\u4e00\u6807\u8bc6\u7b26\u3002 Qos : \u4f5c\u4e1a\u8fd0\u884c\u6240\u5728\u7684\u96c6\u7fa4\u540d\u79f0\u3002 User/Group : \u63d0\u4ea4\u4f5c\u4e1a\u7684\u7528\u6237\u548c\u7528\u6237\u7ec4\u3002 Account \uff1a\u8d26\u6237\u540d State : \u4f5c\u4e1a\u7684\u5f53\u524d\u72b6\u6001\uff08\u4f8b\u5982\uff0cCOMPLETED\u3001FAILED\u3001CANCELLED \u7b49\uff09\u3002 Cores : \u4f5c\u4e1a\u4f7f\u7528\u7684\u6838\u5fc3\u6570\u91cf\u3002 Nodes : \u4f5c\u4e1a\u5206\u914d\u7684\u8282\u70b9\u6570\u91cf\u3002 Cores per node : \u6bcf\u4e2a\u8282\u70b9\u5206\u914d\u7684\u6838\u5fc3\u6570\u91cf\u3002 CPU Utilized : \u4f5c\u4e1a\u5b9e\u9645\u4f7f\u7528\u7684 CPU \u65f6\u95f4\u3002 CPU Efficiency : CPU \u4f7f\u7528\u6548\u7387\uff0c\u901a\u5e38\u8868\u793a\u4e3a\u4f5c\u4e1a\u5b9e\u9645\u4f7f\u7528\u7684 CPU \u65f6\u95f4\u5360\u5206\u914d\u7684\u6838\u5fc3\u5899\u65f6\u95f4\u7684\u767e\u5206\u6bd4\u3002 Job Wall-clock time : \u4f5c\u4e1a\u7684\u5899\u949f\u65f6\u95f4\uff0c\u5373\u4f5c\u4e1a\u4ece\u5f00\u59cb\u5230\u7ed3\u675f\u7684\u603b\u65f6\u95f4\u3002 Memory Utilized : \u4f5c\u4e1a\u5b9e\u9645\u4f7f\u7528\u7684\u5185\u5b58\u91cf\u3002 Memory Efficiency : \u5185\u5b58\u4f7f\u7528\u6548\u7387\uff0c\u901a\u5e38\u8868\u793a\u4e3a\u4f5c\u4e1a\u5b9e\u9645\u4f7f\u7528\u7684\u5185\u5b58\u91cf\u5360\u5206\u914d\u5185\u5b58\u7684\u767e\u5206\u6bd4\u3002 \u4e3b\u8981\u53c2\u6570 -h/--help : \u663e\u793a\u5e2e\u52a9 -C/--config string\uff1a \u914d\u7f6e\u6587\u4ef6\u8def\u5f84(\u9ed8\u8ba4\u4e3a \"/etc/crane/config.yaml\") -- json \u8f93\u51fa\u540e\u7aef\u8fd4\u56de\u4efb\u52a1\u4fe1\u606f -v, --version \u663e\u793aceff \u7684\u7248\u672c \u4f8b\uff1a ceff - h ceff \u4f5c\u4e1a id --json","title":"ceff"},{"location":"command/ceff/#ceff","text":"ceff \u7528\u6765\u663e\u793a\u4f5c\u4e1a\u8fd0\u884c\u8fc7\u7a0b\u4e2d\u7684\u5b9e\u65f6\u60c5\u51b5\u3002 \u67e5\u770b\u4efb\u52a1\u5b9e\u65f6\u60c5\u51b5\uff1a ceff \u4f5c\u4e1a\u540d,\u4f5c\u4e1a\u540d ceff\u8fd0\u884c\u7ed3\u679c\u5c55\u793a \u4e3b\u8981\u8f93\u51fa\u9879 JobId : \u4f5c\u4e1a\u7684\u552f\u4e00\u6807\u8bc6\u7b26\u3002 Qos : \u4f5c\u4e1a\u8fd0\u884c\u6240\u5728\u7684\u96c6\u7fa4\u540d\u79f0\u3002 User/Group : \u63d0\u4ea4\u4f5c\u4e1a\u7684\u7528\u6237\u548c\u7528\u6237\u7ec4\u3002 Account \uff1a\u8d26\u6237\u540d State : \u4f5c\u4e1a\u7684\u5f53\u524d\u72b6\u6001\uff08\u4f8b\u5982\uff0cCOMPLETED\u3001FAILED\u3001CANCELLED \u7b49\uff09\u3002 Cores : \u4f5c\u4e1a\u4f7f\u7528\u7684\u6838\u5fc3\u6570\u91cf\u3002 Nodes : \u4f5c\u4e1a\u5206\u914d\u7684\u8282\u70b9\u6570\u91cf\u3002 Cores per node : \u6bcf\u4e2a\u8282\u70b9\u5206\u914d\u7684\u6838\u5fc3\u6570\u91cf\u3002 CPU Utilized : \u4f5c\u4e1a\u5b9e\u9645\u4f7f\u7528\u7684 CPU \u65f6\u95f4\u3002 CPU Efficiency : CPU \u4f7f\u7528\u6548\u7387\uff0c\u901a\u5e38\u8868\u793a\u4e3a\u4f5c\u4e1a\u5b9e\u9645\u4f7f\u7528\u7684 CPU \u65f6\u95f4\u5360\u5206\u914d\u7684\u6838\u5fc3\u5899\u65f6\u95f4\u7684\u767e\u5206\u6bd4\u3002 Job Wall-clock time : \u4f5c\u4e1a\u7684\u5899\u949f\u65f6\u95f4\uff0c\u5373\u4f5c\u4e1a\u4ece\u5f00\u59cb\u5230\u7ed3\u675f\u7684\u603b\u65f6\u95f4\u3002 Memory Utilized : \u4f5c\u4e1a\u5b9e\u9645\u4f7f\u7528\u7684\u5185\u5b58\u91cf\u3002 Memory Efficiency : \u5185\u5b58\u4f7f\u7528\u6548\u7387\uff0c\u901a\u5e38\u8868\u793a\u4e3a\u4f5c\u4e1a\u5b9e\u9645\u4f7f\u7528\u7684\u5185\u5b58\u91cf\u5360\u5206\u914d\u5185\u5b58\u7684\u767e\u5206\u6bd4\u3002","title":"ceff \u67e5\u770b\u4f5c\u4e1a\u8fd0\u884c\u5b9e\u51b5"},{"location":"command/ceff/#_1","text":"-h/--help : \u663e\u793a\u5e2e\u52a9 -C/--config string\uff1a \u914d\u7f6e\u6587\u4ef6\u8def\u5f84(\u9ed8\u8ba4\u4e3a \"/etc/crane/config.yaml\") -- json \u8f93\u51fa\u540e\u7aef\u8fd4\u56de\u4efb\u52a1\u4fe1\u606f -v, --version \u663e\u793aceff \u7684\u7248\u672c \u4f8b\uff1a ceff - h ceff \u4f5c\u4e1a id --json","title":"\u4e3b\u8981\u53c2\u6570"},{"location":"command/cinfo/","text":"cinfo \u67e5\u770b\u8282\u70b9\u4e0e\u5206\u533a\u72b6\u6001 cinfo\u53ef\u67e5\u8be2\u5404\u5206\u533a\u8282\u70b9\u7684\u961f\u5217\u8d44\u6e90\u4fe1\u606f\u3002 \u67e5\u770b\u5206\u533a\u8282\u70b9\u72b6\u6001\uff1a cinfo cinfo\u8fd0\u884c\u7ed3\u679c\u5c55\u793a \u4e3b\u8981\u8f93\u51fa\u9879 - PARTITION \uff1a\u5206\u533a\u540d - AVAIL \uff1a \u5206\u533a\u72b6\u6001 - up: \u53ef\u7528 - down:\u4e0d\u53ef\u7528 - NODES \uff1a\u8282\u70b9\u6570 - STATE \uff1a \u8282\u70b9\u72b6\u6001 - idle \uff1a \u7a7a\u95f2 - mix\uff1a \u8282\u70b9\u90e8\u5206\u6838\u5fc3\u53ef\u4ee5\u4f7f\u7528 - alloc\uff1a \u8282\u70b9\u5df2\u88ab\u5360\u7528 - down\uff1a \u8282\u70b9\u4e0d\u53ef\u7528 - NODELIST \uff1a \u8282\u70b9\u5217\u8868 \u4e3b\u8981\u53c2\u6570 - -h/--help : \u663e\u793a\u5e2e\u52a9 - -C/--config string \uff1a\u914d\u7f6e\u6587\u4ef6\u8def\u5f84(\u9ed8\u8ba4\u4e3a \"/etc/crane/config.yaml\") - -d/--dead \uff1a\u53ea\u663e\u793a\u65e0\u54cd\u5e94\u8282\u70b9 - -i/--iterate uint \uff1a\u6307\u5b9a\u95f4\u9694\u79d2\u6570\u5237\u65b0\u67e5\u8be2\u7ed3\u679c\u3002\u5982 -i=3\u8868\u793a\u6bcf\u9694\u4e09\u79d2\u8f93\u51fa\u4e00\u6b21\u67e5\u8be2\u7ed3\u679c - -o/--format string \u6307\u5b9a\u8f93\u51fa\u683c\u5f0f - % p /% Partition \u2014\u2014 \u663e\u793a\u5f53\u524d\u73af\u5883\u4e2d\u7684\u6240\u6709\u5206\u533a - % a/% Avail \u2014\u2014 \u663e\u793a\u8282\u70b9\u7684\u72b6\u6001 - % n/% Nodes \u2014\u2014 \u663e\u793a\u5206\u533a\u8282\u70b9\u7684\u6570\u91cf - % s/% State \u2014\u2014 \u663e\u793a\u5206\u533a\u8282\u70b9\u7684\u72b6\u6001 - % l/% NodeList \u2014\u2014 \u663e\u793a\u5206\u533a\u4e2d\u7684\u6240\u6709\u8282\u70b9\u5217\u8868 \u6bcf\u4e2a\u683c\u5f0f\u6807\u8bc6\u7b26\u6216\u5b57\u7b26\u4e32\u90fd\u53ef\u4ee5\u7528\u5bbd\u5ea6\u8bf4\u660e\u7b26\u4fee\u6539\uff08\u4f8b\u5982\uff0c\u201c%.5j\u201d \uff09\u3002\u5982\u679c\u6307\u5b9a\u4e86\u5bbd\u5ea6\uff0c\u5b57\u6bb5\u5c06\u88ab\u683c\u5f0f\u5316\u4e3a\u81f3\u5c11\u8fbe\u5230\u8be5\u5bbd\u5ea6\u3002\u5982\u679c\u683c\u5f0f\u65e0\u6548\u6216\u65e0\u6cd5\u8bc6\u522b\uff0c\u7a0b\u5e8f\u5c06\u62a5\u9519\u5e76\u7ec8\u6b62\u3002 --json \uff1ajson\u683c\u5f0f\u8f93\u51fa\u547d\u4ee4\u6267\u884c\u7ed3\u679c -n/--nodes string \uff1a\u663e\u793a\u6307\u5b9a\u8282\u70b9\u4fe1\u606f\uff0c\u591a\u4e2a\u8282\u70b9\u7528\u9017\u53f7\u9694\u5f00\u3002\u4f8b\uff1acinfo -n crane01,crane02 -N/--noheader \uff1a\u8f93\u51fa\u9690\u85cf\u8868\u5934 -p/--partition string \uff1a\u663e\u793a\u6307\u5b9a\u5206\u533a\u4fe1\u606f\uff0c\u591a\u4e2a\u5206\u533a\u7528\u9017\u53f7\u9694\u5f00\u3002\u4f8b\uff1acinfo -p CPU,GPU -r/--responding \uff1a\u53ea\u663e\u793a\u6709\u54cd\u5e94\u8282\u70b9 -t/--states string \uff1a\u4ec5\u663e\u793a\u72b6\u6001\u7684\u4fe1\u606f\u3002\u72b6\u6001\u53ef\u4ee5\u4e3a(\u4e0d\u533a\u5206\u5927\u5c0f\u5199): IDLE, MIX, ALLOC\u548cDOWN -v/--version \uff1a\u67e5\u8be2\u7248\u672c\u53f7 \u4f8b\uff1a cinfo cinfo -h cinfo -N cinfo -d cinfo -i 3 cinfo -n crane01,crane02,crane03 cinfo - p GPU , CPU cinfo - r cinfo - t IDLE cinfo -v","title":"cinfo"},{"location":"command/cinfo/#cinfo","text":"cinfo\u53ef\u67e5\u8be2\u5404\u5206\u533a\u8282\u70b9\u7684\u961f\u5217\u8d44\u6e90\u4fe1\u606f\u3002 \u67e5\u770b\u5206\u533a\u8282\u70b9\u72b6\u6001\uff1a cinfo cinfo\u8fd0\u884c\u7ed3\u679c\u5c55\u793a \u4e3b\u8981\u8f93\u51fa\u9879 - PARTITION \uff1a\u5206\u533a\u540d - AVAIL \uff1a \u5206\u533a\u72b6\u6001 - up: \u53ef\u7528 - down:\u4e0d\u53ef\u7528 - NODES \uff1a\u8282\u70b9\u6570 - STATE \uff1a \u8282\u70b9\u72b6\u6001 - idle \uff1a \u7a7a\u95f2 - mix\uff1a \u8282\u70b9\u90e8\u5206\u6838\u5fc3\u53ef\u4ee5\u4f7f\u7528 - alloc\uff1a \u8282\u70b9\u5df2\u88ab\u5360\u7528 - down\uff1a \u8282\u70b9\u4e0d\u53ef\u7528 - NODELIST \uff1a \u8282\u70b9\u5217\u8868 \u4e3b\u8981\u53c2\u6570 - -h/--help : \u663e\u793a\u5e2e\u52a9 - -C/--config string \uff1a\u914d\u7f6e\u6587\u4ef6\u8def\u5f84(\u9ed8\u8ba4\u4e3a \"/etc/crane/config.yaml\") - -d/--dead \uff1a\u53ea\u663e\u793a\u65e0\u54cd\u5e94\u8282\u70b9 - -i/--iterate uint \uff1a\u6307\u5b9a\u95f4\u9694\u79d2\u6570\u5237\u65b0\u67e5\u8be2\u7ed3\u679c\u3002\u5982 -i=3\u8868\u793a\u6bcf\u9694\u4e09\u79d2\u8f93\u51fa\u4e00\u6b21\u67e5\u8be2\u7ed3\u679c - -o/--format string \u6307\u5b9a\u8f93\u51fa\u683c\u5f0f - % p /% Partition \u2014\u2014 \u663e\u793a\u5f53\u524d\u73af\u5883\u4e2d\u7684\u6240\u6709\u5206\u533a - % a/% Avail \u2014\u2014 \u663e\u793a\u8282\u70b9\u7684\u72b6\u6001 - % n/% Nodes \u2014\u2014 \u663e\u793a\u5206\u533a\u8282\u70b9\u7684\u6570\u91cf - % s/% State \u2014\u2014 \u663e\u793a\u5206\u533a\u8282\u70b9\u7684\u72b6\u6001 - % l/% NodeList \u2014\u2014 \u663e\u793a\u5206\u533a\u4e2d\u7684\u6240\u6709\u8282\u70b9\u5217\u8868 \u6bcf\u4e2a\u683c\u5f0f\u6807\u8bc6\u7b26\u6216\u5b57\u7b26\u4e32\u90fd\u53ef\u4ee5\u7528\u5bbd\u5ea6\u8bf4\u660e\u7b26\u4fee\u6539\uff08\u4f8b\u5982\uff0c\u201c%.5j\u201d \uff09\u3002\u5982\u679c\u6307\u5b9a\u4e86\u5bbd\u5ea6\uff0c\u5b57\u6bb5\u5c06\u88ab\u683c\u5f0f\u5316\u4e3a\u81f3\u5c11\u8fbe\u5230\u8be5\u5bbd\u5ea6\u3002\u5982\u679c\u683c\u5f0f\u65e0\u6548\u6216\u65e0\u6cd5\u8bc6\u522b\uff0c\u7a0b\u5e8f\u5c06\u62a5\u9519\u5e76\u7ec8\u6b62\u3002 --json \uff1ajson\u683c\u5f0f\u8f93\u51fa\u547d\u4ee4\u6267\u884c\u7ed3\u679c -n/--nodes string \uff1a\u663e\u793a\u6307\u5b9a\u8282\u70b9\u4fe1\u606f\uff0c\u591a\u4e2a\u8282\u70b9\u7528\u9017\u53f7\u9694\u5f00\u3002\u4f8b\uff1acinfo -n crane01,crane02 -N/--noheader \uff1a\u8f93\u51fa\u9690\u85cf\u8868\u5934 -p/--partition string \uff1a\u663e\u793a\u6307\u5b9a\u5206\u533a\u4fe1\u606f\uff0c\u591a\u4e2a\u5206\u533a\u7528\u9017\u53f7\u9694\u5f00\u3002\u4f8b\uff1acinfo -p CPU,GPU -r/--responding \uff1a\u53ea\u663e\u793a\u6709\u54cd\u5e94\u8282\u70b9 -t/--states string \uff1a\u4ec5\u663e\u793a\u72b6\u6001\u7684\u4fe1\u606f\u3002\u72b6\u6001\u53ef\u4ee5\u4e3a(\u4e0d\u533a\u5206\u5927\u5c0f\u5199): IDLE, MIX, ALLOC\u548cDOWN -v/--version \uff1a\u67e5\u8be2\u7248\u672c\u53f7 \u4f8b\uff1a cinfo cinfo -h cinfo -N cinfo -d cinfo -i 3 cinfo -n crane01,crane02,crane03 cinfo - p GPU , CPU cinfo - r cinfo - t IDLE cinfo -v","title":"cinfo \u67e5\u770b\u8282\u70b9\u4e0e\u5206\u533a\u72b6\u6001"},{"location":"command/cqueue/","text":"cqueue \u67e5\u770b\u4f5c\u4e1a\u961f\u5217 cqueue\u53ef\u4ee5\u67e5\u770b\u961f\u5217\u4e2d\u7684\u4f5c\u4e1a\u4fe1\u606f\u3002 \u67e5\u770b\u96c6\u7fa4\u4e2d\u6240\u6709\u961f\u5217\u7684\u4f5c\u4e1a\u4fe1\u606f\uff08\u5305\u62ec\u72b6\u6001pending\u3001running\u3001cancelled\uff09\uff0c\u9ed8\u8ba4\u8f93\u51fa100\u6761\u4fe1\u606f\u3002 cqueue cqueue\u8fd0\u884c\u7ed3\u679c\u5c55\u793a \u4e3b\u8981\u8f93\u51fa\u9879 JobId \uff1a\u4f5c\u4e1a\u53f7 Partition \uff1a\u4f5c\u4e1a\u6240\u5728\u5206\u533a Name : \u4f5c\u4e1a\u540d User \uff1a\u4f5c\u4e1a\u6240\u5c5e\u7528\u6237 Account \uff1a\u4f5c\u4e1a\u6240\u5c5e\u8d26\u6237 Status \uff1a\u4f5c\u4e1a\u72b6\u6001 Type \uff1a \u4f5c\u4e1a\u7c7b\u578b TimeLimit \uff1a\u4f5c\u4e1a\u65f6\u95f4\u9650\u5236 Nodes \uff1a\u4f5c\u4e1a\u6240\u5206\u914d\u8282\u70b9\u6570 NodeList \uff1a \u4f5c\u4e1a\u8fd0\u884c\u7684\u8282\u70b9\u540d \u4e3b\u8981\u53c2\u6570 -A/--Account string \uff1a\u6307\u5b9a\u67e5\u8be2\u4f5c\u4e1a\u6240\u5c5e\u8d26\u6237\uff0c\u6307\u5b9a\u591a\u4e2a\u8d26\u6237\u65f6\u7528\u9017\u53f7\u9694\u5f00 -C/--config string \uff1a\u914d\u7f6e\u6587\u4ef6\u8def\u5f84 -o/--format string \uff1a\u6307\u5b9a\u8f93\u51fa\u683c\u5f0f\u3002\u7531\u767e\u5206\u53f7\uff08%\uff09\u540e\u63a5\u4e00\u4e2a\u5b57\u7b26\u6216\u5b57\u7b26\u4e32\u6807\u8bc6\u3002 \u5728 % \u548c\u683c\u5f0f\u5b57\u7b26/\u5b57\u7b26\u4e32\u4e4b\u95f4\u7528\u70b9\uff08.\uff09\u548c\u6570\u5b57\uff0c\u53ef\u6307\u5b9a\u5b57\u6bb5\u7684\u6700\u5c0f\u5bbd\u5ea6\u3002\u652f\u6301\u7684\u683c\u5f0f\u6807\u8bc6\u7b26\u6216\u5b57\u7b26\u4e32\uff08\u4e0d\u533a\u5206\u5927\u5c0f\u5199\uff09\uff1a %a/%Account\uff1a \u663e\u793a\u4f5c\u4e1a\u5173\u8054\u7684\u8d26\u6237 %c/%AllocCpus\uff1a \u663e\u793a\u4f5c\u4e1a\u5df2\u5206\u914d\u7684 CPU \u6570\u91cf %e/%CpuPerNode\uff1a \u663e\u793a\u4f5c\u4e1a\u6bcf\u4e2a\u8282\u70b9\u8bf7\u6c42\u7684 CPU \u6570\u91cf %h/%ElapsedTime\uff1a \u663e\u793a\u4f5c\u4e1a\u81ea\u542f\u52a8\u4ee5\u6765\u7684\u5df2\u7528\u65f6\u95f4 %j/%JobId\uff1a \u663e\u793a\u4f5c\u4e1a ID %k/%Comment\uff1a \u663e\u793a\u4f5c\u4e1a\u7684\u5907\u6ce8 %l/%NodeList\uff1a \u663e\u793a\u4f5c\u4e1a\u6b63\u5728\u8fd0\u884c\u7684\u8282\u70b9\u5217\u8868 %m/%TimeLimit\uff1a \u663e\u793a\u4f5c\u4e1a\u7684\u65f6\u95f4\u9650\u5236 %n/%MemPerNode\uff1a \u663e\u793a\u4f5c\u4e1a\u6bcf\u4e2a\u8282\u70b9\u8bf7\u6c42\u7684\u5185\u5b58\u91cf %N/%NodeNum\uff1a \u663e\u793a\u4f5c\u4e1a\u8bf7\u6c42\u7684\u8282\u70b9\u6570\u91cf %n/%Name\uff1a \u663e\u793a\u4f5c\u4e1a\u540d\u79f0 %P/%Partition\uff1a \u663e\u793a\u4f5c\u4e1a\u8fd0\u884c\u6240\u5728\u7684\u5206\u533a %p/%Priority\uff1a \u663e\u793a\u4f5c\u4e1a\u7684\u4f18\u5148\u7ea7 %Q/%QOS \uff1a\u663e\u793a\u4f5c\u4e1a\u7684\u670d\u52a1\u8d28\u91cf\uff08QoS\uff09\u7ea7\u522b %R/%Reason\uff1a \u663e\u793a\u4f5c\u4e1a\u6302\u8d77\u7684\u539f\u56e0 %r/%ReqNodes\uff1a \u663e\u793a\u4f5c\u4e1a\u8bf7\u6c42\u7684\u8282\u70b9 %S/%StartTime\uff1a \u663e\u793a\u4f5c\u4e1a\u7684\u5f00\u59cb\u65f6\u95f4 %s/%SubmitTime\uff1a \u663e\u793a\u4f5c\u4e1a\u7684\u63d0\u4ea4\u65f6\u95f4 %t/%State\uff1a \u663e\u793a\u4f5c\u4e1a\u7684\u5f53\u524d\u72b6\u6001 %T/%JobType\uff1a \u663e\u793a\u4f5c\u4e1a\u7c7b\u578b %u/%Uid\uff1a\u663e\u793a\u4f5c\u4e1a\u7684 UID %U/%User\uff1a \u663e\u793a\u63d0\u4ea4\u4f5c\u4e1a\u7684\u7528\u6237 %x/%ExcludeNodes\uff1a \u663e\u793a\u4f5c\u4e1a\u6392\u9664\u7684\u8282\u70b9 \u6bcf\u4e2a\u683c\u5f0f\u6807\u8bc6\u7b26\u6216\u5b57\u7b26\u4e32\u53ef\u7528\u5bbd\u5ea6\u8bf4\u660e\u7b26\u4fee\u6539\uff08\u5982 \"%.5j\" \uff09\u3002 \u82e5\u6307\u5b9a\u5bbd\u5ea6\uff0c\u5219\u4f1a\u88ab\u683c\u5f0f\u5316\u4e3a\u81f3\u5c11\u8fbe\u5230\u8be5\u5bbd\u5ea6\u3002 \u82e5\u683c\u5f0f\u65e0\u6548\u6216\u65e0\u6cd5\u8bc6\u522b\uff0c\u7a0b\u5e8f\u4f1a\u62a5\u9519\u5e76\u7ec8\u6b62\u3002 \u4f8b\uff1a --format \"%.5j %.20n %t\" \u4f1a\u8f93\u51fa\u4f5c\u4e1a ID\uff08\u6700\u5c0f\u5bbd\u5ea6 5\uff09\u3001\u540d\u79f0\uff08\u6700\u5c0f\u5bbd\u5ea6 20\uff09\u548c\u72b6\u6001\u3002 -F/--full : \u663e\u793a\u5b8c\u6574\u7684\u5185\u5bb9\uff0c\u5982\u679c\u672a\u6307\u5b9a\uff0c\u9ed8\u8ba4\u6bcf\u9879\u8f93\u51fa30\u4e2a\u5b57\u7b26 -h/--help : \u663e\u793a\u5e2e\u52a9 -i/--iterate uint \uff1a\u6307\u5b9a\u95f4\u9694\u79d2\u6570\u5237\u65b0\u67e5\u8be2\u7ed3\u679c\u3002\u5982 -i=3\u8868\u793a\u6bcf\u9694\u4e09\u79d2\u8f93\u51fa\u4e00\u6b21\u67e5\u8be2\u7ed3\u679c -j/--job string \uff1a\u6307\u5b9a\u67e5\u8be2\u4f5c\u4e1a\u53f7\uff0c\u6307\u5b9a\u591a\u4e2a\u4f5c\u4e1a\u53f7\u65f6\u7528\u9017\u53f7\u9694\u5f00\u3002\u5982 -j=2,3,4 --json\uff1a json\u683c\u5f0f\u8f93\u51fa\u547d\u4ee4\u6267\u884c\u7ed3\u679c -m/--MaxVisibleLines uint32 \uff1a\u6307\u5b9a\u8f93\u51fa\u7ed3\u679c\u7684\u6700\u5927\u6761\u6570\u3002\u5982-m=500\u8868\u793a\u6700\u591a\u8f93\u51fa500\u884c\u67e5\u8be2\u7ed3\u679c -n/--name string \uff1a\u6307\u5b9a\u67e5\u8be2\u4f5c\u4e1a\u540d\uff0c\u6307\u5b9a\u591a\u4e2a\u4f5c\u4e1a\u540d\u65f6\u7528\u9017\u53f7\u9694\u5f00 -N/--noHeader \uff1a\u8f93\u51fa\u9690\u85cf\u8868\u5934 -p/--partition string \uff1a\u6307\u5b9a\u67e5\u8be2\u4f5c\u4e1a\u6240\u5728\u5206\u533a\uff0c\u6307\u5b9a\u591a\u4e2a\u5206\u533a\u65f6\u7528\u9017\u53f7\u9694\u5f00 -q/--qos string \uff1a\u6307\u5b9a\u67e5\u8be2\u4f5c\u4e1a\u7684QoS\uff0c\u6307\u5b9a\u591a\u4e2aQoS\u65f6\u7528\u9017\u53f7\u9694\u5f00 --self\uff1a \u67e5\u770b\u5f53\u524d\u7528\u6237\u63d0\u4ea4\u7684\u4f5c\u4e1a -S/--start \uff1a\u663e\u793a\u4f5c\u4e1a\u7684\u5f00\u59cb\u65f6\u95f4\uff08pending\u4f5c\u4e1a\u663e\u793a\u9884\u671f\u5f00\u59cb\u65f6\u95f4\uff09 -t/--state string \uff1a\u6307\u5b9a\u67e5\u8be2\u4f5c\u4e1a\u72b6\u6001\uff0c\u6307\u5b9a\u591a\u4e2a\u72b6\u6001\u65f6\u7528\u9017\u53f7\u9694\u5f00 -u/--user string \uff1a\u6307\u5b9a\u67e5\u8be2\u4f5c\u4e1a\u6240\u5c5e\u7528\u6237\uff0c\u6307\u5b9a\u591a\u4e2a\u7528\u6237\u65f6\u7528\u9017\u53f7\u9694\u5f00 -v/--version\uff1a \u67e5\u8be2\u7248\u672c\u53f7 \u4f8b\uff1a cqueue - h cqueue - N cqueue - S cqueue - j 30674 , 30675 cqueue - t Pending cqueue -t r cqueue - u cranetest cqueue - A CraneTest cqueue - i 3 cqueue - p CPU cqueue - m 3 cqueue - o = \"%n %u %.5j %.5t %.3T %.5T\" format\u4e2d\u7684\u6307\u5b9a\u5217\u7684\u5bf9\u5e94\u7f29\u5199\u5bf9\u7167\uff1a j-TaskId\uff1bn-Name\uff1bt-State\uff1bp-Partition\uff1bu-User\uff1ba-Account\uff1bT-Type\uff1bI-NodeIndex\uff1bl-TimeLimit\uff1bN-Nodes cqueue -n test cqueue -N cqueue -q test_qos cqueue --self cqueue -t Running -S 2024 -01-02T15:04:05~2024-01-11T11:12:41","title":"cqueue"},{"location":"command/cqueue/#cqueue","text":"cqueue\u53ef\u4ee5\u67e5\u770b\u961f\u5217\u4e2d\u7684\u4f5c\u4e1a\u4fe1\u606f\u3002 \u67e5\u770b\u96c6\u7fa4\u4e2d\u6240\u6709\u961f\u5217\u7684\u4f5c\u4e1a\u4fe1\u606f\uff08\u5305\u62ec\u72b6\u6001pending\u3001running\u3001cancelled\uff09\uff0c\u9ed8\u8ba4\u8f93\u51fa100\u6761\u4fe1\u606f\u3002 cqueue cqueue\u8fd0\u884c\u7ed3\u679c\u5c55\u793a \u4e3b\u8981\u8f93\u51fa\u9879 JobId \uff1a\u4f5c\u4e1a\u53f7 Partition \uff1a\u4f5c\u4e1a\u6240\u5728\u5206\u533a Name : \u4f5c\u4e1a\u540d User \uff1a\u4f5c\u4e1a\u6240\u5c5e\u7528\u6237 Account \uff1a\u4f5c\u4e1a\u6240\u5c5e\u8d26\u6237 Status \uff1a\u4f5c\u4e1a\u72b6\u6001 Type \uff1a \u4f5c\u4e1a\u7c7b\u578b TimeLimit \uff1a\u4f5c\u4e1a\u65f6\u95f4\u9650\u5236 Nodes \uff1a\u4f5c\u4e1a\u6240\u5206\u914d\u8282\u70b9\u6570 NodeList \uff1a \u4f5c\u4e1a\u8fd0\u884c\u7684\u8282\u70b9\u540d","title":"cqueue \u67e5\u770b\u4f5c\u4e1a\u961f\u5217"},{"location":"command/cqueue/#_1","text":"-A/--Account string \uff1a\u6307\u5b9a\u67e5\u8be2\u4f5c\u4e1a\u6240\u5c5e\u8d26\u6237\uff0c\u6307\u5b9a\u591a\u4e2a\u8d26\u6237\u65f6\u7528\u9017\u53f7\u9694\u5f00 -C/--config string \uff1a\u914d\u7f6e\u6587\u4ef6\u8def\u5f84 -o/--format string \uff1a\u6307\u5b9a\u8f93\u51fa\u683c\u5f0f\u3002\u7531\u767e\u5206\u53f7\uff08%\uff09\u540e\u63a5\u4e00\u4e2a\u5b57\u7b26\u6216\u5b57\u7b26\u4e32\u6807\u8bc6\u3002 \u5728 % \u548c\u683c\u5f0f\u5b57\u7b26/\u5b57\u7b26\u4e32\u4e4b\u95f4\u7528\u70b9\uff08.\uff09\u548c\u6570\u5b57\uff0c\u53ef\u6307\u5b9a\u5b57\u6bb5\u7684\u6700\u5c0f\u5bbd\u5ea6\u3002\u652f\u6301\u7684\u683c\u5f0f\u6807\u8bc6\u7b26\u6216\u5b57\u7b26\u4e32\uff08\u4e0d\u533a\u5206\u5927\u5c0f\u5199\uff09\uff1a %a/%Account\uff1a \u663e\u793a\u4f5c\u4e1a\u5173\u8054\u7684\u8d26\u6237 %c/%AllocCpus\uff1a \u663e\u793a\u4f5c\u4e1a\u5df2\u5206\u914d\u7684 CPU \u6570\u91cf %e/%CpuPerNode\uff1a \u663e\u793a\u4f5c\u4e1a\u6bcf\u4e2a\u8282\u70b9\u8bf7\u6c42\u7684 CPU \u6570\u91cf %h/%ElapsedTime\uff1a \u663e\u793a\u4f5c\u4e1a\u81ea\u542f\u52a8\u4ee5\u6765\u7684\u5df2\u7528\u65f6\u95f4 %j/%JobId\uff1a \u663e\u793a\u4f5c\u4e1a ID %k/%Comment\uff1a \u663e\u793a\u4f5c\u4e1a\u7684\u5907\u6ce8 %l/%NodeList\uff1a \u663e\u793a\u4f5c\u4e1a\u6b63\u5728\u8fd0\u884c\u7684\u8282\u70b9\u5217\u8868 %m/%TimeLimit\uff1a \u663e\u793a\u4f5c\u4e1a\u7684\u65f6\u95f4\u9650\u5236 %n/%MemPerNode\uff1a \u663e\u793a\u4f5c\u4e1a\u6bcf\u4e2a\u8282\u70b9\u8bf7\u6c42\u7684\u5185\u5b58\u91cf %N/%NodeNum\uff1a \u663e\u793a\u4f5c\u4e1a\u8bf7\u6c42\u7684\u8282\u70b9\u6570\u91cf %n/%Name\uff1a \u663e\u793a\u4f5c\u4e1a\u540d\u79f0 %P/%Partition\uff1a \u663e\u793a\u4f5c\u4e1a\u8fd0\u884c\u6240\u5728\u7684\u5206\u533a %p/%Priority\uff1a \u663e\u793a\u4f5c\u4e1a\u7684\u4f18\u5148\u7ea7 %Q/%QOS \uff1a\u663e\u793a\u4f5c\u4e1a\u7684\u670d\u52a1\u8d28\u91cf\uff08QoS\uff09\u7ea7\u522b %R/%Reason\uff1a \u663e\u793a\u4f5c\u4e1a\u6302\u8d77\u7684\u539f\u56e0 %r/%ReqNodes\uff1a \u663e\u793a\u4f5c\u4e1a\u8bf7\u6c42\u7684\u8282\u70b9 %S/%StartTime\uff1a \u663e\u793a\u4f5c\u4e1a\u7684\u5f00\u59cb\u65f6\u95f4 %s/%SubmitTime\uff1a \u663e\u793a\u4f5c\u4e1a\u7684\u63d0\u4ea4\u65f6\u95f4 %t/%State\uff1a \u663e\u793a\u4f5c\u4e1a\u7684\u5f53\u524d\u72b6\u6001 %T/%JobType\uff1a \u663e\u793a\u4f5c\u4e1a\u7c7b\u578b %u/%Uid\uff1a\u663e\u793a\u4f5c\u4e1a\u7684 UID %U/%User\uff1a \u663e\u793a\u63d0\u4ea4\u4f5c\u4e1a\u7684\u7528\u6237 %x/%ExcludeNodes\uff1a \u663e\u793a\u4f5c\u4e1a\u6392\u9664\u7684\u8282\u70b9 \u6bcf\u4e2a\u683c\u5f0f\u6807\u8bc6\u7b26\u6216\u5b57\u7b26\u4e32\u53ef\u7528\u5bbd\u5ea6\u8bf4\u660e\u7b26\u4fee\u6539\uff08\u5982 \"%.5j\" \uff09\u3002 \u82e5\u6307\u5b9a\u5bbd\u5ea6\uff0c\u5219\u4f1a\u88ab\u683c\u5f0f\u5316\u4e3a\u81f3\u5c11\u8fbe\u5230\u8be5\u5bbd\u5ea6\u3002 \u82e5\u683c\u5f0f\u65e0\u6548\u6216\u65e0\u6cd5\u8bc6\u522b\uff0c\u7a0b\u5e8f\u4f1a\u62a5\u9519\u5e76\u7ec8\u6b62\u3002 \u4f8b\uff1a --format \"%.5j %.20n %t\" \u4f1a\u8f93\u51fa\u4f5c\u4e1a ID\uff08\u6700\u5c0f\u5bbd\u5ea6 5\uff09\u3001\u540d\u79f0\uff08\u6700\u5c0f\u5bbd\u5ea6 20\uff09\u548c\u72b6\u6001\u3002 -F/--full : \u663e\u793a\u5b8c\u6574\u7684\u5185\u5bb9\uff0c\u5982\u679c\u672a\u6307\u5b9a\uff0c\u9ed8\u8ba4\u6bcf\u9879\u8f93\u51fa30\u4e2a\u5b57\u7b26 -h/--help : \u663e\u793a\u5e2e\u52a9 -i/--iterate uint \uff1a\u6307\u5b9a\u95f4\u9694\u79d2\u6570\u5237\u65b0\u67e5\u8be2\u7ed3\u679c\u3002\u5982 -i=3\u8868\u793a\u6bcf\u9694\u4e09\u79d2\u8f93\u51fa\u4e00\u6b21\u67e5\u8be2\u7ed3\u679c -j/--job string \uff1a\u6307\u5b9a\u67e5\u8be2\u4f5c\u4e1a\u53f7\uff0c\u6307\u5b9a\u591a\u4e2a\u4f5c\u4e1a\u53f7\u65f6\u7528\u9017\u53f7\u9694\u5f00\u3002\u5982 -j=2,3,4 --json\uff1a json\u683c\u5f0f\u8f93\u51fa\u547d\u4ee4\u6267\u884c\u7ed3\u679c -m/--MaxVisibleLines uint32 \uff1a\u6307\u5b9a\u8f93\u51fa\u7ed3\u679c\u7684\u6700\u5927\u6761\u6570\u3002\u5982-m=500\u8868\u793a\u6700\u591a\u8f93\u51fa500\u884c\u67e5\u8be2\u7ed3\u679c -n/--name string \uff1a\u6307\u5b9a\u67e5\u8be2\u4f5c\u4e1a\u540d\uff0c\u6307\u5b9a\u591a\u4e2a\u4f5c\u4e1a\u540d\u65f6\u7528\u9017\u53f7\u9694\u5f00 -N/--noHeader \uff1a\u8f93\u51fa\u9690\u85cf\u8868\u5934 -p/--partition string \uff1a\u6307\u5b9a\u67e5\u8be2\u4f5c\u4e1a\u6240\u5728\u5206\u533a\uff0c\u6307\u5b9a\u591a\u4e2a\u5206\u533a\u65f6\u7528\u9017\u53f7\u9694\u5f00 -q/--qos string \uff1a\u6307\u5b9a\u67e5\u8be2\u4f5c\u4e1a\u7684QoS\uff0c\u6307\u5b9a\u591a\u4e2aQoS\u65f6\u7528\u9017\u53f7\u9694\u5f00 --self\uff1a \u67e5\u770b\u5f53\u524d\u7528\u6237\u63d0\u4ea4\u7684\u4f5c\u4e1a -S/--start \uff1a\u663e\u793a\u4f5c\u4e1a\u7684\u5f00\u59cb\u65f6\u95f4\uff08pending\u4f5c\u4e1a\u663e\u793a\u9884\u671f\u5f00\u59cb\u65f6\u95f4\uff09 -t/--state string \uff1a\u6307\u5b9a\u67e5\u8be2\u4f5c\u4e1a\u72b6\u6001\uff0c\u6307\u5b9a\u591a\u4e2a\u72b6\u6001\u65f6\u7528\u9017\u53f7\u9694\u5f00 -u/--user string \uff1a\u6307\u5b9a\u67e5\u8be2\u4f5c\u4e1a\u6240\u5c5e\u7528\u6237\uff0c\u6307\u5b9a\u591a\u4e2a\u7528\u6237\u65f6\u7528\u9017\u53f7\u9694\u5f00 -v/--version\uff1a \u67e5\u8be2\u7248\u672c\u53f7 \u4f8b\uff1a cqueue - h cqueue - N cqueue - S cqueue - j 30674 , 30675 cqueue - t Pending cqueue -t r cqueue - u cranetest cqueue - A CraneTest cqueue - i 3 cqueue - p CPU cqueue - m 3 cqueue - o = \"%n %u %.5j %.5t %.3T %.5T\" format\u4e2d\u7684\u6307\u5b9a\u5217\u7684\u5bf9\u5e94\u7f29\u5199\u5bf9\u7167\uff1a j-TaskId\uff1bn-Name\uff1bt-State\uff1bp-Partition\uff1bu-User\uff1ba-Account\uff1bT-Type\uff1bI-NodeIndex\uff1bl-TimeLimit\uff1bN-Nodes cqueue -n test cqueue -N cqueue -q test_qos cqueue --self cqueue -t Running -S 2024 -01-02T15:04:05~2024-01-11T11:12:41","title":"\u4e3b\u8981\u53c2\u6570"},{"location":"command/crun/","text":"crun \u63d0\u4ea4\u4ea4\u4e92\u5f0f\u4efb\u52a1 crun\u4f7f\u7528\u547d\u4ee4\u884c\u6307\u5b9a\u7684\u53c2\u6570\u7533\u8bf7\u8d44\u6e90\u5e76\u5728\u8ba1\u7b97\u8282\u70b9\u542f\u52a8\u6307\u5b9a\u7684\u4efb\u52a1\uff0c\u7528\u6237\u7684\u8f93\u5165\u5c06\u88ab\u8f6c\u53d1\u5230\u8ba1\u7b97\u8282\u70b9\u4e0a\u5bf9\u5e94\u7684\u4efb\u52a1\uff0c\u4efb\u52a1\u7684\u8f93\u51fa\u5c06\u88ab\u8f6c\u53d1\u56de\u7528\u6237\u7ec8\u7aef\u3002crun\u9700\u8981\u5728\u6709cfored\u8fd0\u884c\u7684\u8282\u70b9\u4e0a\u542f\u52a8\u3002 crun\u53ea\u652f\u6301\u901a\u8fc7\u547d\u4ee4\u884c\u6307\u5b9a\u8bf7\u6c42\u53c2\u6570\uff0c\u652f\u6301\u7684\u547d\u4ee4\u884c\u9009\u9879\uff1a -h/--help : \u663e\u793a\u5e2e\u52a9 -A/--account string \uff1a\u63d0\u4ea4\u4f5c\u4e1a\u7684\u8d26\u6237 -D/--chdir string \uff1a\u4efb\u52a1\u5de5\u4f5c\u8def\u5f84 -C/--config string \uff1a\u914d\u7f6e\u6587\u4ef6\u8def\u5f84(\u9ed8\u8ba4 \"/etc/crane/config.yaml\") -c/--cpus-per-task float \uff1a\u6bcf\u4e2a\u4efb\u52a1\u6240\u9700\u7684 CPU \u6570\u91cf\uff08\u9ed8\u8ba4\u503c\u4e3a 1\uff09 --debug-level string\uff1a \u53ef\u7528\u7684\u8c03\u8bd5\u7ea7\u522b\uff1atrace\u3001debug\u3001info\uff08\u9ed8\u8ba4\u503c\u4e3a \"info\"\uff09 -x/--exclude string\uff1a \u4ece\u5206\u914d\u4e2d\u6392\u9664\u7279\u5b9a\u8282\u70b9\uff08\u4ee5\u9017\u53f7\u5206\u9694\u7684\u5217\u8868\uff09 --export string\uff1a \u4f20\u64ad\u73af\u5883\u53d8\u91cf --extra-attr string\uff1a \u4f5c\u4e1a\u7684\u989d\u5916\u5c5e\u6027\uff08JSON \u683c\u5f0f\uff09 --get-user-env\uff1a \u52a0\u8f7d\u7528\u6237\u7684\u767b\u5f55\u73af\u5883\u53d8\u91cf --gres string\uff1a \u6bcf\u4e2a\u4efb\u52a1\u6240\u9700\u7684\u901a\u7528\u8d44\u6e90\uff0c\u683c\u5f0f\uff1a\"gpu:a100:1\" \u6216 \"gpu:1\" -J, --job-name string\uff1a \u4f5c\u4e1a\u540d\u79f0 --mail-type string\uff1a \u5f53\u7279\u5b9a\u4e8b\u4ef6\u53d1\u751f\u65f6\u901a\u8fc7\u90ae\u4ef6\u901a\u77e5\u7528\u6237\uff0c\u652f\u6301\u7684\u503c\uff1aNONE\u3001BEGIN\u3001END\u3001FAIL\u3001TIMELIMIT\u3001ALL\uff08\u9ed8\u8ba4\u503c\u4e3a NONE\uff09 --mail-user string\uff1a \u901a\u77e5\u63a5\u6536\u8005\u7684\u90ae\u4ef6\u5730\u5740 --mem string\uff1a \u6700\u5927\u5b9e\u9645\u5185\u5b58\u91cf\uff0c\u652f\u6301 GB(G, g)\u3001MB(M, m)\u3001KB(K, k) \u548c Bytes(B)\uff0c\u9ed8\u8ba4\u5355\u4f4d\u662f MB -w/--nodelist string\uff1a \u8981\u5206\u914d\u7ed9\u4f5c\u4e1a\u7684\u8282\u70b9\uff08\u4ee5\u9017\u53f7\u5206\u9694\u7684\u5217\u8868\uff09 -N/--nodes uint32: \u8981\u5728\u5176\u4e0a\u8fd0\u884c\u4f5c\u4e1a\u7684\u8282\u70b9\u6570\u91cf\uff08\u683c\u5f0f\u4e3a N = min[-max]\uff0c\u9ed8\u8ba4\u503c\u4e3a 1\uff09 --ntasks-per-node uint32\uff1a \u6bcf\u4e2a\u8282\u70b9\u4e0a\u8981\u8c03\u7528\u7684\u4efb\u52a1\u6570\u91cf\uff08\u9ed8\u8ba4\u503c\u4e3a 1\uff09 -p/--partition string: \u8bf7\u6c42\u7684\u5206\u533a --pty: \u4f7f\u7528\u4f2a\u7ec8\u7aef\u8fd0\u884c -q/--qos string: \u4f5c\u4e1a\u4f7f\u7528\u7684\u670d\u52a1\u8d28\u91cf\uff08QoS\uff09 -r/--reservation string\uff1a \u4f7f\u7528\u9884\u7559\u8d44\u6e90 -t/--time string\uff1a \u65f6\u95f4\u9650\u5236\uff0c\u683c\u5f0f\uff1a\"day-hours:minutes:seconds\"\uff08\u5982 5-0:0:1 \u8868\u793a 5 \u5929 1 \u79d2\uff09\u6216 \"hours:minutes:seconds\"\uff08\u5982 10:1:2 \u8868\u793a 10 \u5c0f\u65f6 1 \u5206\u949f 2 \u79d2\uff09 -v/--version:crun \u7684\u7248\u672c --x11: \u542f\u7528 X11 \u652f\u6301\uff0c\u9ed8\u8ba4\u503c\u4e3a false\u3002\u5982\u679c\u672a\u914d\u5408 --x11-forwarding \u4f7f\u7528\uff0c\u5219\u76f4\u63a5\u4f7f\u7528 X11\uff08\u4e0d\u5b89\u5168\uff09 --x11-forwarding: \u7531 CraneSched \u542f\u7528 X11 \u8f6c\u53d1\uff08\u5b89\u5168\uff09\uff0c\u9ed8\u8ba4\u503c\u4e3a false \u4f8b\uff1a\u5728CPU\u5206\u533a\uff0c\u7533\u8bf7\u4e24\u4e2a\u8282\u70b9\uff0c\u4e00\u4e2aCPU\u6838\u5fc3\uff0c200M\u5185\u5b58\uff0c\u5e76\u8fd0\u884cbash\u7a0b\u5e8f\uff1a crun - c 1 -- mem 200 M - p CPU - N 2 / usr / bin / bash \u8fd0\u884c\u7ed3\u679c\uff1a \u4f8b\uff1a\u7533\u8bf7\u4e00\u4e2a\u8282\u70b9\uff0c\u4e14\u8282\u70b9\u4e0d\u80fd\u662fcrane01,crane02\uff0c\u4efb\u52a1\u540d\u79f0\u4e3atestjob\uff0c\u8fd0\u884c\u65f6\u95f4\u9650\u5236\u4e3a0:25:25\uff0c\u5e76\u8fd0\u884cbash\u7a0b\u5e8f\uff1a \u4f8b\uff1a\u5728GPU\u5206\u533a\u7533\u8bf7\u4e00\u4e2a\u8282\u70b9\u548c200M\u8fd0\u884c\u5185\u5b58\uff0c\u8282\u70b9\u53ea\u80fd\u5728crane02\u3001crane03\u4e2d\u9009\u62e9\uff0c\u5e76\u8fd0\u884cbash\u7a0b\u5e8f\uff1a crun\u8fd8\u53ef\u4ee5\u5728calloc\u4efb\u52a1\u5185\u5d4c\u5957\u542f\u52a8\uff0c\u5c06\u81ea\u52a8\u7ee7\u627fcalloc\u4efb\u52a1\u7684\u6240\u6709\u8d44\u6e90\u3002\u4e0d\u9700\u8981\u6307\u5b9a\u9664\u9700\u8981\u8fd0\u884c\u7684\u7a0b\u5e8f\u5916\u5176\u4ed6\u53c2\u6570\u3002 crun -A ROOT -J test_crun -x cranetest03 --get-user-env --ntasks-per-node 2 -q test_qos -t 00 :20:00 /usr/bin/bash crun -D --debug-level trace --export ALL /path /usr/bin/bash crun -w cranetest04 /usr/bin/bash crun --x11 xclock \u5411crun\u542f\u52a8\u7684\u7a0b\u5e8f\u4f20\u9012\u53c2\u6570 Pass arguments to your programe launched by crun: crun -c 1 -- your_programe --your_args // or crun -c 1 \"your_programe --your_args\"","title":"crun"},{"location":"command/crun/#crun","text":"crun\u4f7f\u7528\u547d\u4ee4\u884c\u6307\u5b9a\u7684\u53c2\u6570\u7533\u8bf7\u8d44\u6e90\u5e76\u5728\u8ba1\u7b97\u8282\u70b9\u542f\u52a8\u6307\u5b9a\u7684\u4efb\u52a1\uff0c\u7528\u6237\u7684\u8f93\u5165\u5c06\u88ab\u8f6c\u53d1\u5230\u8ba1\u7b97\u8282\u70b9\u4e0a\u5bf9\u5e94\u7684\u4efb\u52a1\uff0c\u4efb\u52a1\u7684\u8f93\u51fa\u5c06\u88ab\u8f6c\u53d1\u56de\u7528\u6237\u7ec8\u7aef\u3002crun\u9700\u8981\u5728\u6709cfored\u8fd0\u884c\u7684\u8282\u70b9\u4e0a\u542f\u52a8\u3002 crun\u53ea\u652f\u6301\u901a\u8fc7\u547d\u4ee4\u884c\u6307\u5b9a\u8bf7\u6c42\u53c2\u6570\uff0c\u652f\u6301\u7684\u547d\u4ee4\u884c\u9009\u9879\uff1a -h/--help : \u663e\u793a\u5e2e\u52a9 -A/--account string \uff1a\u63d0\u4ea4\u4f5c\u4e1a\u7684\u8d26\u6237 -D/--chdir string \uff1a\u4efb\u52a1\u5de5\u4f5c\u8def\u5f84 -C/--config string \uff1a\u914d\u7f6e\u6587\u4ef6\u8def\u5f84(\u9ed8\u8ba4 \"/etc/crane/config.yaml\") -c/--cpus-per-task float \uff1a\u6bcf\u4e2a\u4efb\u52a1\u6240\u9700\u7684 CPU \u6570\u91cf\uff08\u9ed8\u8ba4\u503c\u4e3a 1\uff09 --debug-level string\uff1a \u53ef\u7528\u7684\u8c03\u8bd5\u7ea7\u522b\uff1atrace\u3001debug\u3001info\uff08\u9ed8\u8ba4\u503c\u4e3a \"info\"\uff09 -x/--exclude string\uff1a \u4ece\u5206\u914d\u4e2d\u6392\u9664\u7279\u5b9a\u8282\u70b9\uff08\u4ee5\u9017\u53f7\u5206\u9694\u7684\u5217\u8868\uff09 --export string\uff1a \u4f20\u64ad\u73af\u5883\u53d8\u91cf --extra-attr string\uff1a \u4f5c\u4e1a\u7684\u989d\u5916\u5c5e\u6027\uff08JSON \u683c\u5f0f\uff09 --get-user-env\uff1a \u52a0\u8f7d\u7528\u6237\u7684\u767b\u5f55\u73af\u5883\u53d8\u91cf --gres string\uff1a \u6bcf\u4e2a\u4efb\u52a1\u6240\u9700\u7684\u901a\u7528\u8d44\u6e90\uff0c\u683c\u5f0f\uff1a\"gpu:a100:1\" \u6216 \"gpu:1\" -J, --job-name string\uff1a \u4f5c\u4e1a\u540d\u79f0 --mail-type string\uff1a \u5f53\u7279\u5b9a\u4e8b\u4ef6\u53d1\u751f\u65f6\u901a\u8fc7\u90ae\u4ef6\u901a\u77e5\u7528\u6237\uff0c\u652f\u6301\u7684\u503c\uff1aNONE\u3001BEGIN\u3001END\u3001FAIL\u3001TIMELIMIT\u3001ALL\uff08\u9ed8\u8ba4\u503c\u4e3a NONE\uff09 --mail-user string\uff1a \u901a\u77e5\u63a5\u6536\u8005\u7684\u90ae\u4ef6\u5730\u5740 --mem string\uff1a \u6700\u5927\u5b9e\u9645\u5185\u5b58\u91cf\uff0c\u652f\u6301 GB(G, g)\u3001MB(M, m)\u3001KB(K, k) \u548c Bytes(B)\uff0c\u9ed8\u8ba4\u5355\u4f4d\u662f MB -w/--nodelist string\uff1a \u8981\u5206\u914d\u7ed9\u4f5c\u4e1a\u7684\u8282\u70b9\uff08\u4ee5\u9017\u53f7\u5206\u9694\u7684\u5217\u8868\uff09 -N/--nodes uint32: \u8981\u5728\u5176\u4e0a\u8fd0\u884c\u4f5c\u4e1a\u7684\u8282\u70b9\u6570\u91cf\uff08\u683c\u5f0f\u4e3a N = min[-max]\uff0c\u9ed8\u8ba4\u503c\u4e3a 1\uff09 --ntasks-per-node uint32\uff1a \u6bcf\u4e2a\u8282\u70b9\u4e0a\u8981\u8c03\u7528\u7684\u4efb\u52a1\u6570\u91cf\uff08\u9ed8\u8ba4\u503c\u4e3a 1\uff09 -p/--partition string: \u8bf7\u6c42\u7684\u5206\u533a --pty: \u4f7f\u7528\u4f2a\u7ec8\u7aef\u8fd0\u884c -q/--qos string: \u4f5c\u4e1a\u4f7f\u7528\u7684\u670d\u52a1\u8d28\u91cf\uff08QoS\uff09 -r/--reservation string\uff1a \u4f7f\u7528\u9884\u7559\u8d44\u6e90 -t/--time string\uff1a \u65f6\u95f4\u9650\u5236\uff0c\u683c\u5f0f\uff1a\"day-hours:minutes:seconds\"\uff08\u5982 5-0:0:1 \u8868\u793a 5 \u5929 1 \u79d2\uff09\u6216 \"hours:minutes:seconds\"\uff08\u5982 10:1:2 \u8868\u793a 10 \u5c0f\u65f6 1 \u5206\u949f 2 \u79d2\uff09 -v/--version:crun \u7684\u7248\u672c --x11: \u542f\u7528 X11 \u652f\u6301\uff0c\u9ed8\u8ba4\u503c\u4e3a false\u3002\u5982\u679c\u672a\u914d\u5408 --x11-forwarding \u4f7f\u7528\uff0c\u5219\u76f4\u63a5\u4f7f\u7528 X11\uff08\u4e0d\u5b89\u5168\uff09 --x11-forwarding: \u7531 CraneSched \u542f\u7528 X11 \u8f6c\u53d1\uff08\u5b89\u5168\uff09\uff0c\u9ed8\u8ba4\u503c\u4e3a false \u4f8b\uff1a\u5728CPU\u5206\u533a\uff0c\u7533\u8bf7\u4e24\u4e2a\u8282\u70b9\uff0c\u4e00\u4e2aCPU\u6838\u5fc3\uff0c200M\u5185\u5b58\uff0c\u5e76\u8fd0\u884cbash\u7a0b\u5e8f\uff1a crun - c 1 -- mem 200 M - p CPU - N 2 / usr / bin / bash \u8fd0\u884c\u7ed3\u679c\uff1a \u4f8b\uff1a\u7533\u8bf7\u4e00\u4e2a\u8282\u70b9\uff0c\u4e14\u8282\u70b9\u4e0d\u80fd\u662fcrane01,crane02\uff0c\u4efb\u52a1\u540d\u79f0\u4e3atestjob\uff0c\u8fd0\u884c\u65f6\u95f4\u9650\u5236\u4e3a0:25:25\uff0c\u5e76\u8fd0\u884cbash\u7a0b\u5e8f\uff1a \u4f8b\uff1a\u5728GPU\u5206\u533a\u7533\u8bf7\u4e00\u4e2a\u8282\u70b9\u548c200M\u8fd0\u884c\u5185\u5b58\uff0c\u8282\u70b9\u53ea\u80fd\u5728crane02\u3001crane03\u4e2d\u9009\u62e9\uff0c\u5e76\u8fd0\u884cbash\u7a0b\u5e8f\uff1a crun\u8fd8\u53ef\u4ee5\u5728calloc\u4efb\u52a1\u5185\u5d4c\u5957\u542f\u52a8\uff0c\u5c06\u81ea\u52a8\u7ee7\u627fcalloc\u4efb\u52a1\u7684\u6240\u6709\u8d44\u6e90\u3002\u4e0d\u9700\u8981\u6307\u5b9a\u9664\u9700\u8981\u8fd0\u884c\u7684\u7a0b\u5e8f\u5916\u5176\u4ed6\u53c2\u6570\u3002 crun -A ROOT -J test_crun -x cranetest03 --get-user-env --ntasks-per-node 2 -q test_qos -t 00 :20:00 /usr/bin/bash crun -D --debug-level trace --export ALL /path /usr/bin/bash crun -w cranetest04 /usr/bin/bash crun --x11 xclock \u5411crun\u542f\u52a8\u7684\u7a0b\u5e8f\u4f20\u9012\u53c2\u6570 Pass arguments to your programe launched by crun: crun -c 1 -- your_programe --your_args // or crun -c 1 \"your_programe --your_args\"","title":"crun \u63d0\u4ea4\u4ea4\u4e92\u5f0f\u4efb\u52a1"},{"location":"deployment/","text":"CraneSched Deployment Guide Welcome to the CraneSched deployment documentation. This guide will walk you through setting up a high-performance computing cluster with CraneSched's job scheduling system. Recommended OS We strongly recommend Rocky Linux 9 for production deployments due to its stability, long-term support, and modern system components. What You'll Deploy CraneSched consists of three main components: Component Description Location Backend cranectld (scheduler) + craned (worker daemon) Control & Compute nodes Frontend CLI ( cbatch , cqueue , etc.) + other services ( cfored , cplugind ) As needed Database MongoDB for storing jobs, accounts, and usage data Control/Storage node The diagram below shows a typical placement of CraneSched components across node types. graph TB subgraph Login Node CLIs[CLIs] cfored_login[cfored] cplugind_login[cplugind] end subgraph Control Node cranectld[cranectld] cplugind_ctrl[cplugind] end subgraph Compute Node craned[craned] cplugind_compute[cplugind] end subgraph Storage Node mongodb[(MongoDB)] end %% Communication CLIs --> cranectld cfored_login --> cranectld cfored_login <-->|streaming I/O| craned cranectld <-->|dispatch/response| craned cranectld <--> mongodb %% Plugin connections cplugind_ctrl <-.-> cranectld cplugind_login <-.-> cranectld cplugind_compute <-.-> craned Quick Start For a simple single-node test environment: Install backend: Follow Rocky Linux 9 Guide (Steps 1-5) Setup database: Follow Database Configuration Guide Configure cluster: Edit /etc/crane/config.yaml (see Configuration Guide ) Start services: systemctl start cranectld craned (And other frontend services as needed) For production multi-node clusters , follow the complete deployment workflow below. Deployment Workflow Step 1: Build Backend Components Choose your operating system and follow the corresponding guide: RHEL 8+ / Fedora 37+ Recommended for production. Includes environment setup, toolchain installation, and building from source. Debian 11+ / Ubuntu 22.04+ Supported but not officially recommended. Documentation is coming soon. CentOS 7 Legacy support (End of Life). Use only for existing deployments. All guides cover system preparation, dependencies, and compilation. After building: Packaging Guide - Create RPM/DEB packages for easier distribution across nodes Step 2: Setup Database MongoDB stores job history, user accounts, and resource usage data. Database Configuration Install MongoDB on the control node, configure authentication, and connect CraneSched. Step 3: Configure Cluster Define your cluster topology, node resources, and scheduling policies. Cluster Configuration Learn how to edit /etc/crane/config.yaml to configure: Cluster name and control machine Node definitions (CPU, memory, GPUs) Partitions and scheduling priorities Network settings and advanced options Step 4: Deploy to Nodes For multi-node clusters, distribute binaries and configurations efficiently. Multi-node Deployment Use SCP, PDSH, or package managers to deploy CraneSched across your cluster. Step 5: Install Frontend Tools Deploy user-facing tools and services for job submission and management. Frontend Deployment Install Go-based frontend components: cbatch , cqueue , cinfo , cacct , and supporting daemons. Optional Features Plugin System Extend CraneSched with plugins for monitoring, notifications, and more. Plugins Guide - Email notifications, resource monitoring, energy tracking Access Control Restrict SSH access to compute nodes for authorized jobs only. PAM Module - Configure PAM-based access control Advanced Device Management Enable GPU and device isolation with eBPF on systems using cgroup v2. eBPF Support - GRES support with eBPF device control Getting Help Check individual guides for troubleshooting sections Review configuration examples in the Rocky 9 and Config guides Verify services are running: systemctl status cranectld craned Check logs: /var/log/crane/ or journalctl -u cranectld","title":"Overview"},{"location":"deployment/#cranesched-deployment-guide","text":"Welcome to the CraneSched deployment documentation. This guide will walk you through setting up a high-performance computing cluster with CraneSched's job scheduling system. Recommended OS We strongly recommend Rocky Linux 9 for production deployments due to its stability, long-term support, and modern system components.","title":"CraneSched Deployment Guide"},{"location":"deployment/#what-youll-deploy","text":"CraneSched consists of three main components: Component Description Location Backend cranectld (scheduler) + craned (worker daemon) Control & Compute nodes Frontend CLI ( cbatch , cqueue , etc.) + other services ( cfored , cplugind ) As needed Database MongoDB for storing jobs, accounts, and usage data Control/Storage node The diagram below shows a typical placement of CraneSched components across node types. graph TB subgraph Login Node CLIs[CLIs] cfored_login[cfored] cplugind_login[cplugind] end subgraph Control Node cranectld[cranectld] cplugind_ctrl[cplugind] end subgraph Compute Node craned[craned] cplugind_compute[cplugind] end subgraph Storage Node mongodb[(MongoDB)] end %% Communication CLIs --> cranectld cfored_login --> cranectld cfored_login <-->|streaming I/O| craned cranectld <-->|dispatch/response| craned cranectld <--> mongodb %% Plugin connections cplugind_ctrl <-.-> cranectld cplugind_login <-.-> cranectld cplugind_compute <-.-> craned","title":"What You'll Deploy"},{"location":"deployment/#quick-start","text":"For a simple single-node test environment: Install backend: Follow Rocky Linux 9 Guide (Steps 1-5) Setup database: Follow Database Configuration Guide Configure cluster: Edit /etc/crane/config.yaml (see Configuration Guide ) Start services: systemctl start cranectld craned (And other frontend services as needed) For production multi-node clusters , follow the complete deployment workflow below.","title":"Quick Start"},{"location":"deployment/#deployment-workflow","text":"","title":"Deployment Workflow"},{"location":"deployment/#step-1-build-backend-components","text":"Choose your operating system and follow the corresponding guide: RHEL 8+ / Fedora 37+ Recommended for production. Includes environment setup, toolchain installation, and building from source. Debian 11+ / Ubuntu 22.04+ Supported but not officially recommended. Documentation is coming soon. CentOS 7 Legacy support (End of Life). Use only for existing deployments. All guides cover system preparation, dependencies, and compilation. After building: Packaging Guide - Create RPM/DEB packages for easier distribution across nodes","title":"Step 1: Build Backend Components"},{"location":"deployment/#step-2-setup-database","text":"MongoDB stores job history, user accounts, and resource usage data. Database Configuration Install MongoDB on the control node, configure authentication, and connect CraneSched.","title":"Step 2: Setup Database"},{"location":"deployment/#step-3-configure-cluster","text":"Define your cluster topology, node resources, and scheduling policies. Cluster Configuration Learn how to edit /etc/crane/config.yaml to configure: Cluster name and control machine Node definitions (CPU, memory, GPUs) Partitions and scheduling priorities Network settings and advanced options","title":"Step 3: Configure Cluster"},{"location":"deployment/#step-4-deploy-to-nodes","text":"For multi-node clusters, distribute binaries and configurations efficiently. Multi-node Deployment Use SCP, PDSH, or package managers to deploy CraneSched across your cluster.","title":"Step 4: Deploy to Nodes"},{"location":"deployment/#step-5-install-frontend-tools","text":"Deploy user-facing tools and services for job submission and management. Frontend Deployment Install Go-based frontend components: cbatch , cqueue , cinfo , cacct , and supporting daemons.","title":"Step 5: Install Frontend Tools"},{"location":"deployment/#optional-features","text":"","title":"Optional Features"},{"location":"deployment/#plugin-system","text":"Extend CraneSched with plugins for monitoring, notifications, and more. Plugins Guide - Email notifications, resource monitoring, energy tracking","title":"Plugin System"},{"location":"deployment/#access-control","text":"Restrict SSH access to compute nodes for authorized jobs only. PAM Module - Configure PAM-based access control","title":"Access Control"},{"location":"deployment/#advanced-device-management","text":"Enable GPU and device isolation with eBPF on systems using cgroup v2. eBPF Support - GRES support with eBPF device control","title":"Advanced Device Management"},{"location":"deployment/#getting-help","text":"Check individual guides for troubleshooting sections Review configuration examples in the Rocky 9 and Config guides Verify services are running: systemctl status cranectld craned Check logs: /var/log/crane/ or journalctl -u cranectld","title":"Getting Help"},{"location":"deployment/backend/CentOS7/","text":"Deployment Guide for CentOS 7 Warning This guide is for CentOS 7 , which has reached End of Life (EOL) . Future updates of CraneSched rely on modern compilers, so this tutorial may not work as intended and is no longer guaranteed to be maintained. 1. Configure Build Environment All commands below should be executed on the build node as the root user. Install additional repositories: yum install -y epel-release centos-release-scl-rh yum install -y ninja-build patch devtoolset-11 rh-git218 Add to ~/.bash_profile : source scl_source enable devtoolset-11 source scl_source enable rh-git218 2. Environment Preparation 2.1 Disable SELinux setenforce 0 sed -i s#SELINUX = enforcing#SELINUX = disabled# /etc/selinux/config 2.2 Install Certificates yum -y install ca-certificates 2.3 Synchronize System Time yum -y install ntp ntpdate systemctl start ntpd systemctl enable ntpd timedatectl set-timezone Asia/Shanghai 2.4 Configure Firewall Tip If you have multiple nodes, perform this step on each node . Otherwise, inter-node communication will fail. Please see the config file /etc/crane/config.yaml for port configuration details. systemctl stop firewalld systemctl disable firewalld If your cluster requires the firewall to remain active, open the following ports: firewall-cmd --add-port = 10013 /tcp --permanent --zone = public firewall-cmd --add-port = 10012 /tcp --permanent --zone = public firewall-cmd --add-port = 10011 /tcp --permanent --zone = public firewall-cmd --add-port = 10010 /tcp --permanent --zone = public firewall-cmd --add-port = 873 /tcp --permanent --zone = public firewall-cmd --reload 3. Install Dependencies yum install -y openssl-devel curl-devel pam-devel zlib-devel zlib-static libaio-devel automake libcurl-devel Install libcgroup from source: yum install -y tar bison flex systemd-devel wget https://github.com/libcgroup/libcgroup/releases/download/v3.1.0/libcgroup-3.1.0.tar.gz tar -zxvf libcgroup-3.1.0.tar.gz cd libcgroup-3.1.0 ./configure make -j make install 4. Install Toolchain CraneSched requires the following toolchain versions: CMake \u2265 3.24 libstdc++ \u2265 11 clang \u2265 19 or g++ \u2265 14 4.1 CMake sudo yum install -y wget wget https://github.com/Kitware/CMake/releases/download/v3.26.4/cmake-3.26.4-linux-x86_64.sh bash cmake-3.26.4-linux-x86_64.sh --prefix = /usr/local --skip-license cmake --version 4.2 GCC 14 CraneSched requires libstdc++ \u2265 11 . The default GCC 4.8 on CentOS 7 is too old, so we need to build and install GCC 14 from source. yum install -y tar bzip2 wget https://ftp.gnu.org/gnu/gcc/gcc-14.1.0/gcc-14.1.0.tar.gz tar -zxvf gcc-14.1.0.tar.gz cd gcc-14.1.0 ./contrib/download_prerequisites mkdir build && cd build ../configure --enable-checking = release --enable-languages = c,c++ --disable-multilib make -j make install If you are using clang, you can use libstdc++ from GCC 14 by adding the following flags to CMake (step 6): -DCMAKE_C_FLAGS_INIT = \"--gcc-toolchain=/usr/local\" \\ -DCMAKE_CXX_FLAGS_INIT = \"--gcc-toolchain=/usr/local\" 5. Build and Install git clone https://github.com/PKUHPC/CraneSched.git cd CraneSched mkdir build && cd build cmake -G Ninja -DCMAKE_C_COMPILER = /usr/bin/gcc \\ -DCMAKE_CXX_COMPILER = /usr/bin/g++ .. cmake --build . ninja install Info If you prefer to use RPM packages, please refer to the Packaging Guide for instructions. For deploying CraneSched to multiple nodes, please follow the Multi-node Deployment Guide . 6. Install and Configure MongoDB MongoDB is required on the control node only. Please follow the Database Configuration Guide for detailed instructions. Info CentOS 7 supports up to MongoDB 7.0. See the database configuration guide for specific installation instructions. 7. PAM Module Setup PAM module configuration is optional but recommended for production clusters. Please follow the PAM Module Configuration Guide for detailed instructions. 8. Configure and Launch Services For cluster configuration details, see the Cluster Configuration Guide . After configuring, start the services: # If using systemd: systemctl start cranectld systemctl start craned # If using executables directly: cd build/src CraneCtld/cranectld Craned/craned","title":"CentOS 7"},{"location":"deployment/backend/CentOS7/#deployment-guide-for-centos-7","text":"Warning This guide is for CentOS 7 , which has reached End of Life (EOL) . Future updates of CraneSched rely on modern compilers, so this tutorial may not work as intended and is no longer guaranteed to be maintained.","title":"Deployment Guide for CentOS 7"},{"location":"deployment/backend/CentOS7/#1-configure-build-environment","text":"All commands below should be executed on the build node as the root user. Install additional repositories: yum install -y epel-release centos-release-scl-rh yum install -y ninja-build patch devtoolset-11 rh-git218 Add to ~/.bash_profile : source scl_source enable devtoolset-11 source scl_source enable rh-git218","title":"1. Configure Build Environment"},{"location":"deployment/backend/CentOS7/#2-environment-preparation","text":"","title":"2. Environment Preparation"},{"location":"deployment/backend/CentOS7/#21-disable-selinux","text":"setenforce 0 sed -i s#SELINUX = enforcing#SELINUX = disabled# /etc/selinux/config","title":"2.1 Disable SELinux"},{"location":"deployment/backend/CentOS7/#22-install-certificates","text":"yum -y install ca-certificates","title":"2.2 Install Certificates"},{"location":"deployment/backend/CentOS7/#23-synchronize-system-time","text":"yum -y install ntp ntpdate systemctl start ntpd systemctl enable ntpd timedatectl set-timezone Asia/Shanghai","title":"2.3 Synchronize System Time"},{"location":"deployment/backend/CentOS7/#24-configure-firewall","text":"Tip If you have multiple nodes, perform this step on each node . Otherwise, inter-node communication will fail. Please see the config file /etc/crane/config.yaml for port configuration details. systemctl stop firewalld systemctl disable firewalld If your cluster requires the firewall to remain active, open the following ports: firewall-cmd --add-port = 10013 /tcp --permanent --zone = public firewall-cmd --add-port = 10012 /tcp --permanent --zone = public firewall-cmd --add-port = 10011 /tcp --permanent --zone = public firewall-cmd --add-port = 10010 /tcp --permanent --zone = public firewall-cmd --add-port = 873 /tcp --permanent --zone = public firewall-cmd --reload","title":"2.4 Configure Firewall"},{"location":"deployment/backend/CentOS7/#3-install-dependencies","text":"yum install -y openssl-devel curl-devel pam-devel zlib-devel zlib-static libaio-devel automake libcurl-devel Install libcgroup from source: yum install -y tar bison flex systemd-devel wget https://github.com/libcgroup/libcgroup/releases/download/v3.1.0/libcgroup-3.1.0.tar.gz tar -zxvf libcgroup-3.1.0.tar.gz cd libcgroup-3.1.0 ./configure make -j make install","title":"3. Install Dependencies"},{"location":"deployment/backend/CentOS7/#4-install-toolchain","text":"CraneSched requires the following toolchain versions: CMake \u2265 3.24 libstdc++ \u2265 11 clang \u2265 19 or g++ \u2265 14","title":"4. Install Toolchain"},{"location":"deployment/backend/CentOS7/#41-cmake","text":"sudo yum install -y wget wget https://github.com/Kitware/CMake/releases/download/v3.26.4/cmake-3.26.4-linux-x86_64.sh bash cmake-3.26.4-linux-x86_64.sh --prefix = /usr/local --skip-license cmake --version","title":"4.1 CMake"},{"location":"deployment/backend/CentOS7/#42-gcc-14","text":"CraneSched requires libstdc++ \u2265 11 . The default GCC 4.8 on CentOS 7 is too old, so we need to build and install GCC 14 from source. yum install -y tar bzip2 wget https://ftp.gnu.org/gnu/gcc/gcc-14.1.0/gcc-14.1.0.tar.gz tar -zxvf gcc-14.1.0.tar.gz cd gcc-14.1.0 ./contrib/download_prerequisites mkdir build && cd build ../configure --enable-checking = release --enable-languages = c,c++ --disable-multilib make -j make install If you are using clang, you can use libstdc++ from GCC 14 by adding the following flags to CMake (step 6): -DCMAKE_C_FLAGS_INIT = \"--gcc-toolchain=/usr/local\" \\ -DCMAKE_CXX_FLAGS_INIT = \"--gcc-toolchain=/usr/local\"","title":"4.2 GCC 14"},{"location":"deployment/backend/CentOS7/#5-build-and-install","text":"git clone https://github.com/PKUHPC/CraneSched.git cd CraneSched mkdir build && cd build cmake -G Ninja -DCMAKE_C_COMPILER = /usr/bin/gcc \\ -DCMAKE_CXX_COMPILER = /usr/bin/g++ .. cmake --build . ninja install Info If you prefer to use RPM packages, please refer to the Packaging Guide for instructions. For deploying CraneSched to multiple nodes, please follow the Multi-node Deployment Guide .","title":"5. Build and Install"},{"location":"deployment/backend/CentOS7/#6-install-and-configure-mongodb","text":"MongoDB is required on the control node only. Please follow the Database Configuration Guide for detailed instructions. Info CentOS 7 supports up to MongoDB 7.0. See the database configuration guide for specific installation instructions.","title":"6. Install and Configure MongoDB"},{"location":"deployment/backend/CentOS7/#7-pam-module-setup","text":"PAM module configuration is optional but recommended for production clusters. Please follow the PAM Module Configuration Guide for detailed instructions.","title":"7. PAM Module Setup"},{"location":"deployment/backend/CentOS7/#8-configure-and-launch-services","text":"For cluster configuration details, see the Cluster Configuration Guide . After configuring, start the services: # If using systemd: systemctl start cranectld systemctl start craned # If using executables directly: cd build/src CraneCtld/cranectld Craned/craned","title":"8. Configure and Launch Services"},{"location":"deployment/backend/Rocky9/","text":"Deployment Guide for Rocky Linux 9 Tip This tutorial is primarily designed for Rocky Linux 9 , but it should be compatible with any RHEL-based distribution (e.g., Rocky Linux 8, AlmaLinux). The instructions are tailored for the x86-64 architecture. For other architectures, such as ARM64, ensure you modify the download links and commands as needed. Please run all commands as the root user throughout this tutorial. 1. Environment Preparation 1.1 Add EPEL Repository dnf install -y yum-utils epel-release dnf config-manager --set-enabled crb 1.2 Enable Time Synchronization dnf install -y chrony systemctl restart systemd-timedated timedatectl set-timezone Asia/Shanghai timedatectl set-ntp true 1.3 Configure Firewall Tip If you have multiple nodes, perform this step on each node . Otherwise, inter-node communication will fail. Please see the config file /etc/crane/config.yaml for port configuration details. systemctl stop firewalld systemctl disable firewalld If your cluster requires the firewall to remain active, open the following ports: firewall-cmd --add-port = 10013 /tcp --permanent --zone = public firewall-cmd --add-port = 10012 /tcp --permanent --zone = public firewall-cmd --add-port = 10011 /tcp --permanent --zone = public firewall-cmd --add-port = 10010 /tcp --permanent --zone = public firewall-cmd --add-port = 873 /tcp --permanent --zone = public firewall-cmd --reload 1.4 Disable SELinux # Temporary (will be reset after reboot) setenforce 0 # Permanent (survives reboot) sed -i s#SELINUX = enforcing#SELINUX = disabled# /etc/selinux/config 1.5 Select CGroup Version (Optional) Rocky 9 uses CGroup v2 by default. CraneSched uses CGroup v1 by default. If you prefer to enable CGroup v2 support, you\u2019ll need additional configuration , or you can switch the system to use CGroup v1. 1.5.1 Configure CGroup v1 If your system is already using CGroup v1, skip this section. # Set kernel boot parameters to switch to CGroup v1 grubby --update-kernel = /boot/vmlinuz- $( uname -r ) \\ --args = \"systemd.unified_cgroup_hierarchy=0 systemd.legacy_systemd_cgroup_controller\" # Reboot to apply changes reboot # Verify version mount | grep cgroup 1.5.2 Configure CGroup v2 # Check if sub-cgroups have access to resources (expect to see cpu, io, memory, etc.) cat /sys/fs/cgroup/cgroup.subtree_control # Grant resource permissions to subgroups echo '+cpuset +cpu +io +memory +pids' > /sys/fs/cgroup/cgroup.subtree_control Additionally, if you plan to use GRES with CGroup v2, please refer to the eBPF guide for setup instructions. 2. Install Toolchain The toolchain must meet the following version requirements: cmake \u2265 3.24 If using clang , version \u2265 19 If using g++ , version \u2265 14 2.1 Install Build Tools dnf install -y \\ gcc-toolset-14 \\ cmake \\ patch \\ flex \\ bison \\ ninja-build echo 'source /opt/rh/gcc-toolset-14/enable' >> /etc/profile.d/extra.sh source /etc/profile.d/extra.sh 2.2 Install Common Utilities dnf install -y tar curl unzip git 3. Install Project Dependencies dnf install -y \\ libstdc++-devel \\ libstdc++-static \\ openssl-devel \\ curl-devel \\ pam-devel \\ zlib-devel \\ libaio-devel \\ systemd-devel \\ libcurl-devel \\ automake 4. Install and Configure MongoDB MongoDB is required on the control node only. Please follow the Database Configuration Guide for detailed instructions. 5. Install and Configure CraneSched 5.1 Build and Install Configure and build CraneSched: git clone https://github.com/PKUHPC/CraneSched.git cd CraneSched mkdir -p build && cd build # For CGroup v1 cmake -G Ninja .. cmake --build . # For CGroup v2 cmake -G Ninja .. -DCRANE_ENABLE_CGROUP_V2 = true cmake --build . Install the built binaries: Tip If you prefer to use RPM packages, please see the Packaging Guide for instructions. cmake --install . For deploying CraneSched to multiple nodes, please follow the Multi-node Deployment Guide . 5.2 Configure PAM Module PAM module configuration is optional but recommended for production clusters to control user access. Please follow the PAM Module Configuration Guide for detailed instructions. 5.3 Configure the Cluster For cluster configuration details, see the Cluster Configuration Guide . 6. Start CraneSched Run manually (foreground): cranectld craned Or use systemd: systemctl daemon-reload systemctl enable cranectld --now systemctl enable craned --now","title":"Rocky Linux 9"},{"location":"deployment/backend/Rocky9/#deployment-guide-for-rocky-linux-9","text":"Tip This tutorial is primarily designed for Rocky Linux 9 , but it should be compatible with any RHEL-based distribution (e.g., Rocky Linux 8, AlmaLinux). The instructions are tailored for the x86-64 architecture. For other architectures, such as ARM64, ensure you modify the download links and commands as needed. Please run all commands as the root user throughout this tutorial.","title":"Deployment Guide for Rocky Linux 9"},{"location":"deployment/backend/Rocky9/#1-environment-preparation","text":"","title":"1. Environment Preparation"},{"location":"deployment/backend/Rocky9/#11-add-epel-repository","text":"dnf install -y yum-utils epel-release dnf config-manager --set-enabled crb","title":"1.1 Add EPEL Repository"},{"location":"deployment/backend/Rocky9/#12-enable-time-synchronization","text":"dnf install -y chrony systemctl restart systemd-timedated timedatectl set-timezone Asia/Shanghai timedatectl set-ntp true","title":"1.2 Enable Time Synchronization"},{"location":"deployment/backend/Rocky9/#13-configure-firewall","text":"Tip If you have multiple nodes, perform this step on each node . Otherwise, inter-node communication will fail. Please see the config file /etc/crane/config.yaml for port configuration details. systemctl stop firewalld systemctl disable firewalld If your cluster requires the firewall to remain active, open the following ports: firewall-cmd --add-port = 10013 /tcp --permanent --zone = public firewall-cmd --add-port = 10012 /tcp --permanent --zone = public firewall-cmd --add-port = 10011 /tcp --permanent --zone = public firewall-cmd --add-port = 10010 /tcp --permanent --zone = public firewall-cmd --add-port = 873 /tcp --permanent --zone = public firewall-cmd --reload","title":"1.3 Configure Firewall"},{"location":"deployment/backend/Rocky9/#14-disable-selinux","text":"# Temporary (will be reset after reboot) setenforce 0 # Permanent (survives reboot) sed -i s#SELINUX = enforcing#SELINUX = disabled# /etc/selinux/config","title":"1.4 Disable SELinux"},{"location":"deployment/backend/Rocky9/#15-select-cgroup-version-optional","text":"Rocky 9 uses CGroup v2 by default. CraneSched uses CGroup v1 by default. If you prefer to enable CGroup v2 support, you\u2019ll need additional configuration , or you can switch the system to use CGroup v1.","title":"1.5 Select CGroup Version (Optional)"},{"location":"deployment/backend/Rocky9/#151-configure-cgroup-v1","text":"If your system is already using CGroup v1, skip this section. # Set kernel boot parameters to switch to CGroup v1 grubby --update-kernel = /boot/vmlinuz- $( uname -r ) \\ --args = \"systemd.unified_cgroup_hierarchy=0 systemd.legacy_systemd_cgroup_controller\" # Reboot to apply changes reboot # Verify version mount | grep cgroup","title":"1.5.1 Configure CGroup v1"},{"location":"deployment/backend/Rocky9/#152-configure-cgroup-v2","text":"# Check if sub-cgroups have access to resources (expect to see cpu, io, memory, etc.) cat /sys/fs/cgroup/cgroup.subtree_control # Grant resource permissions to subgroups echo '+cpuset +cpu +io +memory +pids' > /sys/fs/cgroup/cgroup.subtree_control Additionally, if you plan to use GRES with CGroup v2, please refer to the eBPF guide for setup instructions.","title":"1.5.2 Configure CGroup v2"},{"location":"deployment/backend/Rocky9/#2-install-toolchain","text":"The toolchain must meet the following version requirements: cmake \u2265 3.24 If using clang , version \u2265 19 If using g++ , version \u2265 14","title":"2. Install Toolchain"},{"location":"deployment/backend/Rocky9/#21-install-build-tools","text":"dnf install -y \\ gcc-toolset-14 \\ cmake \\ patch \\ flex \\ bison \\ ninja-build echo 'source /opt/rh/gcc-toolset-14/enable' >> /etc/profile.d/extra.sh source /etc/profile.d/extra.sh","title":"2.1 Install Build Tools"},{"location":"deployment/backend/Rocky9/#22-install-common-utilities","text":"dnf install -y tar curl unzip git","title":"2.2 Install Common Utilities"},{"location":"deployment/backend/Rocky9/#3-install-project-dependencies","text":"dnf install -y \\ libstdc++-devel \\ libstdc++-static \\ openssl-devel \\ curl-devel \\ pam-devel \\ zlib-devel \\ libaio-devel \\ systemd-devel \\ libcurl-devel \\ automake","title":"3. Install Project Dependencies"},{"location":"deployment/backend/Rocky9/#4-install-and-configure-mongodb","text":"MongoDB is required on the control node only. Please follow the Database Configuration Guide for detailed instructions.","title":"4. Install and Configure MongoDB"},{"location":"deployment/backend/Rocky9/#5-install-and-configure-cranesched","text":"","title":"5. Install and Configure CraneSched"},{"location":"deployment/backend/Rocky9/#51-build-and-install","text":"Configure and build CraneSched: git clone https://github.com/PKUHPC/CraneSched.git cd CraneSched mkdir -p build && cd build # For CGroup v1 cmake -G Ninja .. cmake --build . # For CGroup v2 cmake -G Ninja .. -DCRANE_ENABLE_CGROUP_V2 = true cmake --build . Install the built binaries: Tip If you prefer to use RPM packages, please see the Packaging Guide for instructions. cmake --install . For deploying CraneSched to multiple nodes, please follow the Multi-node Deployment Guide .","title":"5.1 Build and Install"},{"location":"deployment/backend/Rocky9/#52-configure-pam-module","text":"PAM module configuration is optional but recommended for production clusters to control user access. Please follow the PAM Module Configuration Guide for detailed instructions.","title":"5.2 Configure PAM Module"},{"location":"deployment/backend/Rocky9/#53-configure-the-cluster","text":"For cluster configuration details, see the Cluster Configuration Guide .","title":"5.3 Configure the Cluster"},{"location":"deployment/backend/Rocky9/#6-start-cranesched","text":"Run manually (foreground): cranectld craned Or use systemd: systemctl daemon-reload systemctl enable cranectld --now systemctl enable craned --now","title":"6. Start CraneSched"},{"location":"deployment/backend/eBPF/","text":"eBPF for GRES on cgroup v2 When using cgroup v2 with GRES (Generic RESources, e.g., GPUs), CraneSched relies on eBPF to enforce device-level GRES limits. This guide walks you through setting up eBPF on your compute nodes. 1. Install Clang 19+ Tip If your system already installed clang >= 19 or your distribution provides clang 19 packages, you can skip this section. Refer to LLVM's official documentation for more details. Install prerequisites: dnf install \\ bpftool \\ bcc \\ bcc-tools \\ elfutils-libelf-devel \\ zlib-devel Build and install Clang 19 from source: # Clone LLVM 19.1.0 git clone --depth = 1 --branch llvmorg-19.1.0 https://github.com/llvm/llvm-project.git \\ llvm-project-19.1.0 cd llvm-project-19.1.0/ # Build dependencies dnf install -y libedit-devel ncurses-devel libxml2-devel python3-devel swig # Build LLVM/Clang mkdir build && cd build cmake -DCMAKE_INSTALL_PREFIX = '/usr/local' \\ -DCMAKE_BUILD_TYPE = 'Release' -G Ninja \\ -DLLVM_ENABLE_PROJECTS = 'clang;clang-tools-extra;lld;lldb' -DLLVM_ENABLE_RUNTIMES = all \\ -DLLVM_TARGETS_TO_BUILD = 'X86;BPF' ../llvm ninja && ninja install # Build and install libc++/libc++abi/libunwind cd ../ mkdir build-libcxx && cd build-libcxx cmake -G Ninja -DCMAKE_INSTALL_PREFIX = '/usr/local' -DCMAKE_C_COMPILER = clang \\ -DCMAKE_CXX_COMPILER = clang++ -DCMAKE_BUILD_TYPE = Release -S ../runtimes \\ -DLLVM_ENABLE_RUNTIMES = \"libcxx;libcxxabi;libunwind\" ninja cxx cxxabi unwind #ninja check-cxx check-cxxabi check-unwind ninja install-cxx install-cxxabi install-unwind # Install ASan and TSan headers and libraries for development builds (compiler-rt) cd ../ mkdir build-compiler-rt && cd build-compiler-rt cmake ../compiler-rt -DCMAKE_C_COMPILER = clang -DCMAKE_CXX_COMPILER = clang++ \\ -DCMAKE_INSTALL_PREFIX = '/usr/local' -DCMAKE_BUILD_TYPE = 'Release' -G Ninja \\ -DLLVM_CMAKE_DIR = ../cmake/modules ninja install Download and build/install libbpf: # Download and extract libbpf wget https://github.com/libbpf/libbpf/archive/refs/tags/v1.4.6.zip unzip v1.4.6.zip cd libbpf-1.4.6/src # Build and install make make install 2. Build with eBPF Support CraneSched could be built with GCC or Clang. However, to use eBPF features, you must use Clang 19 or above. When building CraneSched, ensure that Clang 19 is correctly installed and available in your PATH and build with the following CMake options: -DCRANE_ENABLE_CGROUP_V2=ON -DCRANE_ENABLE_BPF=ON After building, you should see cgroup_dev_bpf.o in the src/Misc/BPF/ directory of the build output. 3. eBPF Configuration In the project build directory: cp ./src/Misc/BPF/cgroup_dev_bpf.o /etc/crane/cgroup_dev_bpf.o Check whether child cgroups have the relevant controllers enabled (e.g., cpu, io, memory, etc.): cat /sys/fs/cgroup/cgroup.subtree_control Enable controllers for child cgroups: echo '+cpuset +cpu +io +memory +pids' > /sys/fs/cgroup/cgroup.subtree_control 4. Mounting the BPF Filesystem If you see errors like the following: libbpf: specified path /sys/fs/bpf/dev_map is not on BPF FS libbpf: map 'dev_map': failed to auto-pin at '/sys/fs/bpf/dev_map': -22 libbpf: map 'dev_map': failed to create: Invalid argument(-22) libbpf: failed to load object 'cgroup_dev_bpf.o' Failed to load BPF object Check whether the BPF filesystem is mounted: mount | grep bpf If not mounted, do the following: Mount the BPF filesystem: mount -t bpf bpf /sys/fs/bpf Mount the BPF debug filesystem: mount -t debugfs none /sys/kernel/debug To view device access logs, run: cat /sys/kernel/debug/tracing/trace_pipe","title":"eBPF Support"},{"location":"deployment/backend/eBPF/#ebpf-for-gres-on-cgroup-v2","text":"When using cgroup v2 with GRES (Generic RESources, e.g., GPUs), CraneSched relies on eBPF to enforce device-level GRES limits. This guide walks you through setting up eBPF on your compute nodes.","title":"eBPF for GRES on cgroup v2"},{"location":"deployment/backend/eBPF/#1-install-clang-19","text":"Tip If your system already installed clang >= 19 or your distribution provides clang 19 packages, you can skip this section. Refer to LLVM's official documentation for more details. Install prerequisites: dnf install \\ bpftool \\ bcc \\ bcc-tools \\ elfutils-libelf-devel \\ zlib-devel Build and install Clang 19 from source: # Clone LLVM 19.1.0 git clone --depth = 1 --branch llvmorg-19.1.0 https://github.com/llvm/llvm-project.git \\ llvm-project-19.1.0 cd llvm-project-19.1.0/ # Build dependencies dnf install -y libedit-devel ncurses-devel libxml2-devel python3-devel swig # Build LLVM/Clang mkdir build && cd build cmake -DCMAKE_INSTALL_PREFIX = '/usr/local' \\ -DCMAKE_BUILD_TYPE = 'Release' -G Ninja \\ -DLLVM_ENABLE_PROJECTS = 'clang;clang-tools-extra;lld;lldb' -DLLVM_ENABLE_RUNTIMES = all \\ -DLLVM_TARGETS_TO_BUILD = 'X86;BPF' ../llvm ninja && ninja install # Build and install libc++/libc++abi/libunwind cd ../ mkdir build-libcxx && cd build-libcxx cmake -G Ninja -DCMAKE_INSTALL_PREFIX = '/usr/local' -DCMAKE_C_COMPILER = clang \\ -DCMAKE_CXX_COMPILER = clang++ -DCMAKE_BUILD_TYPE = Release -S ../runtimes \\ -DLLVM_ENABLE_RUNTIMES = \"libcxx;libcxxabi;libunwind\" ninja cxx cxxabi unwind #ninja check-cxx check-cxxabi check-unwind ninja install-cxx install-cxxabi install-unwind # Install ASan and TSan headers and libraries for development builds (compiler-rt) cd ../ mkdir build-compiler-rt && cd build-compiler-rt cmake ../compiler-rt -DCMAKE_C_COMPILER = clang -DCMAKE_CXX_COMPILER = clang++ \\ -DCMAKE_INSTALL_PREFIX = '/usr/local' -DCMAKE_BUILD_TYPE = 'Release' -G Ninja \\ -DLLVM_CMAKE_DIR = ../cmake/modules ninja install Download and build/install libbpf: # Download and extract libbpf wget https://github.com/libbpf/libbpf/archive/refs/tags/v1.4.6.zip unzip v1.4.6.zip cd libbpf-1.4.6/src # Build and install make make install","title":"1. Install Clang 19+"},{"location":"deployment/backend/eBPF/#2-build-with-ebpf-support","text":"CraneSched could be built with GCC or Clang. However, to use eBPF features, you must use Clang 19 or above. When building CraneSched, ensure that Clang 19 is correctly installed and available in your PATH and build with the following CMake options: -DCRANE_ENABLE_CGROUP_V2=ON -DCRANE_ENABLE_BPF=ON After building, you should see cgroup_dev_bpf.o in the src/Misc/BPF/ directory of the build output.","title":"2. Build with eBPF Support"},{"location":"deployment/backend/eBPF/#3-ebpf-configuration","text":"In the project build directory: cp ./src/Misc/BPF/cgroup_dev_bpf.o /etc/crane/cgroup_dev_bpf.o Check whether child cgroups have the relevant controllers enabled (e.g., cpu, io, memory, etc.): cat /sys/fs/cgroup/cgroup.subtree_control Enable controllers for child cgroups: echo '+cpuset +cpu +io +memory +pids' > /sys/fs/cgroup/cgroup.subtree_control","title":"3. eBPF Configuration"},{"location":"deployment/backend/eBPF/#4-mounting-the-bpf-filesystem","text":"If you see errors like the following: libbpf: specified path /sys/fs/bpf/dev_map is not on BPF FS libbpf: map 'dev_map': failed to auto-pin at '/sys/fs/bpf/dev_map': -22 libbpf: map 'dev_map': failed to create: Invalid argument(-22) libbpf: failed to load object 'cgroup_dev_bpf.o' Failed to load BPF object Check whether the BPF filesystem is mounted: mount | grep bpf If not mounted, do the following: Mount the BPF filesystem: mount -t bpf bpf /sys/fs/bpf Mount the BPF debug filesystem: mount -t debugfs none /sys/kernel/debug To view device access logs, run: cat /sys/kernel/debug/tracing/trace_pipe","title":"4. Mounting the BPF Filesystem"},{"location":"deployment/backend/packaging/","text":"Packaging Guide This guide covers how to build and install RPM and DEB packages for CraneSched. Overview CraneSched uses CPack to generate packages for easy distribution and installation. The build system supports creating both RPM (for Red Hat-based systems) and DEB (for Debian-based systems) packages. Package Components Tip Frontend components are not included in these packages. Please refer to the Frontend Deployment Guide for frontend installation. CraneSched is divided into two main package components: cranectld - Control daemon package (for control nodes) craned - Execution daemon package (for compute nodes) Each package includes: Binary executables Systemd service files Configuration file templates PAM security module (for craned package) Prerequisites Before building packages, ensure you have: Built CraneSched - Complete the build process as described in the Rocky Linux 9 or CentOS 7 guides CMake 3.24+ - Required for package generation RPM tools (for RPM packages): # Rocky/CentOS/Fedora dnf install -y rpm-build DEB tools (for DEB packages): # Ubuntu/Debian apt-get install -y dpkg-dev Building Packages 1. Configure and Build Navigate to your build directory and ensure the project is properly configured: cd CraneSched/build # For CGroup v1 (default) cmake -G Ninja .. # For CGroup v2 cmake -G Ninja .. -DCRANE_ENABLE_CGROUP_V2 = true # Build the project cmake --build . Tip For production deployments, use Release build type: cmake -G Ninja -DCMAKE_BUILD_TYPE = Release .. 2. Generate Packages CPack is configured to build both RPM and DEB packages simultaneously: # Generate both RPM and DEB packages cpack -G \"RPM;DEB\" # Or generate only RPM packages cpack -G RPM # Or generate only DEB packages cpack -G DEB Info The default configuration ( cpack without -G ) generates both RPM and DEB packages. 3. Locate Generated Packages After successful build, packages will be in your build directory: ls -lh *.rpm *.deb Expected output (example): CraneSched-1.1.2-Linux-x86_64-cranectld.rpm CraneSched-1.1.2-Linux-x86_64-craned.rpm CraneSched-1.1.2-Linux-x86_64-cranectld.deb CraneSched-1.1.2-Linux-x86_64-craned.deb Installing Packages RPM-based Systems Control Node: sudo rpm -ivh CraneSched-*-cranectld.rpm Compute Node: sudo rpm -ivh CraneSched-*-craned.rpm Update existing installation: sudo rpm -Uvh CraneSched-*-cranectld.rpm sudo rpm -Uvh CraneSched-*-craned.rpm DEB-based Systems Control Node: sudo dpkg -i CraneSched-*-cranectld.deb Compute Node: sudo dpkg -i CraneSched-*-craned.deb Update existing installation: sudo dpkg -i CraneSched-*-cranectld.deb sudo dpkg -i CraneSched-*-craned.deb Package Details cranectld Package Contains files for the control node: /usr/local/bin/cranectld # Control daemon binary /etc/systemd/system/cranectld.service # Systemd service file /etc/crane/config.yaml.sample # Cluster configuration template /etc/crane/database.yaml.sample # Database configuration template craned Package Contains files for compute nodes: /usr/local/bin/craned # Execution daemon binary /etc/systemd/system/craned.service # Systemd service file /etc/crane/config.yaml.sample # Cluster configuration template /usr/lib64/security/pam_crane.so # PAM authentication module Post-Installation Actions Both packages include a post-installation script that automatically: Creates the crane system user (if not exists) Creates /var/crane directory with appropriate permissions Creates /etc/crane directory Copies sample configuration files to /etc/crane/config.yaml (if not exists) Copies database configuration to /etc/crane/database.yaml (if not exists, cranectld only) Sets appropriate file ownership and permissions","title":"Packaging Guide"},{"location":"deployment/backend/packaging/#packaging-guide","text":"This guide covers how to build and install RPM and DEB packages for CraneSched.","title":"Packaging Guide"},{"location":"deployment/backend/packaging/#overview","text":"CraneSched uses CPack to generate packages for easy distribution and installation. The build system supports creating both RPM (for Red Hat-based systems) and DEB (for Debian-based systems) packages.","title":"Overview"},{"location":"deployment/backend/packaging/#package-components","text":"Tip Frontend components are not included in these packages. Please refer to the Frontend Deployment Guide for frontend installation. CraneSched is divided into two main package components: cranectld - Control daemon package (for control nodes) craned - Execution daemon package (for compute nodes) Each package includes: Binary executables Systemd service files Configuration file templates PAM security module (for craned package)","title":"Package Components"},{"location":"deployment/backend/packaging/#prerequisites","text":"Before building packages, ensure you have: Built CraneSched - Complete the build process as described in the Rocky Linux 9 or CentOS 7 guides CMake 3.24+ - Required for package generation RPM tools (for RPM packages): # Rocky/CentOS/Fedora dnf install -y rpm-build DEB tools (for DEB packages): # Ubuntu/Debian apt-get install -y dpkg-dev","title":"Prerequisites"},{"location":"deployment/backend/packaging/#building-packages","text":"","title":"Building Packages"},{"location":"deployment/backend/packaging/#1-configure-and-build","text":"Navigate to your build directory and ensure the project is properly configured: cd CraneSched/build # For CGroup v1 (default) cmake -G Ninja .. # For CGroup v2 cmake -G Ninja .. -DCRANE_ENABLE_CGROUP_V2 = true # Build the project cmake --build . Tip For production deployments, use Release build type: cmake -G Ninja -DCMAKE_BUILD_TYPE = Release ..","title":"1. Configure and Build"},{"location":"deployment/backend/packaging/#2-generate-packages","text":"CPack is configured to build both RPM and DEB packages simultaneously: # Generate both RPM and DEB packages cpack -G \"RPM;DEB\" # Or generate only RPM packages cpack -G RPM # Or generate only DEB packages cpack -G DEB Info The default configuration ( cpack without -G ) generates both RPM and DEB packages.","title":"2. Generate Packages"},{"location":"deployment/backend/packaging/#3-locate-generated-packages","text":"After successful build, packages will be in your build directory: ls -lh *.rpm *.deb Expected output (example): CraneSched-1.1.2-Linux-x86_64-cranectld.rpm CraneSched-1.1.2-Linux-x86_64-craned.rpm CraneSched-1.1.2-Linux-x86_64-cranectld.deb CraneSched-1.1.2-Linux-x86_64-craned.deb","title":"3. Locate Generated Packages"},{"location":"deployment/backend/packaging/#installing-packages","text":"","title":"Installing Packages"},{"location":"deployment/backend/packaging/#rpm-based-systems","text":"Control Node: sudo rpm -ivh CraneSched-*-cranectld.rpm Compute Node: sudo rpm -ivh CraneSched-*-craned.rpm Update existing installation: sudo rpm -Uvh CraneSched-*-cranectld.rpm sudo rpm -Uvh CraneSched-*-craned.rpm","title":"RPM-based Systems"},{"location":"deployment/backend/packaging/#deb-based-systems","text":"Control Node: sudo dpkg -i CraneSched-*-cranectld.deb Compute Node: sudo dpkg -i CraneSched-*-craned.deb Update existing installation: sudo dpkg -i CraneSched-*-cranectld.deb sudo dpkg -i CraneSched-*-craned.deb","title":"DEB-based Systems"},{"location":"deployment/backend/packaging/#package-details","text":"","title":"Package Details"},{"location":"deployment/backend/packaging/#cranectld-package","text":"Contains files for the control node: /usr/local/bin/cranectld # Control daemon binary /etc/systemd/system/cranectld.service # Systemd service file /etc/crane/config.yaml.sample # Cluster configuration template /etc/crane/database.yaml.sample # Database configuration template","title":"cranectld Package"},{"location":"deployment/backend/packaging/#craned-package","text":"Contains files for compute nodes: /usr/local/bin/craned # Execution daemon binary /etc/systemd/system/craned.service # Systemd service file /etc/crane/config.yaml.sample # Cluster configuration template /usr/lib64/security/pam_crane.so # PAM authentication module","title":"craned Package"},{"location":"deployment/backend/packaging/#post-installation-actions","text":"Both packages include a post-installation script that automatically: Creates the crane system user (if not exists) Creates /var/crane directory with appropriate permissions Creates /etc/crane directory Copies sample configuration files to /etc/crane/config.yaml (if not exists) Copies database configuration to /etc/crane/database.yaml (if not exists, cranectld only) Sets appropriate file ownership and permissions","title":"Post-Installation Actions"},{"location":"deployment/configuration/config/","text":"Cluster Configuration This guide explains how to configure CraneSched through the /etc/crane/config.yaml file to set up your cluster topology, partitions, and scheduling policies. Info The configuration file must be identical on all nodes (control and compute nodes). Any changes require restarting the affected services. Quick Start Example A minimal configuration for a 4-node cluster: # Cluster identification ControlMachine : crane01 ClusterName : my_cluster # Database configuration DbConfigPath : /etc/crane/database.yaml # Node definitions Nodes : - name : \"crane[01-04]\" cpu : 4 memory : 8G # Partition definitions Partitions : - name : compute nodes : \"crane[01-04]\" priority : 5 DefaultPartition : compute Essential Configuration Cluster Settings Define basic cluster information: # Hostname of the node running cranectld (control node) ControlMachine : crane01 # Name of this cluster ClusterName : my_cluster # Path to database configuration file DbConfigPath : /etc/crane/database.yaml # Base directory for CraneSched data and logs CraneBaseDir : /var/crane/ ControlMachine : Must be the actual hostname of your control node ClusterName : Used for identification in multi-cluster environments CraneBaseDir : All relative paths are based on this directory Node Definitions Specify compute node resources: Nodes : # Node range notation - name : \"crane[01-04]\" cpu : 4 memory : 8G # Individual nodes - name : \"crane05\" cpu : 8 memory : 16G # Nodes with GPUs - name : \"crane[06-07]\" cpu : 8 memory : 32G gres : - name : gpu type : a100 DeviceFileRegex : /dev/nvidia[0-3] Node Parameters: name : Hostname or range (e.g., node[01-10] ) cpu : Number of CPU cores memory : Total memory (supports K, M, G, T suffixes) gres : Generic resources like GPUs (optional) Node Range Notation: crane[01-04] expands to: crane01, crane02, crane03, crane04 cn[1-3,5] expands to: cn1, cn2, cn3, cn5 Partition Configuration Organize nodes into partitions: Partitions : # CPU partition - name : CPU nodes : \"crane[01-04]\" priority : 5 # GPU partition - name : GPU nodes : \"crane[05-08]\" priority : 3 DefaultMemPerCpu : 4096 # 4GB per CPU (in MB) MaxMemPerCpu : 8192 # 8GB maximum per CPU # Default partition for job submission DefaultPartition : CPU Partition Parameters: name : Partition identifier nodes : Node range belonging to this partition priority : Higher values = higher priority (affects scheduling) DefaultMemPerCpu : Default memory per CPU in MB (0 = let scheduler decide) MaxMemPerCpu : Maximum memory per CPU in MB (0 = no limit) Scheduling Policy Configure job scheduling behavior: # Scheduling algorithm # Options: priority/basic, priority/multifactor PriorityType : priority/multifactor # Favor smaller jobs in scheduling PriorityFavorSmall : true # Maximum age for priority calculation (days-hours format) PriorityMaxAge : 14-0 # Priority factor weights PriorityWeightAge : 500 # Job wait time weight PriorityWeightFairShare : 10000 # Fair share weight PriorityWeightJobSize : 0 # Job size weight (0=disabled) PriorityWeightPartition : 1000 # Partition priority weight PriorityWeightQoS : 1000000 # QoS priority weight Network Settings Control Node (cranectld) # Listening address and ports for cranectld CraneCtldListenAddr : 0.0.0.0 CraneCtldListenPort : 10011 CraneCtldForInternalListenPort : 10013 Compute Nodes (craned) # Listening address and port for craned CranedListenAddr : 0.0.0.0 CranedListenPort : 10010 # Health check settings Craned : PingInterval : 15 # Ping cranectld every 15 seconds CraneCtldTimeout : 5 # Timeout for cranectld connection Advanced Options TLS Encryption Enable encrypted communication between nodes: TLS : Enabled : true InternalCertFilePath : /etc/crane/tls/internal.pem InternalKeyFilePath : /etc/crane/tls/internal.key ExternalCertFilePath : /etc/crane/tls/external.pem ExternalKeyFilePath : /etc/crane/tls/external.key CaFilePath : /etc/crane/tls/ca.pem AllowedNodes : \"crane[01-10]\" DomainSuffix : crane.local GPU Configuration Define GPU resources with device control: Nodes : - name : \"gpu[01-02]\" cpu : 16 memory : 64G gres : - name : gpu type : a100 # Regex matching device files DeviceFileRegex : /dev/nvidia[0-3] # Additional device files per GPU DeviceFileList : - /dev/dri/renderer[0-3] # Environment injector for runtime EnvInjector : nvidia Queue Limits Control job queue size and scheduling behavior: # Maximum pending jobs in queue (max: 900000) PendingQueueMaxSize : 900000 # Jobs to schedule per cycle (max: 200000) ScheduledBatchSize : 100000 # Reject jobs when queue is full RejectJobsBeyondCapacity : false Partition Access Control Restrict partition access by account: Partitions : - name : restricted nodes : \"special[01-04]\" priority : 10 # Only these accounts can use this partition AllowedAccounts : project1,project2 - name : public nodes : \"compute[01-20]\" priority : 5 # All accounts except these can use this partition DeniedAccounts : banned_account Warning AllowedAccounts and DeniedAccounts are mutually exclusive. If AllowedAccounts is set, DeniedAccounts is ignored. Logging and Debugging Configure log levels and locations: # Log levels: trace, debug, info, warn, error CraneCtldDebugLevel : info CranedDebugLevel : info # Log file paths (relative to CraneBaseDir) CraneCtldLogFile : cranectld/cranectld.log CranedLogFile : craned/craned.log # Run in foreground (useful for debugging) CraneCtldForeground : false CranedForeground : false Applying Changes After modifying the configuration: Distribute to all nodes : # Using pdsh pdcp -w crane [ 01 -04 ] /etc/crane/config.yaml /etc/crane/ Restart services : # Control node systemctl restart cranectld # Compute nodes pdsh -w crane [ 01 -04 ] systemctl restart craned Verify changes : cinfo # Check node and partition status Troubleshooting Nodes not appearing : Check ControlMachine hostname matches actual control node hostname. Configuration mismatch warnings : Ensure /etc/crane/config.yaml is identical on all nodes. Jobs not scheduling : Verify partition configuration and node membership. Resource limits : Check that requested resources don't exceed node definitions.","title":"Cluster Configuration"},{"location":"deployment/configuration/config/#cluster-configuration","text":"This guide explains how to configure CraneSched through the /etc/crane/config.yaml file to set up your cluster topology, partitions, and scheduling policies. Info The configuration file must be identical on all nodes (control and compute nodes). Any changes require restarting the affected services.","title":"Cluster Configuration"},{"location":"deployment/configuration/config/#quick-start-example","text":"A minimal configuration for a 4-node cluster: # Cluster identification ControlMachine : crane01 ClusterName : my_cluster # Database configuration DbConfigPath : /etc/crane/database.yaml # Node definitions Nodes : - name : \"crane[01-04]\" cpu : 4 memory : 8G # Partition definitions Partitions : - name : compute nodes : \"crane[01-04]\" priority : 5 DefaultPartition : compute","title":"Quick Start Example"},{"location":"deployment/configuration/config/#essential-configuration","text":"","title":"Essential Configuration"},{"location":"deployment/configuration/config/#cluster-settings","text":"Define basic cluster information: # Hostname of the node running cranectld (control node) ControlMachine : crane01 # Name of this cluster ClusterName : my_cluster # Path to database configuration file DbConfigPath : /etc/crane/database.yaml # Base directory for CraneSched data and logs CraneBaseDir : /var/crane/ ControlMachine : Must be the actual hostname of your control node ClusterName : Used for identification in multi-cluster environments CraneBaseDir : All relative paths are based on this directory","title":"Cluster Settings"},{"location":"deployment/configuration/config/#node-definitions","text":"Specify compute node resources: Nodes : # Node range notation - name : \"crane[01-04]\" cpu : 4 memory : 8G # Individual nodes - name : \"crane05\" cpu : 8 memory : 16G # Nodes with GPUs - name : \"crane[06-07]\" cpu : 8 memory : 32G gres : - name : gpu type : a100 DeviceFileRegex : /dev/nvidia[0-3] Node Parameters: name : Hostname or range (e.g., node[01-10] ) cpu : Number of CPU cores memory : Total memory (supports K, M, G, T suffixes) gres : Generic resources like GPUs (optional) Node Range Notation: crane[01-04] expands to: crane01, crane02, crane03, crane04 cn[1-3,5] expands to: cn1, cn2, cn3, cn5","title":"Node Definitions"},{"location":"deployment/configuration/config/#partition-configuration","text":"Organize nodes into partitions: Partitions : # CPU partition - name : CPU nodes : \"crane[01-04]\" priority : 5 # GPU partition - name : GPU nodes : \"crane[05-08]\" priority : 3 DefaultMemPerCpu : 4096 # 4GB per CPU (in MB) MaxMemPerCpu : 8192 # 8GB maximum per CPU # Default partition for job submission DefaultPartition : CPU Partition Parameters: name : Partition identifier nodes : Node range belonging to this partition priority : Higher values = higher priority (affects scheduling) DefaultMemPerCpu : Default memory per CPU in MB (0 = let scheduler decide) MaxMemPerCpu : Maximum memory per CPU in MB (0 = no limit)","title":"Partition Configuration"},{"location":"deployment/configuration/config/#scheduling-policy","text":"Configure job scheduling behavior: # Scheduling algorithm # Options: priority/basic, priority/multifactor PriorityType : priority/multifactor # Favor smaller jobs in scheduling PriorityFavorSmall : true # Maximum age for priority calculation (days-hours format) PriorityMaxAge : 14-0 # Priority factor weights PriorityWeightAge : 500 # Job wait time weight PriorityWeightFairShare : 10000 # Fair share weight PriorityWeightJobSize : 0 # Job size weight (0=disabled) PriorityWeightPartition : 1000 # Partition priority weight PriorityWeightQoS : 1000000 # QoS priority weight","title":"Scheduling Policy"},{"location":"deployment/configuration/config/#network-settings","text":"","title":"Network Settings"},{"location":"deployment/configuration/config/#control-node-cranectld","text":"# Listening address and ports for cranectld CraneCtldListenAddr : 0.0.0.0 CraneCtldListenPort : 10011 CraneCtldForInternalListenPort : 10013","title":"Control Node (cranectld)"},{"location":"deployment/configuration/config/#compute-nodes-craned","text":"# Listening address and port for craned CranedListenAddr : 0.0.0.0 CranedListenPort : 10010 # Health check settings Craned : PingInterval : 15 # Ping cranectld every 15 seconds CraneCtldTimeout : 5 # Timeout for cranectld connection","title":"Compute Nodes (craned)"},{"location":"deployment/configuration/config/#advanced-options","text":"","title":"Advanced Options"},{"location":"deployment/configuration/config/#tls-encryption","text":"Enable encrypted communication between nodes: TLS : Enabled : true InternalCertFilePath : /etc/crane/tls/internal.pem InternalKeyFilePath : /etc/crane/tls/internal.key ExternalCertFilePath : /etc/crane/tls/external.pem ExternalKeyFilePath : /etc/crane/tls/external.key CaFilePath : /etc/crane/tls/ca.pem AllowedNodes : \"crane[01-10]\" DomainSuffix : crane.local","title":"TLS Encryption"},{"location":"deployment/configuration/config/#gpu-configuration","text":"Define GPU resources with device control: Nodes : - name : \"gpu[01-02]\" cpu : 16 memory : 64G gres : - name : gpu type : a100 # Regex matching device files DeviceFileRegex : /dev/nvidia[0-3] # Additional device files per GPU DeviceFileList : - /dev/dri/renderer[0-3] # Environment injector for runtime EnvInjector : nvidia","title":"GPU Configuration"},{"location":"deployment/configuration/config/#queue-limits","text":"Control job queue size and scheduling behavior: # Maximum pending jobs in queue (max: 900000) PendingQueueMaxSize : 900000 # Jobs to schedule per cycle (max: 200000) ScheduledBatchSize : 100000 # Reject jobs when queue is full RejectJobsBeyondCapacity : false","title":"Queue Limits"},{"location":"deployment/configuration/config/#partition-access-control","text":"Restrict partition access by account: Partitions : - name : restricted nodes : \"special[01-04]\" priority : 10 # Only these accounts can use this partition AllowedAccounts : project1,project2 - name : public nodes : \"compute[01-20]\" priority : 5 # All accounts except these can use this partition DeniedAccounts : banned_account Warning AllowedAccounts and DeniedAccounts are mutually exclusive. If AllowedAccounts is set, DeniedAccounts is ignored.","title":"Partition Access Control"},{"location":"deployment/configuration/config/#logging-and-debugging","text":"Configure log levels and locations: # Log levels: trace, debug, info, warn, error CraneCtldDebugLevel : info CranedDebugLevel : info # Log file paths (relative to CraneBaseDir) CraneCtldLogFile : cranectld/cranectld.log CranedLogFile : craned/craned.log # Run in foreground (useful for debugging) CraneCtldForeground : false CranedForeground : false","title":"Logging and Debugging"},{"location":"deployment/configuration/config/#applying-changes","text":"After modifying the configuration: Distribute to all nodes : # Using pdsh pdcp -w crane [ 01 -04 ] /etc/crane/config.yaml /etc/crane/ Restart services : # Control node systemctl restart cranectld # Compute nodes pdsh -w crane [ 01 -04 ] systemctl restart craned Verify changes : cinfo # Check node and partition status","title":"Applying Changes"},{"location":"deployment/configuration/config/#troubleshooting","text":"Nodes not appearing : Check ControlMachine hostname matches actual control node hostname. Configuration mismatch warnings : Ensure /etc/crane/config.yaml is identical on all nodes. Jobs not scheduling : Verify partition configuration and node membership. Resource limits : Check that requested resources don't exceed node definitions.","title":"Troubleshooting"},{"location":"deployment/configuration/database/","text":"Database Configuration This guide covers MongoDB installation and configuration for CraneSched. Info MongoDB is only required on the control node . Compute nodes do not need MongoDB. 1. Install MongoDB Install MongoDB Community Edition Please follow the official MongoDB installation guide for your operating system: MongoDB Community Edition Installation Guide CentOS 7 Users CentOS 7 has reached End of Life (EOL) and the latest MongoDB version it supports is MongoDB 7.0 . Please refer to the MongoDB 7.0 installation guide for Red Hat/CentOS 7 . Example: Rocky Linux 9 For Rocky Linux 9, the installation process typically involves: # Create repository file (check MongoDB docs for the latest version) cat > /etc/yum.repos.d/mongodb-org.repo << 'EOF' [mongodb-org-8.2] name=MongoDB Repository baseurl=https://repo.mongodb.org/yum/redhat/9/mongodb-org/8.2/x86_64/ gpgcheck=1 enabled=1 gpgkey=https://pgp.mongodb.com/server-8.0.asc EOF dnf makecache dnf install -y mongodb-org systemctl enable mongod systemctl start mongod Example: Ubuntu/Debian For Ubuntu/Debian systems: # Follow the official guide for the latest instructions # Typical steps include importing GPG key and adding repository curl -fsSL https://www.mongodb.org/static/pgp/server-8.0.asc | \\ sudo gpg -o /usr/share/keyrings/mongodb-server-8.0.gpg \\ --dearmor echo \"deb [ signed-by=/usr/share/keyrings/mongodb-server-8.0.gpg ] http://repo.mongodb.org/apt/debian bookworm/mongodb-org/8.2 main\" | sudo tee /etc/apt/sources.list.d/mongodb-org-8.2.list # Add repository and install sudo apt-get update sudo apt-get install -y mongodb-org sudo systemctl enable mongod sudo systemctl start mongod Tip The examples above may become outdated. Always refer to the official MongoDB documentation for the most current installation instructions. 2. Configure MongoDB 2.1 Generate Key File Create a key file for replica set authentication: openssl rand -base64 756 | sudo -u mongod tee /var/lib/mongo/mongo.key sudo -u mongod chmod 400 /var/lib/mongo/mongo.key 2.2 Create Admin User Connect to MongoDB and create an admin user: mongosh use admin db.createUser ({ user: 'admin' , pwd: '123456' , # Change this in production! roles: [{ role: 'root' , db: 'admin' }] }) db.shutdownServer () quit Warning Use a strong password for the admin user in production environments. 2.3 Enable Authentication and Replication Edit /etc/mongod.conf : security : authorization : enabled keyFile : /var/lib/mongo/mongo.key replication : replSetName : crane_rs Restart MongoDB: systemctl restart mongod 2.4 Initialize Replica Set mongosh use admin db.auth ( \"admin\" , \"123456\" ) config = { \"_id\" : \"crane_rs\" , \"members\" : [ { \"_id\" : 0 , \"host\" : \"<hostname>:27017\" # Replace with your control node hostname } # Add more members here if you have multiple MongoDB nodes ] } rs.initiate ( config ) quit 3. Configure CraneSched Database Connection Create /etc/crane/database.yaml on the control node only : DbUser : admin DbPassword : \"123456\" DbHost : localhost DbPort : 27017 DbReplSetName : crane_rs DbName : crane_db Warning This file should only exist on the control node and be readable only by root or the crane user: chmod 600 /etc/crane/database.yaml Troubleshooting Connection failed : Ensure MongoDB is running and the firewall allows port 27017. Authentication failed : Verify the username and password in database.yaml match the MongoDB user. Replica set errors : Check that the replSetName in mongod.conf matches the value in database.yaml .","title":"Database Configuration"},{"location":"deployment/configuration/database/#database-configuration","text":"This guide covers MongoDB installation and configuration for CraneSched. Info MongoDB is only required on the control node . Compute nodes do not need MongoDB.","title":"Database Configuration"},{"location":"deployment/configuration/database/#1-install-mongodb","text":"","title":"1. Install MongoDB"},{"location":"deployment/configuration/database/#install-mongodb-community-edition","text":"Please follow the official MongoDB installation guide for your operating system: MongoDB Community Edition Installation Guide CentOS 7 Users CentOS 7 has reached End of Life (EOL) and the latest MongoDB version it supports is MongoDB 7.0 . Please refer to the MongoDB 7.0 installation guide for Red Hat/CentOS 7 .","title":"Install MongoDB Community Edition"},{"location":"deployment/configuration/database/#example-rocky-linux-9","text":"For Rocky Linux 9, the installation process typically involves: # Create repository file (check MongoDB docs for the latest version) cat > /etc/yum.repos.d/mongodb-org.repo << 'EOF' [mongodb-org-8.2] name=MongoDB Repository baseurl=https://repo.mongodb.org/yum/redhat/9/mongodb-org/8.2/x86_64/ gpgcheck=1 enabled=1 gpgkey=https://pgp.mongodb.com/server-8.0.asc EOF dnf makecache dnf install -y mongodb-org systemctl enable mongod systemctl start mongod","title":"Example: Rocky Linux 9"},{"location":"deployment/configuration/database/#example-ubuntudebian","text":"For Ubuntu/Debian systems: # Follow the official guide for the latest instructions # Typical steps include importing GPG key and adding repository curl -fsSL https://www.mongodb.org/static/pgp/server-8.0.asc | \\ sudo gpg -o /usr/share/keyrings/mongodb-server-8.0.gpg \\ --dearmor echo \"deb [ signed-by=/usr/share/keyrings/mongodb-server-8.0.gpg ] http://repo.mongodb.org/apt/debian bookworm/mongodb-org/8.2 main\" | sudo tee /etc/apt/sources.list.d/mongodb-org-8.2.list # Add repository and install sudo apt-get update sudo apt-get install -y mongodb-org sudo systemctl enable mongod sudo systemctl start mongod Tip The examples above may become outdated. Always refer to the official MongoDB documentation for the most current installation instructions.","title":"Example: Ubuntu/Debian"},{"location":"deployment/configuration/database/#2-configure-mongodb","text":"","title":"2. Configure MongoDB"},{"location":"deployment/configuration/database/#21-generate-key-file","text":"Create a key file for replica set authentication: openssl rand -base64 756 | sudo -u mongod tee /var/lib/mongo/mongo.key sudo -u mongod chmod 400 /var/lib/mongo/mongo.key","title":"2.1 Generate Key File"},{"location":"deployment/configuration/database/#22-create-admin-user","text":"Connect to MongoDB and create an admin user: mongosh use admin db.createUser ({ user: 'admin' , pwd: '123456' , # Change this in production! roles: [{ role: 'root' , db: 'admin' }] }) db.shutdownServer () quit Warning Use a strong password for the admin user in production environments.","title":"2.2 Create Admin User"},{"location":"deployment/configuration/database/#23-enable-authentication-and-replication","text":"Edit /etc/mongod.conf : security : authorization : enabled keyFile : /var/lib/mongo/mongo.key replication : replSetName : crane_rs Restart MongoDB: systemctl restart mongod","title":"2.3 Enable Authentication and Replication"},{"location":"deployment/configuration/database/#24-initialize-replica-set","text":"mongosh use admin db.auth ( \"admin\" , \"123456\" ) config = { \"_id\" : \"crane_rs\" , \"members\" : [ { \"_id\" : 0 , \"host\" : \"<hostname>:27017\" # Replace with your control node hostname } # Add more members here if you have multiple MongoDB nodes ] } rs.initiate ( config ) quit","title":"2.4 Initialize Replica Set"},{"location":"deployment/configuration/database/#3-configure-cranesched-database-connection","text":"Create /etc/crane/database.yaml on the control node only : DbUser : admin DbPassword : \"123456\" DbHost : localhost DbPort : 27017 DbReplSetName : crane_rs DbName : crane_db Warning This file should only exist on the control node and be readable only by root or the crane user: chmod 600 /etc/crane/database.yaml","title":"3. Configure CraneSched Database Connection"},{"location":"deployment/configuration/database/#troubleshooting","text":"Connection failed : Ensure MongoDB is running and the firewall allows port 27017. Authentication failed : Verify the username and password in database.yaml match the MongoDB user. Replica set errors : Check that the replSetName in mongod.conf matches the value in database.yaml .","title":"Troubleshooting"},{"location":"deployment/configuration/multi-node/","text":"Multi-node Deployment This guide covers deploying CraneSched binaries and configurations across multiple nodes in a cluster. Prerequisites CraneSched is built and tested on one node (typically the control node) All nodes have network connectivity and can resolve hostnames You have root or sudo access to all nodes Deployment Methods Method 1: Using SCP Suitable for small clusters or when deploying to specific nodes individually. Deploy to Control Node # Copy control node binary ssh cranectl \"mkdir -p /etc/crane\" scp CraneSched-1.1.2-Linux-x86_64-cranectld.rpm cranectl:/tmp ssh cranectl \"rpm -ivh /tmp/CraneSched-1.1.2-Linux-x86_64-cranectld.rpm\" scp /etc/crane/config.yaml cranectl:/etc/crane/ scp /etc/crane/database.yaml cranectl:/etc/crane/ Deploy to Compute Nodes # Copy compute node binary ssh crane02 \"mkdir -p /etc/crane\" scp CraneSched-1.1.2-Linux-x86_64-craned.rpm crane02:/tmp ssh crane02 \"rpm -ivh /tmp/CraneSched-1.1.2-Linux-x86_64-craned.rpm\" scp /etc/crane/config.yaml crane02:/etc/crane/ Repeat for each compute node (crane03, crane04, etc.). Method 2: Using PDSH Recommended for larger clusters. PDSH allows parallel execution across multiple nodes. Install PDSH Rocky 9: dnf install -y pdsh CentOS 7: yum install -y pdsh Deploy to Control Nodes # Copy and install cranectld pdcp -w cranectl CraneSched-1.1.2-Linux-x86_64-cranectld.rpm /tmp pdsh -w cranectl \"rpm -ivh /tmp/CraneSched-1.1.2-Linux-x86_64-cranectld.rpm\" # Copy configuration files pdcp -w cranectl /etc/crane/config.yaml /etc/crane/ pdcp -w cranectl /etc/crane/database.yaml /etc/crane/ # Start service pdsh -w cranectl \"systemctl daemon-reload\" pdsh -w cranectl \"systemctl enable cranectld\" pdsh -w cranectl \"systemctl start cranectld\" Deploy to Compute Nodes # Copy and install craned pdcp -w crane [ 01 -04 ] CraneSched-1.1.2-Linux-x86_64-craned.rpm /tmp pdsh -w crane [ 01 -04 ] \"rpm -ivh /tmp/CraneSched-1.1.2-Linux-x86_64-craned.rpm\" # Copy configuration file pdcp -w crane [ 01 -04 ] /etc/crane/config.yaml /etc/crane/ # Start service pdsh -w crane [ 01 -04 ] \"systemctl daemon-reload\" pdsh -w crane [ 01 -04 ] \"systemctl enable craned\" pdsh -w crane [ 01 -04 ] \"systemctl start craned\" Alternative: Manual Binary Copy If you built from source instead of RPM packages: # Control node scp /usr/local/bin/cranectld cranectl:/usr/local/bin/ scp /etc/systemd/system/cranectld.service cranectl:/etc/systemd/system/ # Compute nodes pdcp -w crane [ 01 -04 ] /usr/local/bin/craned /usr/local/bin/ pdcp -w crane [ 01 -04 ] /etc/systemd/system/craned.service /etc/systemd/system/ Verify Deployment After deployment, verify that all nodes are online: cinfo You should see all compute nodes listed with state IDLE or ALLOC . Updating Deployed Nodes To update an existing deployment: # Stop services pdsh -w cranectl \"systemctl stop cranectld\" pdsh -w crane [ 01 -04 ] \"systemctl stop craned\" # Deploy new version pdcp -w cranectl CraneSched-new-version-cranectld.rpm /tmp pdsh -w cranectl \"rpm -Uvh /tmp/CraneSched-new-version-cranectld.rpm\" pdcp -w crane [ 01 -04 ] CraneSched-new-version-craned.rpm /tmp pdsh -w crane [ 01 -04 ] \"rpm -Uvh /tmp/CraneSched-new-version-craned.rpm\" # Start services pdsh -w cranectl \"systemctl start cranectld\" pdsh -w crane [ 01 -04 ] \"systemctl start craned\" Troubleshooting PDSH not found : Install the pdsh package from EPEL repository. Permission denied : Ensure SSH key authentication is set up for root or your deployment user. Nodes not appearing in cinfo : Check firewall settings and ensure inter-node communication is allowed on required ports (10010-10013). Service fails to start : Check logs with journalctl -u cranectld or journalctl -u craned .","title":"Multi-node Deployment"},{"location":"deployment/configuration/multi-node/#multi-node-deployment","text":"This guide covers deploying CraneSched binaries and configurations across multiple nodes in a cluster.","title":"Multi-node Deployment"},{"location":"deployment/configuration/multi-node/#prerequisites","text":"CraneSched is built and tested on one node (typically the control node) All nodes have network connectivity and can resolve hostnames You have root or sudo access to all nodes","title":"Prerequisites"},{"location":"deployment/configuration/multi-node/#deployment-methods","text":"","title":"Deployment Methods"},{"location":"deployment/configuration/multi-node/#method-1-using-scp","text":"Suitable for small clusters or when deploying to specific nodes individually.","title":"Method 1: Using SCP"},{"location":"deployment/configuration/multi-node/#deploy-to-control-node","text":"# Copy control node binary ssh cranectl \"mkdir -p /etc/crane\" scp CraneSched-1.1.2-Linux-x86_64-cranectld.rpm cranectl:/tmp ssh cranectl \"rpm -ivh /tmp/CraneSched-1.1.2-Linux-x86_64-cranectld.rpm\" scp /etc/crane/config.yaml cranectl:/etc/crane/ scp /etc/crane/database.yaml cranectl:/etc/crane/","title":"Deploy to Control Node"},{"location":"deployment/configuration/multi-node/#deploy-to-compute-nodes","text":"# Copy compute node binary ssh crane02 \"mkdir -p /etc/crane\" scp CraneSched-1.1.2-Linux-x86_64-craned.rpm crane02:/tmp ssh crane02 \"rpm -ivh /tmp/CraneSched-1.1.2-Linux-x86_64-craned.rpm\" scp /etc/crane/config.yaml crane02:/etc/crane/ Repeat for each compute node (crane03, crane04, etc.).","title":"Deploy to Compute Nodes"},{"location":"deployment/configuration/multi-node/#method-2-using-pdsh","text":"Recommended for larger clusters. PDSH allows parallel execution across multiple nodes.","title":"Method 2: Using PDSH"},{"location":"deployment/configuration/multi-node/#install-pdsh","text":"Rocky 9: dnf install -y pdsh CentOS 7: yum install -y pdsh","title":"Install PDSH"},{"location":"deployment/configuration/multi-node/#deploy-to-control-nodes","text":"# Copy and install cranectld pdcp -w cranectl CraneSched-1.1.2-Linux-x86_64-cranectld.rpm /tmp pdsh -w cranectl \"rpm -ivh /tmp/CraneSched-1.1.2-Linux-x86_64-cranectld.rpm\" # Copy configuration files pdcp -w cranectl /etc/crane/config.yaml /etc/crane/ pdcp -w cranectl /etc/crane/database.yaml /etc/crane/ # Start service pdsh -w cranectl \"systemctl daemon-reload\" pdsh -w cranectl \"systemctl enable cranectld\" pdsh -w cranectl \"systemctl start cranectld\"","title":"Deploy to Control Nodes"},{"location":"deployment/configuration/multi-node/#deploy-to-compute-nodes_1","text":"# Copy and install craned pdcp -w crane [ 01 -04 ] CraneSched-1.1.2-Linux-x86_64-craned.rpm /tmp pdsh -w crane [ 01 -04 ] \"rpm -ivh /tmp/CraneSched-1.1.2-Linux-x86_64-craned.rpm\" # Copy configuration file pdcp -w crane [ 01 -04 ] /etc/crane/config.yaml /etc/crane/ # Start service pdsh -w crane [ 01 -04 ] \"systemctl daemon-reload\" pdsh -w crane [ 01 -04 ] \"systemctl enable craned\" pdsh -w crane [ 01 -04 ] \"systemctl start craned\"","title":"Deploy to Compute Nodes"},{"location":"deployment/configuration/multi-node/#alternative-manual-binary-copy","text":"If you built from source instead of RPM packages: # Control node scp /usr/local/bin/cranectld cranectl:/usr/local/bin/ scp /etc/systemd/system/cranectld.service cranectl:/etc/systemd/system/ # Compute nodes pdcp -w crane [ 01 -04 ] /usr/local/bin/craned /usr/local/bin/ pdcp -w crane [ 01 -04 ] /etc/systemd/system/craned.service /etc/systemd/system/","title":"Alternative: Manual Binary Copy"},{"location":"deployment/configuration/multi-node/#verify-deployment","text":"After deployment, verify that all nodes are online: cinfo You should see all compute nodes listed with state IDLE or ALLOC .","title":"Verify Deployment"},{"location":"deployment/configuration/multi-node/#updating-deployed-nodes","text":"To update an existing deployment: # Stop services pdsh -w cranectl \"systemctl stop cranectld\" pdsh -w crane [ 01 -04 ] \"systemctl stop craned\" # Deploy new version pdcp -w cranectl CraneSched-new-version-cranectld.rpm /tmp pdsh -w cranectl \"rpm -Uvh /tmp/CraneSched-new-version-cranectld.rpm\" pdcp -w crane [ 01 -04 ] CraneSched-new-version-craned.rpm /tmp pdsh -w crane [ 01 -04 ] \"rpm -Uvh /tmp/CraneSched-new-version-craned.rpm\" # Start services pdsh -w cranectl \"systemctl start cranectld\" pdsh -w crane [ 01 -04 ] \"systemctl start craned\"","title":"Updating Deployed Nodes"},{"location":"deployment/configuration/multi-node/#troubleshooting","text":"PDSH not found : Install the pdsh package from EPEL repository. Permission denied : Ensure SSH key authentication is set up for root or your deployment user. Nodes not appearing in cinfo : Check firewall settings and ensure inter-node communication is allowed on required ports (10010-10013). Service fails to start : Check logs with journalctl -u cranectld or journalctl -u craned .","title":"Troubleshooting"},{"location":"deployment/configuration/pam/","text":"PAM Module Configuration This guide explains how to configure the CraneSched PAM module to control user access to compute nodes. Warning Only configure PAM after the cluster is fully deployed and running . Test the configuration carefully \u2014 misconfiguration can lock you out of SSH access. Always keep a backup SSH session open when editing PAM configuration. Purpose The PAM module ( pam_crane.so ) ensures that only users with active jobs can log in to compute nodes, preventing unauthorized access to cluster resources. Installation Steps 1. Copy PAM Module After building CraneSched, copy the PAM module to the system library: cp build/src/Misc/Pam/pam_crane.so /usr/lib64/security/ 2. Edit PAM Configuration Edit /etc/pam.d/sshd and add the following lines: Add account required pam_crane.so before account include password-auth Add session required pam_crane.so after session include password-auth Example configuration: #%PAM-1.0 auth required pam_sepermit.so auth substack password-auth auth include postlogin # Used with polkit to reauthorize users in remote sessions -auth optional pam_reauthorize.so prepare account required pam_crane.so account required pam_nologin.so account include password-auth password include password-auth # pam_selinux.so close should be the first session rule session required pam_selinux.so close session required pam_loginuid.so # pam_selinux.so open should only be followed by sessions to be executed in the user context session required pam_selinux.so open env_params session required pam_namespace.so session optional pam_keyinit.so force revoke session include password-auth session required pam_crane.so session include postlogin # Used with polkit to reauthorize users in remote sessions -session optional pam_reauthorize.so prepare Warning The line session required pam_crane.so must be placed after session include password-auth . Testing Keep an existing SSH session open as a backup Test SSH login with a regular user account Verify that users can only access nodes where they have active jobs Deployment to Multiple Nodes If you have multiple compute nodes, deploy the PAM module to all of them: # Using pdsh pdcp -w crane [ 01 -04 ] /usr/lib64/security/pam_crane.so /usr/lib64/security/ # Manually copy PAM configuration or use a configuration management tool Troubleshooting Locked out of SSH : Boot into single-user mode or use console access to restore /etc/pam.d/sshd . PAM module not loading : Check file permissions and SELinux context: ls -lZ /usr/lib64/security/pam_crane.so Users can't log in with jobs : Verify that craned is running and can communicate with cranectld .","title":"PAM Module"},{"location":"deployment/configuration/pam/#pam-module-configuration","text":"This guide explains how to configure the CraneSched PAM module to control user access to compute nodes. Warning Only configure PAM after the cluster is fully deployed and running . Test the configuration carefully \u2014 misconfiguration can lock you out of SSH access. Always keep a backup SSH session open when editing PAM configuration.","title":"PAM Module Configuration"},{"location":"deployment/configuration/pam/#purpose","text":"The PAM module ( pam_crane.so ) ensures that only users with active jobs can log in to compute nodes, preventing unauthorized access to cluster resources.","title":"Purpose"},{"location":"deployment/configuration/pam/#installation-steps","text":"","title":"Installation Steps"},{"location":"deployment/configuration/pam/#1-copy-pam-module","text":"After building CraneSched, copy the PAM module to the system library: cp build/src/Misc/Pam/pam_crane.so /usr/lib64/security/","title":"1. Copy PAM Module"},{"location":"deployment/configuration/pam/#2-edit-pam-configuration","text":"Edit /etc/pam.d/sshd and add the following lines: Add account required pam_crane.so before account include password-auth Add session required pam_crane.so after session include password-auth Example configuration: #%PAM-1.0 auth required pam_sepermit.so auth substack password-auth auth include postlogin # Used with polkit to reauthorize users in remote sessions -auth optional pam_reauthorize.so prepare account required pam_crane.so account required pam_nologin.so account include password-auth password include password-auth # pam_selinux.so close should be the first session rule session required pam_selinux.so close session required pam_loginuid.so # pam_selinux.so open should only be followed by sessions to be executed in the user context session required pam_selinux.so open env_params session required pam_namespace.so session optional pam_keyinit.so force revoke session include password-auth session required pam_crane.so session include postlogin # Used with polkit to reauthorize users in remote sessions -session optional pam_reauthorize.so prepare Warning The line session required pam_crane.so must be placed after session include password-auth .","title":"2. Edit PAM Configuration"},{"location":"deployment/configuration/pam/#testing","text":"Keep an existing SSH session open as a backup Test SSH login with a regular user account Verify that users can only access nodes where they have active jobs","title":"Testing"},{"location":"deployment/configuration/pam/#deployment-to-multiple-nodes","text":"If you have multiple compute nodes, deploy the PAM module to all of them: # Using pdsh pdcp -w crane [ 01 -04 ] /usr/lib64/security/pam_crane.so /usr/lib64/security/ # Manually copy PAM configuration or use a configuration management tool","title":"Deployment to Multiple Nodes"},{"location":"deployment/configuration/pam/#troubleshooting","text":"Locked out of SSH : Boot into single-user mode or use console access to restore /etc/pam.d/sshd . PAM module not loading : Check file permissions and SELinux context: ls -lZ /usr/lib64/security/pam_crane.so Users can't log in with jobs : Verify that craned is running and can communicate with cranectld .","title":"Troubleshooting"},{"location":"deployment/frontend/frontend/","text":"Deployment Guide of Frontend Components Tip This tutorial has been verified on Rocky Linux 9 . It is expected to work on other systemd-based distributions, such as Debian, Ubuntu, AlmaLinux, and Fedora . This tutorial is designed for x86-64 architecture. For other architectures, such as ARM64 , ensure you modify the download links and commands as needed. This guide assumes a demo cluster with the following nodes: login01 : User login and job submission node. cranectld : Control node. crane[01-04] : Compute nodes. Please run all commands as the root user throughout this tutorial. Ensure the backend environment installation is completed before proceeding. Overview A brief overview of the main frontend components you will install and run: CLI tools ( cbatch , cqueue , cinfo ...): User-facing command-line utilities for job submission, querying queues and job status, accounting, and job control. Designed to be lightweight and distributed to user login nodes. They communicate with the control node ( cranectld ). cfored (Frontend daemon for interactive jobs): Provides support for interactive jobs (used by crun , calloc ). Typically runs on login nodes where interactive jobs are submitted. Managed by systemd as cfored.service . cplugind (Plugin daemon): Loads and manages plugins (mail, monitor, energy, event, etc.) and exposes plugin services to CraneSched components. Must be running on nodes that require plugin functionality. Plugin .so files and plugin configuration are registered in /etc/crane/config.yaml . 1. Install Golang GOLANG_TARBALL = go1.22.0.linux-amd64.tar.gz # ARM architecture: wget https://dl.google.com/go/go1.22.0.linux-arm64.tar.gz curl -L https://go.dev/dl/ ${ GOLANG_TARBALL } -o /tmp/go.tar.gz # Remove old Golang environment rm -rf /usr/local/go tar -C /usr/local -xzf /tmp/go.tar.gz && rm /tmp/go.tar.gz echo 'export GOPATH=/root/go' >> /etc/profile.d/go.sh echo 'export PATH=$GOPATH/bin:/usr/local/go/bin:$PATH' >> /etc/profile.d/go.sh echo 'go env -w GO111MODULE=on' >> /etc/profile.d/go.sh echo 'go env -w GOPROXY=https://goproxy.cn,direct' >> /etc/profile.d/go.sh source /etc/profile.d/go.sh go install google.golang.org/protobuf/cmd/protoc-gen-go@latest go install google.golang.org/grpc/cmd/protoc-gen-go-grpc@latest 2. Install Protoc PROTOC_ZIP = protoc-30.2-linux-x86_64.zip # aarch64: protoc-30.2-linux-aarch_64.zip curl -L https://github.com/protocolbuffers/protobuf/releases/download/v30.2/ ${ PROTOC_ZIP } -o /tmp/protoc.zip unzip /tmp/protoc.zip -d /usr/local rm /tmp/protoc.zip /usr/local/readme.txt 3. Pull the frontend repository git clone https://github.com/PKUHPC/CraneSched-FrontEnd.git 4. Build and install The working directory is CraneSched-FrontEnd. In this directory, compile all Golang components and install. cd CraneSched-FrontEnd make make install 5. Distribute and start services pdcp -w login01,cranectld,crane [ 01 -04 ] build/bin/* /usr/local/bin/ pdcp -w login01,cranectld,crane [ 01 -04 ] etc/* /usr/lib/systemd/system/ # If you need to submit interactive jobs (crun, calloc), enable Cfored: pdsh -w login01 systemctl daemon-reload pdsh -w login01 systemctl enable cfored pdsh -w login01 systemctl start cfored # If you configured with plugin, enable cplugind pdsh -w login01,crane [ 01 -04 ] systemctl daemon-reload pdsh -w login01,cranectld,crane [ 01 -04 ] systemctl enable cplugind pdsh -w login01,cranectld,crane [ 01 -04 ] systemctl start cplugind 6. Install CLI aliases (optional) You can install Slurm-style aliases for Crane using the following commands, allowing you to use Crane with Slurm command forms: cat > /etc/profile.d/cwrapper.sh << 'EOF' alias sbatch='cwrapper sbatch' alias sacct='cwrapper sacct' alias sacctmgr='cwrapper sacctmgr' alias scancel='cwrapper scancel' alias scontrol='cwrapper scontrol' alias sinfo='cwrapper sinfo' alias squeue='cwrapper squeue' alias srun='cwrapper srun' alias salloc='cwrapper salloc' EOF pdcp -w login01,crane [ 01 -04 ] /etc/profile.d/cwrapper.sh /etc/profile.d/cwrapper.sh pdsh -w login01,crane [ 01 -04 ] chmod 644 /etc/profile.d/cwrapper.sh","title":"Deployment"},{"location":"deployment/frontend/frontend/#deployment-guide-of-frontend-components","text":"Tip This tutorial has been verified on Rocky Linux 9 . It is expected to work on other systemd-based distributions, such as Debian, Ubuntu, AlmaLinux, and Fedora . This tutorial is designed for x86-64 architecture. For other architectures, such as ARM64 , ensure you modify the download links and commands as needed. This guide assumes a demo cluster with the following nodes: login01 : User login and job submission node. cranectld : Control node. crane[01-04] : Compute nodes. Please run all commands as the root user throughout this tutorial. Ensure the backend environment installation is completed before proceeding.","title":"Deployment Guide of Frontend Components"},{"location":"deployment/frontend/frontend/#overview","text":"A brief overview of the main frontend components you will install and run: CLI tools ( cbatch , cqueue , cinfo ...): User-facing command-line utilities for job submission, querying queues and job status, accounting, and job control. Designed to be lightweight and distributed to user login nodes. They communicate with the control node ( cranectld ). cfored (Frontend daemon for interactive jobs): Provides support for interactive jobs (used by crun , calloc ). Typically runs on login nodes where interactive jobs are submitted. Managed by systemd as cfored.service . cplugind (Plugin daemon): Loads and manages plugins (mail, monitor, energy, event, etc.) and exposes plugin services to CraneSched components. Must be running on nodes that require plugin functionality. Plugin .so files and plugin configuration are registered in /etc/crane/config.yaml .","title":"Overview"},{"location":"deployment/frontend/frontend/#1-install-golang","text":"GOLANG_TARBALL = go1.22.0.linux-amd64.tar.gz # ARM architecture: wget https://dl.google.com/go/go1.22.0.linux-arm64.tar.gz curl -L https://go.dev/dl/ ${ GOLANG_TARBALL } -o /tmp/go.tar.gz # Remove old Golang environment rm -rf /usr/local/go tar -C /usr/local -xzf /tmp/go.tar.gz && rm /tmp/go.tar.gz echo 'export GOPATH=/root/go' >> /etc/profile.d/go.sh echo 'export PATH=$GOPATH/bin:/usr/local/go/bin:$PATH' >> /etc/profile.d/go.sh echo 'go env -w GO111MODULE=on' >> /etc/profile.d/go.sh echo 'go env -w GOPROXY=https://goproxy.cn,direct' >> /etc/profile.d/go.sh source /etc/profile.d/go.sh go install google.golang.org/protobuf/cmd/protoc-gen-go@latest go install google.golang.org/grpc/cmd/protoc-gen-go-grpc@latest","title":"1. Install Golang"},{"location":"deployment/frontend/frontend/#2-install-protoc","text":"PROTOC_ZIP = protoc-30.2-linux-x86_64.zip # aarch64: protoc-30.2-linux-aarch_64.zip curl -L https://github.com/protocolbuffers/protobuf/releases/download/v30.2/ ${ PROTOC_ZIP } -o /tmp/protoc.zip unzip /tmp/protoc.zip -d /usr/local rm /tmp/protoc.zip /usr/local/readme.txt","title":"2. Install Protoc"},{"location":"deployment/frontend/frontend/#3-pull-the-frontend-repository","text":"git clone https://github.com/PKUHPC/CraneSched-FrontEnd.git","title":"3. Pull the frontend repository"},{"location":"deployment/frontend/frontend/#4-build-and-install","text":"The working directory is CraneSched-FrontEnd. In this directory, compile all Golang components and install. cd CraneSched-FrontEnd make make install","title":"4. Build and install"},{"location":"deployment/frontend/frontend/#5-distribute-and-start-services","text":"pdcp -w login01,cranectld,crane [ 01 -04 ] build/bin/* /usr/local/bin/ pdcp -w login01,cranectld,crane [ 01 -04 ] etc/* /usr/lib/systemd/system/ # If you need to submit interactive jobs (crun, calloc), enable Cfored: pdsh -w login01 systemctl daemon-reload pdsh -w login01 systemctl enable cfored pdsh -w login01 systemctl start cfored # If you configured with plugin, enable cplugind pdsh -w login01,crane [ 01 -04 ] systemctl daemon-reload pdsh -w login01,cranectld,crane [ 01 -04 ] systemctl enable cplugind pdsh -w login01,cranectld,crane [ 01 -04 ] systemctl start cplugind","title":"5. Distribute and start services"},{"location":"deployment/frontend/frontend/#6-install-cli-aliases-optional","text":"You can install Slurm-style aliases for Crane using the following commands, allowing you to use Crane with Slurm command forms: cat > /etc/profile.d/cwrapper.sh << 'EOF' alias sbatch='cwrapper sbatch' alias sacct='cwrapper sacct' alias sacctmgr='cwrapper sacctmgr' alias scancel='cwrapper scancel' alias scontrol='cwrapper scontrol' alias sinfo='cwrapper sinfo' alias squeue='cwrapper squeue' alias srun='cwrapper srun' alias salloc='cwrapper salloc' EOF pdcp -w login01,crane [ 01 -04 ] /etc/profile.d/cwrapper.sh /etc/profile.d/cwrapper.sh pdsh -w login01,crane [ 01 -04 ] chmod 644 /etc/profile.d/cwrapper.sh","title":"6. Install CLI aliases (optional)"},{"location":"deployment/frontend/plugins/","text":"Plugin Guide Overview The CraneSched plugin system is modular and disabled by default. The cplugind daemon must be running on each node to enable plugin functionality. Plugins and cplugind versions are strictly coupled and must be updated together. Info Plugins are optional. If you do not require any plugin features, you can skip this guide. Plugin Architecture Each CraneSched plugin consists of: Shared library ( .so file): Plugin implementation Plugin configuration ( .yaml file, optional): Plugin-specific settings Configuration Files Understanding the distinction between two types of configuration files is crucial: Global Configuration ( /etc/crane/config.yaml ) : CraneSched's main configuration file containing settings for CLIs, backend, cfored, and cplugind. Plugin paths are registered here. Plugin Configuration (e.g., monitor.yaml ) : Individual plugin settings. Can be located anywhere readable, with the absolute path specified in the global configuration. Available Plugins CraneSched currently provides the following plugins: Plugin Description Mail Send email notifications on job state changes Monitor Collect job resource usage metrics to InfluxDB (Grafana integration supported) Energy Monitor power consumption for nodes and jobs to InfluxDB Event Record node state changes to InfluxDB Installing cplugind The cplugind daemon is part of the CraneSched-FrontEnd repository. Refer to the Frontend Deployment Guide for installation instructions. Start cplugind manually or via systemd: systemctl enable cplugind systemctl start cplugind Enabling Plugins Edit the global configuration file /etc/crane/config.yaml : Plugin : # Toggle the plugin module in CraneSched Enabled : true # Socket path relative to CraneBaseDir PlugindSockPath : \"cplugind/cplugind.sock\" # Debug level: trace, debug, or info (use info in production) PlugindDebugLevel : \"info\" # Plugins to load Plugins : - Name : \"monitor\" Path : \"/path/to/monitor.so\" Config : \"/path/to/monitor.yaml\" Configuration Options Enabled : Controls whether CraneCtld/Craned uses the plugin system PlugindDebugLevel : Log level (trace/debug/info; recommended: info for production) Plugins : List of plugins to load on this node Name : Plugin identifier (any string) Path : Absolute path to the .so file Config : Absolute path to the plugin configuration file Monitor Plugin The monitor plugin collects job-level resource usage metrics and requires installation on compute nodes. Prerequisites Install InfluxDB (not required on compute nodes, but must be network-accessible): docker run -d -p 8086 :8086 --name influxdb2 influxdb:2 Access the web UI at http://<Host IP>:8086 and complete the setup wizard. Record the following information: Username : your_username Bucket : your_bucket_name Org : your_organization Token : your_token Measurement : ResourceUsage Configuration Create plugin configuration file (e.g., /etc/crane/monitor.yaml ): # Cgroup path pattern (%j is replaced with job's cgroup path) Cgroup : CPU : \"/sys/fs/cgroup/cpuacct/%j/cpuacct.usage\" Memory : \"/sys/fs/cgroup/memory/%j/memory.usage_in_bytes\" ProcList : \"/sys/fs/cgroup/memory/%j/cgroup.procs\" # InfluxDB Configuration Database : Username : \"your_username\" Bucket : \"your_bucket_name\" Org : \"your_organization\" Token : \"your_token\" Measurement : \"ResourceUsage\" Url : \"http://localhost:8086\" # Sampling interval in milliseconds Interval : 1000 # Buffer size for batch writes BufferSize : 32 Note : For cgroup v2, update paths accordingly. Register plugin in global configuration ( /etc/crane/config.yaml ): Plugins : - Name : \"monitor\" Path : \"/path/to/build/plugin/monitor.so\" Config : \"/etc/crane/monitor.yaml\" The monitor.so file is typically located in the frontend build directory ( build/plugin/monitor.so ). Mail Plugin The mail plugin sends job notifications via email from the CraneSched control node (cranectld). System Mail Configuration Install mail dependencies : dnf install s-nail postfix ca-certificates Note : The mail command may have different names across distributions. Postfix handles SMTP requests; sendmail can be used as an alternative. Configure SMTP settings by creating /etc/s-nail.rc : set from = \"your_email@example.com\" set smtp = \"smtp.example.com\" set smtp-auth-user = \"your_email@example.com\" set smtp-auth-password = \"your_app_password\" set smtp-auth = login Important : Use an application-specific password, not your account password. Refer to your email provider's documentation. Test mail functionality : echo \"Test Mail Body\" | mail -s \"Test Mail Subject\" recipient@example.com Plugin Configuration Create mail plugin configuration (e.g., /etc/crane/mail.yaml ): # Sender address for mail notifications SenderAddr : example@example.com # Send subject-only emails (no body) SubjectOnly : false Register plugin in /etc/crane/config.yaml (similar to monitor plugin). Using Email Notifications In Job Scripts #!/bin/bash #CBATCH --nodes 1 #CBATCH --ntasks-per-node 1 #CBATCH --mem 1G #CBATCH -J EmailTest #CBATCH --mail-type ALL #CBATCH --mail-user user@example.com hostname echo \"This job triggers email notifications\" sleep 60 Submit the job: cbatch emailjob.sh Command Line cbatch --mail-type = ALL --mail-user = user@example.com test.job Mail Types Type Description BEGIN Job starts running (Pending \u2192 Running) FAILED Job execution fails TIMELIMIT Job exceeds time limit END Job finishes (Running \u2192 Completed/Failed/Cancelled) ALL All of the above events Parameter Priority --mail-type and --mail-user automatically update the internal --extra-attr parameter In job scripts: Later options override earlier ones On command line: --mail-type/user always take precedence over --extra-attr Recommendation : Use --mail-type/user unless you have specific requirements for --extra-attr Troubleshooting Verify Enabled: true in /etc/crane/config.yaml Check that cplugind is running: systemctl status cplugind Ensure .so file paths are absolute and readable Check cplugind logs for errors","title":"Plugins"},{"location":"deployment/frontend/plugins/#plugin-guide","text":"","title":"Plugin Guide"},{"location":"deployment/frontend/plugins/#overview","text":"The CraneSched plugin system is modular and disabled by default. The cplugind daemon must be running on each node to enable plugin functionality. Plugins and cplugind versions are strictly coupled and must be updated together. Info Plugins are optional. If you do not require any plugin features, you can skip this guide.","title":"Overview"},{"location":"deployment/frontend/plugins/#plugin-architecture","text":"Each CraneSched plugin consists of: Shared library ( .so file): Plugin implementation Plugin configuration ( .yaml file, optional): Plugin-specific settings","title":"Plugin Architecture"},{"location":"deployment/frontend/plugins/#configuration-files","text":"Understanding the distinction between two types of configuration files is crucial: Global Configuration ( /etc/crane/config.yaml ) : CraneSched's main configuration file containing settings for CLIs, backend, cfored, and cplugind. Plugin paths are registered here. Plugin Configuration (e.g., monitor.yaml ) : Individual plugin settings. Can be located anywhere readable, with the absolute path specified in the global configuration.","title":"Configuration Files"},{"location":"deployment/frontend/plugins/#available-plugins","text":"CraneSched currently provides the following plugins: Plugin Description Mail Send email notifications on job state changes Monitor Collect job resource usage metrics to InfluxDB (Grafana integration supported) Energy Monitor power consumption for nodes and jobs to InfluxDB Event Record node state changes to InfluxDB","title":"Available Plugins"},{"location":"deployment/frontend/plugins/#installing-cplugind","text":"The cplugind daemon is part of the CraneSched-FrontEnd repository. Refer to the Frontend Deployment Guide for installation instructions. Start cplugind manually or via systemd: systemctl enable cplugind systemctl start cplugind","title":"Installing cplugind"},{"location":"deployment/frontend/plugins/#enabling-plugins","text":"Edit the global configuration file /etc/crane/config.yaml : Plugin : # Toggle the plugin module in CraneSched Enabled : true # Socket path relative to CraneBaseDir PlugindSockPath : \"cplugind/cplugind.sock\" # Debug level: trace, debug, or info (use info in production) PlugindDebugLevel : \"info\" # Plugins to load Plugins : - Name : \"monitor\" Path : \"/path/to/monitor.so\" Config : \"/path/to/monitor.yaml\"","title":"Enabling Plugins"},{"location":"deployment/frontend/plugins/#configuration-options","text":"Enabled : Controls whether CraneCtld/Craned uses the plugin system PlugindDebugLevel : Log level (trace/debug/info; recommended: info for production) Plugins : List of plugins to load on this node Name : Plugin identifier (any string) Path : Absolute path to the .so file Config : Absolute path to the plugin configuration file","title":"Configuration Options"},{"location":"deployment/frontend/plugins/#monitor-plugin","text":"The monitor plugin collects job-level resource usage metrics and requires installation on compute nodes.","title":"Monitor Plugin"},{"location":"deployment/frontend/plugins/#prerequisites","text":"Install InfluxDB (not required on compute nodes, but must be network-accessible): docker run -d -p 8086 :8086 --name influxdb2 influxdb:2 Access the web UI at http://<Host IP>:8086 and complete the setup wizard. Record the following information: Username : your_username Bucket : your_bucket_name Org : your_organization Token : your_token Measurement : ResourceUsage","title":"Prerequisites"},{"location":"deployment/frontend/plugins/#configuration","text":"Create plugin configuration file (e.g., /etc/crane/monitor.yaml ): # Cgroup path pattern (%j is replaced with job's cgroup path) Cgroup : CPU : \"/sys/fs/cgroup/cpuacct/%j/cpuacct.usage\" Memory : \"/sys/fs/cgroup/memory/%j/memory.usage_in_bytes\" ProcList : \"/sys/fs/cgroup/memory/%j/cgroup.procs\" # InfluxDB Configuration Database : Username : \"your_username\" Bucket : \"your_bucket_name\" Org : \"your_organization\" Token : \"your_token\" Measurement : \"ResourceUsage\" Url : \"http://localhost:8086\" # Sampling interval in milliseconds Interval : 1000 # Buffer size for batch writes BufferSize : 32 Note : For cgroup v2, update paths accordingly. Register plugin in global configuration ( /etc/crane/config.yaml ): Plugins : - Name : \"monitor\" Path : \"/path/to/build/plugin/monitor.so\" Config : \"/etc/crane/monitor.yaml\" The monitor.so file is typically located in the frontend build directory ( build/plugin/monitor.so ).","title":"Configuration"},{"location":"deployment/frontend/plugins/#mail-plugin","text":"The mail plugin sends job notifications via email from the CraneSched control node (cranectld).","title":"Mail Plugin"},{"location":"deployment/frontend/plugins/#system-mail-configuration","text":"Install mail dependencies : dnf install s-nail postfix ca-certificates Note : The mail command may have different names across distributions. Postfix handles SMTP requests; sendmail can be used as an alternative. Configure SMTP settings by creating /etc/s-nail.rc : set from = \"your_email@example.com\" set smtp = \"smtp.example.com\" set smtp-auth-user = \"your_email@example.com\" set smtp-auth-password = \"your_app_password\" set smtp-auth = login Important : Use an application-specific password, not your account password. Refer to your email provider's documentation. Test mail functionality : echo \"Test Mail Body\" | mail -s \"Test Mail Subject\" recipient@example.com","title":"System Mail Configuration"},{"location":"deployment/frontend/plugins/#plugin-configuration","text":"Create mail plugin configuration (e.g., /etc/crane/mail.yaml ): # Sender address for mail notifications SenderAddr : example@example.com # Send subject-only emails (no body) SubjectOnly : false Register plugin in /etc/crane/config.yaml (similar to monitor plugin).","title":"Plugin Configuration"},{"location":"deployment/frontend/plugins/#using-email-notifications","text":"","title":"Using Email Notifications"},{"location":"deployment/frontend/plugins/#in-job-scripts","text":"#!/bin/bash #CBATCH --nodes 1 #CBATCH --ntasks-per-node 1 #CBATCH --mem 1G #CBATCH -J EmailTest #CBATCH --mail-type ALL #CBATCH --mail-user user@example.com hostname echo \"This job triggers email notifications\" sleep 60 Submit the job: cbatch emailjob.sh","title":"In Job Scripts"},{"location":"deployment/frontend/plugins/#command-line","text":"cbatch --mail-type = ALL --mail-user = user@example.com test.job","title":"Command Line"},{"location":"deployment/frontend/plugins/#mail-types","text":"Type Description BEGIN Job starts running (Pending \u2192 Running) FAILED Job execution fails TIMELIMIT Job exceeds time limit END Job finishes (Running \u2192 Completed/Failed/Cancelled) ALL All of the above events","title":"Mail Types"},{"location":"deployment/frontend/plugins/#parameter-priority","text":"--mail-type and --mail-user automatically update the internal --extra-attr parameter In job scripts: Later options override earlier ones On command line: --mail-type/user always take precedence over --extra-attr Recommendation : Use --mail-type/user unless you have specific requirements for --extra-attr","title":"Parameter Priority"},{"location":"deployment/frontend/plugins/#troubleshooting","text":"Verify Enabled: true in /etc/crane/config.yaml Check that cplugind is running: systemctl status cplugind Ensure .so file paths are absolute and readable Check cplugind logs for errors","title":"Troubleshooting"},{"location":"referrence/exit_code/","text":"Job Exit Code The EXITCODE column in the cacct command records the reason for a user's job termination. For example, in 0:15 , the first code ( 0 ) is the primary code , and the second code ( 15 ) is the secondary code . Primary Code Value of 0-255 for Program exit return value Secondary Code Program exit signal: 0-63: Signal value Crane-defined codes: 64: Terminated 65: Permission Denied 66: Cgroup Error 67: File Not Found 68: Spawn Process Failed 69: Exceeded Time Limit 70: Crane Daemon Down 71: Execution Error 72: RPC Failure JSON Format Explanation Values 0\u2013255 represent the exit return value . Values 256\u2013320 represent program exit signals . Values above 320 represent Crane-defined codes .","title":"Exit Codes"},{"location":"referrence/exit_code/#job-exit-code","text":"The EXITCODE column in the cacct command records the reason for a user's job termination. For example, in 0:15 , the first code ( 0 ) is the primary code , and the second code ( 15 ) is the secondary code .","title":"Job Exit Code"},{"location":"referrence/exit_code/#primary-code","text":"Value of 0-255 for Program exit return value","title":"Primary Code"},{"location":"referrence/exit_code/#secondary-code","text":"Program exit signal: 0-63: Signal value Crane-defined codes: 64: Terminated 65: Permission Denied 66: Cgroup Error 67: File Not Found 68: Spawn Process Failed 69: Exceeded Time Limit 70: Crane Daemon Down 71: Execution Error 72: RPC Failure","title":"Secondary Code"},{"location":"referrence/exit_code/#json-format-explanation","text":"Values 0\u2013255 represent the exit return value . Values 256\u2013320 represent program exit signals . Values above 320 represent Crane-defined codes .","title":"JSON Format Explanation"}]}